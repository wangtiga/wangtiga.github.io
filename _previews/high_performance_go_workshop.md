---
layout: post
title:  "[è¯‘]High Performance Go Workshop"
date:   2020-01-01 12:00:00 +0800
tags:   tech
---

* category
{:toc}




# High Performance Go Workshop [^HighPerformanceWorkShopEN] [^HighPerformanceWorkShopGithub] [^HighPerformanceWorkShopCN1] [^HighPerformanceWorkShopCN2] 


> å…¶ä»–å€¼å¾—å‚è€ƒçš„æ–‡ç«  [^GoMillionTCP] [^PerformanceIntruction]

Dave Cheney[dave@cheney.net](mailto:dave@cheney.net)Version Dotgo-2019-3-G660848,2019-04-26



## Overview

The goal for this workshop is to give you the tools you need to diagnose performance problems in your Go applications and fix them.

Through the day weâ€™ll work from the smallâ€‰â€”â€‰learning how to write benchmarks, then profiling a small piece of code. Then step out and talk about the execution tracer, the garbage collector and tracing running applications. The remainder of the day will be a chance for you to ask questions, experiment with your own code.

You can find the latest version of this presentation at

[http://bit.ly/dotgo2019](http://bit.ly/dotgo2019)

## Welcome

Hello and welcome! ğŸ‰

The goal for this workshop is to give you the tools you need to diagnose performance problems in your Go applications and fix them.

Through the day weâ€™ll work from the smallâ€‰â€”â€‰learning how to write benchmarks, then profiling a small piece of code. Then step out and talk about the execution tracer, the garbage collector and tracing running applications. The remainder of the day will be a chance for you to ask questions, experiment with your own code.

### Instructors

-   Dave Cheney  [dave@cheney.net](mailto:dave@cheney.net)
    

### License and Materials

This workshop is a collaboration between  [David Cheney](https://twitter.com/davecheney)  and  [Francesc Campoy](https://twitter.com/francesc).

This presentation is licensed under the  [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)  licence.

### Prerequisites

The are several software downloads you will need today.

#### The workshop repository

Download the source to this document and code samples at  [https://github.com/davecheney/high-performance-go-workshop](https://github.com/davecheney/high-performance-go-workshop)

#### Laptop, power supplies, etc.

The workshop material targets Go 1.12.

[Download Go 1.12](https://golang.org/dl/)

If youâ€™ve already upgraded to Go 1.13 thatâ€™s ok. There are always some small changes to optimisation choices between minor Go releases and Iâ€™ll try to point those out as we go along.

#### Graphviz

The section on pprof requires the  `dot`  program which ships with the  `graphviz`  suite of tools.

-   Linux:  `[sudo] apt-get install graphviz`
    
-   OSX:
    
-   MacPorts:  `sudo port install graphviz`
    
-   Homebrew:  `brew install graphviz`
    
-   [Windows](https://graphviz.gitlab.io/download/#Windows)  (untested)
    

#### Google Chrome

The section on the execution tracer requires Google Chrome. It will not work with Safari, Edge, Firefox, or IE 4.01. Please tell your battery Iâ€™m sorry.

[Download Google Chrome](https://www.google.com/chrome/)

#### Your own code to profile and optimise

The final section of the day will be an open session where you can experiment with the tools youâ€™ve learnt.

### One more thing â€¦

This isnâ€™t a lecture, itâ€™s a conversation. Weâ€™ll have lots of breaks to ask questions.

If you donâ€™t understand something, or think what youâ€™re hearing not correct, please ask.

## 1. The past, present, and future of Microprocessor performance å¾®å¤„ç†å™¨æ€§èƒ½çš„è¿‡å»ï¼Œç°åœ¨å’Œæœªæ¥

This is a workshop about writing high performance code. In other workshops I talk about decoupled design and maintainability, but weâ€™re here today to talk about performance.

I want to start today with a short lecture on how I think about the history of the evolution of computers and why I think writing high performance software is important .

ä»Šå¤©æ¼”è®²çš„ä¸»è¦å†…å®¹ä¸»è¦æ˜¯ï¼š
æœ‰å…³è®¡ç®—æœºå‘å±•å†å²,æˆ‘çš„ä¸€äº›æ€è€ƒ;
ä¸ºä»€ä¹ˆç¼–å†™é«˜æ€§èƒ½çš„è½¯ä»¶å¾ˆé‡è¦ã€‚

The reality is that software runs on hardware, so to talk about writing high performance code, first we need to talk about the hardware that runs our code.

å› ä¸ºè½¯ä»¶æ˜¯åœ¨ç¡¬ä»¶ä¸Šè¿è¡Œçš„ï¼Œæ‰€ä»¥ï¼Œè¦æƒ³è®¨è®ºå¦‚ä½•ç¼–å†™é«˜æ€§èƒ½è½¯ä»¶çš„è¯é¢˜ï¼Œæˆ‘ä»¬å…ˆè¯´ä¸€è¯´è¿è¡Œä»£ç çš„ç¡¬ä»¶ã€‚

### 1.1. Mechanical Sympathy æœºæ¢°å…±æƒ…

Sympathy![image 20180818145606919](https://dave.cheney.net/high-performance-go-workshop/images/image-20180818145606919.png)

There is a term in popular use at the moment, youâ€™ll hear people like Martin Thompson or Bill Kennedy talk about â€œmechanical sympathyâ€.

ä½ å¯èƒ½ä» é©¬ä¸Â·æ±¤æ™®æ£®ï¼ˆMartin Thompsonï¼‰æˆ–æ¯”å°”Â·è‚¯å°¼è¿ªï¼ˆBill Kennedyï¼‰è®¨è®ºè¿‡ â€œMechanical Sympathyâ€ è¿™ä¸€æœ¯è¯­ã€‚

The name "Mechanical Sympathy" comes from the great racing car driver Jackie Stewart, who was a 3 times world Formula 1 champion. He believed that the best drivers had enough understanding of how a machine worked so they could work in harmony with it.

"Mechanical Sympathy" æœ€æ—©ç”±æ›¾ä¸‰åº¦è·å¾—ä¸–ç•Œä¸€çº§æ–¹ç¨‹å¼èµ›è½¦å† å†›çš„ èµ›è½¦æ‰‹æ°åŸºÂ·æ–¯å›¾å°”ç‰¹ï¼ˆJackie Stewartï¼‰æå‡ºã€‚
ä»–è®¤ä¸ºï¼Œå¥½çš„é©¾é©¶å‘˜è‚¯å®šå¯¹æœºå™¨çš„å·¥ä½œåŸç†æœ‰è¶³å¤Ÿäº†è§£ï¼Œè¿™æ ·ä»–ä»¬æ‰èƒ½ä¸æœºå™¨å’Œè°å·¥ä½œã€‚

To be a great race car driver, you donâ€™t need to be a great mechanic, but you need to have more than a cursory understanding of how a motor car works.

è¦æˆä¸ºä¸€åå‡ºè‰²çš„èµ›è½¦æ‰‹ï¼Œæ‚¨ä¸éœ€è¦æˆä¸ºä¸€åå‡ºè‰²çš„æœºæ¢°å¸ˆï¼Œä½†æ‚¨éœ€è¦å¯¹æ±½è½¦çš„å·¥ä½œåŸç†æœ‰ä¸€ä¸ªç²—ç•¥çš„äº†è§£ã€‚

I believe the same is true for us as software engineers. I donâ€™t think any of us in this room will be a professional CPU designer, but that doesnâ€™t mean we can ignore the problems that CPU designers face.

æˆ‘è§‰å¾—è½¯ä»¶å·¥ç¨‹å¸ˆä¹Ÿæ˜¯ä¸€æ ·ã€‚
æˆ‘ä»¬ä¸å¿…æˆä¸ºä¸“ä¸šçš„CPUè®¾è®¡è€…ï¼Œä½†éœ€è¦äº†è§£CPUè®¾è®¡äººå‘˜æ‰€é¢ä¸´çš„é—®é¢˜ã€‚

### 1.2. Six orders of magnitude

Thereâ€™s a common internet meme that goes something like this;

![jalopnik](https://dave.cheney.net/high-performance-go-workshop/images/jalopnik.png)

Of course this is preposterous, but it underscores just how much has changed in the computing industry.

As software authors all of us in this room have benefited from Mooreâ€™s Law, the doubling of the number of available transistors on a chip every 18 months, for 40 years. No other industry has experienced a  six order of magnitude  [1](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#_footnotedef_1 "View footnote.") improvement in their tools in the space of a lifetime.

è¿™é‡Œæ‰€æœ‰è½¯ä»¶å¼€å‘è€…éƒ½å—ç›Šäºæ‘©å°”å®šå¾‹ï¼Œå³40å¹´æ¥æ¯18ä¸ªæœˆå°†èŠ¯ç‰‡ä¸Šå¯ç”¨æ™¶ä½“ç®¡çš„æ•°é‡å¢åŠ ä¸€å€ã€‚
æ²¡æœ‰å…¶ä»–è¡Œä¸šåœ¨ä½¿ç”¨å¯¿å‘½å†…å¯¹å·¥å…·çš„æ”¹è¿›è¾¾åˆ°å…­ä¸ªæ•°é‡çº§ã€‚

But this is all changing.

ä½†è¿™ç§å¥½å¤„é©¬ä¸Šå°†æ¶ˆå¤±ã€‚



### 1.3. Are computers still getting faster? è®¡ç®—æœºä¼šè¶Šæ¥è¶Šå¿«å—ï¼Ÿ

So the fundamental question is, confronted with statistic like the ones in the image above, should we ask the question  _are computers still getting faster_?

æˆ‘ä»¬åº”è¯¥å…³å¿ƒçš„é—®é¢˜æ˜¯ï¼šè®¡ç®—æœºä¼šä¸€ç›´è¶Šæ¥è¶Šå¿«å—ï¼Ÿ

If computers are still getting faster then maybe we donâ€™t need to care about the performance of our code, we just wait a bit and the hardware manufacturers will solve our performance problems for us.

å¦‚æœè®¡ç®—æœºçš„é€Ÿåº¦ä»åœ¨ä¸æ–­æé«˜ï¼Œé‚£ä¹ˆä¹Ÿè®¸æˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒä»£ç çš„æ€§èƒ½ï¼Œåªéœ€ç¨ç­‰ä¸€ä¸‹ï¼Œç¡¬ä»¶åˆ¶é€ å•†å°±ä¼šä¸ºæˆ‘ä»¬è§£å†³æ€§èƒ½é—®é¢˜ã€‚



#### 1.3.1. Letâ€™s look at the data

This is the classic data youâ€™ll find in textbooks like  _Computer Architecture, A Quantitative Approach_  by John L. Hennessy and David A. Patterson. This graph was taken from the 5th edition

ä¸‹é¢çš„ç»å…¸æ•°æ®å¯ä»¥åœ¨ ã€Š è®¡ç®—æœºä½“ç³»ç»“æ„ã€‹ï¼Œã€Šçº¦ç¿°Â·äº¨å°¼è¥¿å’Œå¤§å«Â·å¸•ç‰¹æ£®çš„å®šé‡æ–¹æ³•ã€‹æ‰¾åˆ° ã€‚ æ­¤å›¾æ‘˜è‡ªç¬¬5ç‰ˆã€‚

![2313.processorperf](https://community.cadence.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-01-06/2313.processorperf.jpg)

In the 5th edition, Hennessey and Patterson argue that there are three eras of computing performance

-   The first was the 1970â€™s and early 80â€™s which was the formative years. Microprocessors as we know them today didnâ€™t really exist, computers were built from discrete transistors or small scale integrated circuits. Cost, size, and the limitations in the understanding of material science were the limiting factor.
    
-   From the mid 80s to 2004 the trend line is clear. Computer integer performance improved on average by 52% each year. Computer power doubled every two years, hence people conflated Mooreâ€™s law â€” the doubling of the number of transistors on a die, with computer performance.
    
-   Then we come to the third era of computer performance. Things slow down. The aggregate rate of change is 22% per year.


åœ¨ç¬¬5ç‰ˆä¸­ï¼Œè½©å°¼è¯—ï¼ˆHennesseyï¼‰å’Œå¸•ç‰¹æ£®ï¼ˆPattersonï¼‰è®¤ä¸ºè®¡ç®—æ€§èƒ½å­˜åœ¨ä¸‰ä¸ªæ—¶ä»£:

- 1970 ~ 1985 æ˜¯è®¡ç®—æœºå½¢æˆçš„å¹´ä»£ã€‚å¾®å¤„ç†å™¨ä¸å­˜åœ¨ã€‚è®¡ç®—æœºç”±é›†æˆç”µè·¯ç»„æˆã€€ã€‚æˆæœ¬ï¼Œå°ºå¯¸ï¼ŒåŠææ–™ç§‘å­¦çš„å‘å±•æ˜¯ä¸»è¦é™åˆ¶å› ç´ ã€‚ 
- 1985 ~ 2004 æ˜¯è°ƒæ•´å‘å±•çš„å¹´ä»£ã€‚è®¡ç®—æœºæ€§èƒ½æ¯å¹´æé«˜52ï¼…ã€‚è®¡ç®—èƒ½åŠ›æ¯ä¸¤å¹´ç¿»ä¸€ç¿»ã€‚å› æ­¤äººä»¬å°†æ‘©å°”å®šå¾‹å’Œè®¡ç®—æœºæ€§èƒ½æ··ä¸ºä¸€è°ˆï¼Œæ‘©å°”å®šå¾‹æ˜¯æŒ‡èŠ¯ç‰‡ä¸Šæ™¶ä¼‘ç®¡çš„æ•°é‡æ¯ä¸¤å¹´ç¿»ä¸€ç¿»ã€‚
- 2004 ~ è‡³ä»Š(2012)ã€€å¢æ¶¨æ”¾ç¼“ï¼Œæ¯å¹´æ€»å…±å¢åŠ  22% ã€‚


That previous graph only went up to 2012, but fortunately in 2012  [Jeff Preshing](http://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/)  wrote a  [tool to scrape the Spec website and build your own graph](https://github.com/preshing/analyze-spec-benchmarks).

ä¸Šå›¾æ•°æ®åªåˆ°2012å¹´ã€‚ä½†ã€€Jeff Preshing åœ¨2012å¹´å†™äº†ä¸€ä¸ªçˆ¬å–ã€€Spec ç½‘ç«™æ•°ç”Ÿæˆå›¾è¡¨çš„å·¥å…·ï¼Œä¸‹å›¾å°±æ˜¯æ­¤å·¥å…·ç”Ÿæˆçš„ã€€1995ï½2017ã€€å¹´é—´çš„ã€€Specã€€æ•°æ®ã€€ã€‚


![intgraph](https://dave.cheney.net/high-performance-go-workshop/images/int_graph.png)

So this is the same graph using Spec data from 1995 til 2017.

To me, rather than the step change we saw in the 2012 data, Iâ€™d say that  _single core_  performance is approaching a limit. The numbers are slightly better for floating point, but for us in the room doing line of business applications, this is probably not that relevant.

è§‚å¯Ÿ 2012 å¹´çš„æ•°æ®å¯ä»¥å‘ç°ï¼Œ_å•æ ¸_ æ•´æ•°è¿ç®—å•å…ƒçš„æ€§èƒ½å·²ç»æ¥è¿‘æé™ã€‚
è™½ç„¶æµ®ç‚¹æ•°è¿ç®—å•å…ƒçš„æ•°æ®å¯èƒ½ä¼šç¨å¥½ä¸€äº›ï¼Œä½†å¯¹æˆ‘ä»¬åšä¸šåŠ¡ç¨‹åºçš„äººæ¥è¯´ï¼ŒåŒºåˆ«ä¸å¤§ã€‚

> NOTE: æ•´æ•°è¿ç®—å•å…ƒã€æµ®ç‚¹æ•°è¿ç®—å•å…ƒ integer performance  floating point performance æ˜¯æœ‰åŒºåˆ«çš„ã€‚è¯¦ç»†æƒ…å†µ TODO google CSAPP äº†è§£æµ®ç‚¹æ•°è®¡ç®—


#### 1.3.2. Yes, computer are still getting faster, slowly è®¡ç®—æœºè¿˜åœ¨å˜å¿«ï¼Œä½†æ˜¯åœ¨æ…¢æ…¢å˜å¿« [^CPUMax4GHz]


> The first thing to remember about the ending of Mooreâ€™s law is something Gordon Moore told me. He said "All exponentials come to an end".â€‰â€”â€‰[John Hennessy](https://www.youtube.com/watch?v=Azt8Nc-mtKM)

This is Hennessyâ€™s quote from Google Next 18 and his Turing Award lecture. His contention is yes, CPU performance is still improving. However, single threaded integer performance is still improving around 2-3% per year. At this rate its going to take 20 years of compounding growth to double integer performance. Compare that to the go-go days of the 90â€™s where performance was doubling every two years.

Why is this happening?

> æˆˆç™»Â·æ‘©å°”å‘Šè¯‰æˆ‘ï¼Œæ‘©å°”å®šå¾‹ä¸­æŒ‡æ•°å¢æ¶¨çš„è¿‡ç¨‹å³å°†ç»“æŸã€‚ â€”â€‰[John Hennessy](https://www.youtube.com/watch?v=Azt8Nc-mtKM)

è¿™æ˜¯è½©å°¼è¯—ï¼ˆHennessyï¼‰åœ¨Google Next 18ä¸Šçš„å¼•ç”¨ä»¥åŠä»–åœ¨å›¾çµå¥–ä¸Šçš„æ¼”è®²ã€‚ ä»–è®¤ä¸ºCPUæ€§èƒ½ä»åœ¨æé«˜,ä½†æ˜¯å•çº¿ç¨‹ integer performance æ¯å¹´ä»…æé«˜2-3ï¼…å·¦å³ã€‚ ä»¥è¿™ç§é€Ÿåº¦ï¼Œå®ƒå°†éœ€è¦20å¹´çš„å¤åˆå¢é•¿æ‰èƒ½ä½¿æ•´æ•°è¿ç®—çš„æ€§èƒ½ç¿»å€ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œ90å¹´ä»£çš„å‘å±•è¶‹åŠ¿æ˜¯æ¯ä¸¤å¹´ç¿»ä¸€ç•ªã€‚

åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ



### 1.4. Clock speeds æ—¶é’Ÿé€Ÿåº¦

![stuttering](https://dave.cheney.net/high-performance-go-workshop/images/stuttering.png)

This graph from 2015 demonstrates this well. The top line shows the number of transistors on a die. This has continued in a roughly linear trend line since the 1970â€™s. As this is a log/lin graph this linear series represents exponential growth.

However, If we look at the middle line, we see clock speeds have not increased in a decade, we see that cpu speeds stalled around 2004

The bottom graph shows thermal dissipation power; that is electrical power that is turned into heat, follows a same patternâ€”clock speeds and cpu heat dissipation are correlated.


2015 å¹´çš„è¿™å¼ å›¾å¾ˆå¥½åœ°è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚

ç¬¬ä¸€æ¡çº¿æ˜¾ç¤ºäº†èŠ¯ç‰‡ä¸Šçš„æ™¶ä½“ç®¡æ•°é‡ã€‚è‡ª 1970 å¹´ä»£ä»¥æ¥ï¼Œä¸€ç›´ä»¥çº¿æ€§è¶‹åŠ¿æŒç»­å¢é•¿ã€‚ ç”±äºè¿™æ˜¯ log/lin å›¾ï¼Œå› æ­¤è¯¥å›¾è¡¨ç¤ºçš„æ˜¯æŒ‡æ•°å¢é•¿ã€‚

å¦‚æœæˆ‘ä»¬çœ‹ä¸­é—´çš„çº¿ï¼Œä¼šå‘ç°æ—¶é’Ÿé€Ÿåº¦è¿‘åå¹´æ¥æ²¡æœ‰å¢åŠ ï¼Œ CPU é€Ÿåº¦åœ¨ 2004 å¹´å·¦å³åœæ»äº†ã€‚

æœ€ä¸‹é¢ä¸€æ¡çº¿ï¼Œè¡¨ç¤ºæ•£çƒ­åŠŸç‡ï¼›å³å˜æˆçƒ­é‡çš„ç”µèƒ½ï¼Œå®ƒå’Œæ—¶é’Ÿé€Ÿåº¦çš„èµ°å‘å·®ä¸å¤šï¼Œæ‰€ä»¥æ—¶é’Ÿé€Ÿåº¦å’Œ cpu æ•£çƒ­ä¹Ÿæœ‰ä¸€äº›å…³ç³»ã€‚



### 1.5. Heat å‘çƒ­

Why does a CPU produce heat? Itâ€™s a solid state device, there are no moving components, so effects like friction are not (directly) relevant here.

ä¸ºä»€ä¹ˆ CPU ä¼šäº§ç”Ÿçƒ­é‡ï¼Ÿå®ƒæ˜¯å›ºå®šä¸åŠ¨çš„è®¾å¤‡ï¼Œä¹Ÿæ²¡æœ‰ä»€ä¹ˆéœ€è¦æ¥å›æ´»åŠ¨çš„é›¶ä»¶ï¼Œæ‰€ä»¥è¿™é‡Œäº§ç”Ÿçš„çƒ­é‡è‚¯å®šå’Œæ‘©æ“¦ç”Ÿçƒ­æ— å…³ã€‚


This digram is taken from a great  [data sheet produced by TI](http://www.ti.com/lit/an/scaa035b/scaa035b.pdf). In this model the switch in N typed devices is attracted to a positive voltage P type devices are repelled from a positive voltage.

ä¸‹å›¾æ¥è‡ª TI å…¬å¸çš„æ•°æ®ã€‚
åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼ŒN device ä¸­çš„å¼€å…³è¢«å¸å¼•åˆ°æ­£ç”µå‹ä¸Šï¼ŒP device è¢«æ’æ–¥åœ¨æ­£ç”µå‹ä¸Šã€‚

![cmos inverter](https://dave.cheney.net/high-performance-go-workshop/images/cmos-inverter.png)

The power consumption of a CMOS device, which is what every transistor in this room, on your desk, and in your pocket, is made from, is combination of three factors.

1.  Static power. When a transistor is static, that is, not changing its state, there is a small amount of current that leaks through the transistor to ground. The smaller the transistor, the more leakage. Leakage increases with temperature. Even a minute amount of leakage adds up when you have billions of transistors!
    
2.  Dynamic power. When a transistor transitions from one state to another, it must charge or discharge the various capacitances it is connected to the gate. Dynamic power per transistor is the voltage squared times the capacitance and the frequency of change. Lowering the voltage can reduce the power consumed by a transistor, but lower voltages causes the transistor to switch slower.
    
3.  Crowbar, or short circuit current. We like to think of transistors as digital devices occupying one state or another, off or on, atomically. In reality a transistor is an analog device. As a switch a transistor starts out  _mostly_  off, and transitions, or switches, to a state of being  _mostly_  on. This transition or switching time is very fast, in modern processors it is in the order of pico seconds, but that still represents a period of time when there is a low resistance path from Vcc to ground. The faster the transistor switches, its frequency, the more heat is dissipated.
    

æˆ‘ä»¬èº«è¾¹èƒ½çœ‹åˆ°çš„æ‰€æœ‰ CMOS è®¾å¤‡åŠŸè€—ä¸»è¦ç”±ä»¥ä¸‹ä¸‰éƒ¨åˆ†ç»„æˆã€‚

- 1.é™æ€åŠŸè€—ã€‚å½“æ™¶ä½“ç®¡æ²¡æœ‰çŠ¶æ€å˜åŒ–æ—¶ï¼Œåªæœ‰å°‘é‡ç”µæµæ³„æ¼åˆ°å¤§åœ°ã€‚æ™¶ä½“ç®¡è¶Šå°ï¼Œæ³„æ¼è¶Šå¤šï¼›æ¸©åº¦è¶Šé«˜ï¼Œæ³„æ¼è¶Šå¤šã€‚æˆé•¿ä¸Šäº¿çš„æ™¶ä½“ç®¡æ³„æ¼çš„ç”µé‡ç´¯ç§¯åˆ°ä¸€èµ·æ˜¯éå¸¸å·¨å¤§çš„ã€‚
- 2.åŠ¨æ€åŠŸè€—ã€‚å½“æ™¶ä½“ç®¡è¿›è¡ŒçŠ¶æ€è½¬æ¢æ—¶ï¼Œè¦å¯¹æ …æä¸Šçš„ç”µå®¹å……æ”¾ç”µã€‚æ¯ä¸ªæ™¶ä½“ç®¡çš„åŠ¨æ€åŠŸè€—æ˜¯ ç”µå‹xç”µå®¹xé¢‘ç‡^2 ã€‚ä½ç”µå‹ä¼šå‹ä½æ™¶ä½“ç®¡çš„èƒ½è€—ã€‚ä½†ä½ç”µå‹ä¹Ÿä¼šä½¿æ™¶ä½“ç®¡çš„å¼€å…³é€Ÿåº¦å˜æ…¢ã€‚
- 3.çŸ­è·¯ã€‚æˆ‘ä»¬ç»å¸¸æŠŠæ™¶ä½“ç®¡å½“æˆæ•°å­—è®¾å¤‡ï¼Œä½†å®ƒå®é™…ä¸Šæ˜¯æ¨¡æ‹Ÿè®¾å¤‡ã€‚ä¸€ä¸ªå¯åŠ¨æ—¶æ˜¯ off çŠ¶æ€ï¼ŒçŠ¶æ€åˆ‡æ¢æ—¶æ˜¯ on çŠ¶æ€çš„ switch ã€‚è¿™ä¸ªè½¬æ¢è¿‡ç¨‹å¾ˆå¿«ï¼Œç°åœ¨å¤„ç†å™¨å¤§çº¦åªéœ€è¦ã€€ä¸€çš®ç§’(pico second)ï¼Œä½†å½“ä» Vcc åˆ° ground çš„ç”µé˜»è·¯å¾„å¾ˆä½æ—¶ï¼Œè¿™ä¸ªæ—¶é—´ä¹Ÿä¸ç®—çŸ­äº†ã€‚è¿™ä¸ªã€€switch è½¬æ¢å¾—è¶Šå¿«ï¼Œé¢‘ç‡è¶Šé«˜ï¼Œå®ƒçš„æ¸©åº¦ä¹Ÿè¶Šé«˜ã€‚

> TODO gate æ …æ æŠ€æœ¯æ¦‚å¿µæ˜¯æŒ‡ä»€ä¹ˆ?




### 1.6. The end of Dennard scaling ( ä¸¹çº³å¾· æ‰©å±•çš„ç»ˆç»“ )

To understand what happened next we need to look to a paper written in 1974 co-authored by  [Robert H. Dennard](https://en.wikipedia.org/wiki/Robert_H._Dennard). Dennardâ€™s Scaling law states roughly that as transistors get smaller their  [power density](https://en.wikipedia.org/wiki/Power_density)  stays constant. Smaller transistors can run at lower voltages, have lower gate capacitance, and switch faster, which helps reduce the amount of dynamic power.

å‚è€ƒ Robert H. Dennard 1974å¹´è®ºæ–‡å¯çŸ¥ã€‚
æ ¹æ® Dennardâ€™s Scaling å®šå¾‹ï¼Œåœ¨æ™¶ä½“ç®¡å°åˆ°ä¸€å®šç¨‹åº¦åï¼Œå…¶ power density ä¿æŒæ’å®šã€‚
æ™¶ä½“ç®¡è¶Šå°ï¼Œæ‰€éœ€è¦çš„ç”µå‹è¶Šä½ï¼Œæ …æç”µå®¹ä¹Ÿè¶Šå°ï¼Œå¹¶ä¸”å¼€å…³é€Ÿåº¦æ›´å¿«ï¼Œè¿™æ ·æ€»çš„åŠ¨æ€åŠŸè€—åè€Œä¼šé™ä½ã€‚

> NOTE: power density (åŠŸç‡å¯†åº¦) watts (ç“¦ç‰¹ åŠŸç‡) hot plate (çƒ­é“æ¿)  nuclear reactor (æ ¸ååº”å †) rocket nozzle (ç«ç®­å–·å°„å™¨)  

So how did that work out?

çœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿ

![Screen Shot 2014 04 14 at 8.49.48 AM](http://semiengineering.com/wp-content/uploads/2014/04/Screen-Shot-2014-04-14-at-8.49.48-AM.png)

It turns out not so great. As the gate length of the transistor approaches the width of a few silicon atom, the relationship between transistor size, voltage, and importantly leakage broke down.

å¹¶éè¿™æ ·ã€‚å½“æ™¶ä½“ç®¡çš„æ …æé•¿åº¦å°åˆ° silicon atom ï¼ˆç¡…åŸå­ï¼‰å®½åº¦æ—¶ã€‚æ™¶ä½“ç®¡å¤§å°ã€ç”µå‹ã€leakageã€€ä¹‹é—´çš„è§„å¾‹å‘ç”Ÿäº†å˜åŒ–ã€‚


It was postulated at the  [Micro-32 conference in 1999](https://pdfs.semanticscholar.org/6a82/1a3329a60def23235c75b152055c36d40437.pdf)  that if we followed the trend line of increasing clock speed and shrinking transistor dimensions then within a processor generation the transistor junction would approach the temperature of the core of a nuclear reactor. Obviously this is was lunacy. The Pentium 4  [marked the end of the line](https://arstechnica.com/uncategorized/2004/10/4311-2/)  for single core, high frequency, consumer CPUs.

åœ¨1999çš„ Micro-32 ä¼šè®®æ›¾ç»æ¨æµ‹ï¼Œå¦‚æœç»§ç»­æé«˜æ—¶é’Ÿé€Ÿåº¦ï¼Œå‡å°æ™¶ä½“ç®¡å°ºå¯¸ï¼ŒæŒ‰ä»¥ä¸Šè¶‹åŠ¿å›¾çš„å‘å±•ï¼Œå°†ç”Ÿäº§å‡ºæ™¶ä½“ç®®ç»“æ¸©åº¦è¾¾åˆ°æ ¸ååº”å †æ¸©åº¦çš„å¤„ç†ç†ã€‚
è¿™æœ‰ç‚¹è’è°¬äº†ã€‚
å•æ ¸å¿ƒã€é«˜é¢‘ç‡çš„æ¶ˆè´¹ç±»CPUå¥”è…¾4å¤„ç†å™¨ï¼Œæ˜¯æœ€åä¸€ä¸ªç¬¦åˆä¸Šé¢è¶‹åŠ¿çº¿çš„CPUã€‚



Returning to this graph, we see that the reason clock speeds have stalled is because cpuâ€™s exceeded our ability to cool them. By 2006 reducing the size of the transistor no longer improved its power efficiency.

æˆ‘ä»¬å†å›è¿‡å¤´è®¨è®ºåˆšæ‰è¿™å¼ å›¾ã€‚
æ—¶é’Ÿé€Ÿåº¦åœæ­¢å¢é•¿çš„ä¸»è¦åŸå› æ˜¯å†·å´ CPU çš„æŠ€æœ¯èƒ½åŠ›è·Ÿä¸ä¸Šã€‚
æ‰€ä»¥åˆ° 2006 å¹´æ—¶ï¼Œå‡å°æ™¶ä½“ç®¡å°ºå¯¸å·²ç»æ— æ³•æé«˜åŠŸç‡äº†ã€‚


We now know that CPU feature size reductions are primarily aimed at reducing power consumption. Reducing power consumption doesnâ€™t just mean â€œgreenâ€, like recycle, save the planet. The primary goal is to keep power consumption, and thus heat dissipation,  [below levels that will damage the CPU](https://en.wikipedia.org/wiki/Electromigration#Practical_implications_of_electromigration).

æˆ‘ä»¬çŸ¥é“ï¼Œå‡å° CPU å°ºå¯¸çš„ä¸»è¦åŸå› æ˜¯ä¸ºäº†é™ä½èƒ½è€—ã€‚
é™ä½èƒ½è€—å¹¶éä¸ºäº†â€œç»¿è‰²ç¯ä¿â€ï¼Œä¸æ˜¯ä¸ºäº†èŠ‚çº¦èµ„æºï¼Œä¿æŠ¤åœ°çƒç¯å¢ƒã€‚
ä¸»è¦ç›®æ ‡åªæ˜¯å°†ä¿æŒèƒ½è€—ï¼Œåœ¨ç°æœ‰æ•£çƒ­èƒ½åŠ›ä¸‹ï¼Œé˜²æ­¢çƒ­é‡è¿‡é«˜æŸåCPUã€‚


![stuttering](https://dave.cheney.net/high-performance-go-workshop/images/stuttering.png)

But, there is one part of the graph that is continuing to increase, the number of transistors on a die. The march of cpu features size, more transistors in the same given area, has both positive and negative effects.

ä½†æ˜¯å›¾ä¸­æ™¶ä½“ç®®çš„æ•°é‡ä»ç„¶åœ¨æŒç»­å¢åŠ ã€‚
CPUå°ºå¯¸å˜å¤§ï¼Œå…¶ä¸­å°±èƒ½æ”¾æ›´å¤šæ™¶ä½“ç®®ã€‚è¿™å³æœ‰æ­£é¢å½±å“ä¹Ÿæœ‰è´Ÿé¢å½±å“ã€‚

Also, as you can see in the insert, the cost per transistor continued to fall until around 5 years ago, and then the cost per transistor started to go back up again.

ä¸Šå›¾ä¸­å·¦ä¸Šè§’çš„æ’å›¾æ˜¾ç¤ºï¼Œæ™¶ä½“ç®¡å•ä»·åœ¨2012å¹´å‰ä¸€ç›´åœ¨ä¸‹é™ï¼Œéšå2015å¹´å•ä»·åˆå¼€å§‹ä¸Šé•¿ã€‚

> NOTE: å›¾ä¸­æ˜¾ç¤ºçš„æ˜¯1ç¾å…ƒèƒ½è´­ä¹°çš„æ™¶ä½“ç®¡æ•°é‡ï¼Œæ‰€ä»¥è·Ÿå•ä»·è¶‹åŠ¿åˆšå¥½ç›¸åã€‚



![moores law](https://whatsthebigdata.files.wordpress.com/2016/08/moores-law.png)

Not only is it getting more expensive to create smaller transistors, itâ€™s getting harder. This report from 2016 shows the prediction of what the chip makers believed would occur in 2013; two years later they had missed all their predictions, and while I donâ€™t have an updated version of this report, there are no signs that they are going to be able to reverse this trend.

åˆ¶é€ æ›´å°çš„æ™¶ä½“ç®¡ä¸ä»…æ›´è´µäº†ï¼Œä¹Ÿæ›´éš¾äº†ã€‚
ä¸Šé¢2016å¹´çš„æŠ¥å‘Šæ˜¾ç¤ºï¼ŒèŠ¯ç‰‡åˆ¶é€ å•†é¢„æµ‹2013å¹´ physical gate length ä» 20 nanometersã€€å¼€å§‹æ¯å¹´å‡å° 2 nanometer ã€‚
ä½†å†çœ‹ä¸¤å¹´å2015çš„é¢„æµ‹å›¾å‘ç°ï¼Œå®ƒä»¬æ˜¾ç¤ºæ²¡æœ‰è¾¾åˆ°ä¹‹å‰é¢„æœŸã€‚
è™½ç„¶æˆ‘æ²¡æœ‰ç›¸å…³æŠ¥å‘Šçš„æœ€æ–°ç‰ˆæœ¬ï¼Œä½†æ²¡æœ‰è¿¹è±¡è¡¨æ˜è°èƒ½æ‰­è½¬è¿™ä¸€è¶‹åŠ¿ã€‚

It is costing intel, TSMC, AMD, and Samsung billions of dollars because they have to build new fabs, buy all new process tooling. So while the number of transistors per die continues to increase, their unit cost has started to increase.

è‹±ç‰¹å°”ï¼Œå°ç§¯ç”µï¼ŒAMDå’Œä¸‰æ˜Ÿç­‰å‚å•†åœ¨å»ºå‚ï¼Œè´­ç½®ç”Ÿäº§å·¥å…·çš„èŠ±è´¹é«˜è¾¾æ•°åäº¿ç¾å…ƒã€‚
å³ä½¿å•ä¸ªèŠ¯ç‰‡çš„æ™¶ä½“ç®¡æ•°é‡åœ¨å¢åŠ ï¼Œè¿™äº›èŠ¯ç‰‡çš„å•ä½æˆæœ¬ä¹Ÿä»ç„¶åœ¨ä¸Šæ¶¨ã€‚


> Even the term gate length, measured in nano meters, has become ambiguous. Various manufacturers measure the size of their transistors in different ways allowing them to demonstrate a smaller number than their competitors without perhaps delivering. This is the Non-GAAP Earning reporting model of CPU manufacturers.

> æœ¯è¯­ gate length (æ …æé•¿åº¦)çš„å•ä½ nano meterï¼ˆçº³ç±³ï¼‰å®šä¹‰ä¹Ÿæœ‰äº›æ¨¡ç³Šã€‚
> æ¯ä¸ªå‚å•†æµ‹é‡æ™¶ä½“ç®¡å¤§å°çš„æ–¹å¼éƒ½ä¸ä¸€æ ·ï¼Œæ‰€ä»¥å®ƒä»¬æ€»èƒ½å±•ç¤ºå‡ºæ¯”ç«å‚æ›´å°å°ºå¯¸çš„æ ·å“ã€‚
> è¿™æ˜¯CPUåˆ¶é€ å•† Non-GAAP æ”¶ç›ŠæŠ¥å‘Šæ¨¡å‹ã€‚


### 1.7. More cores

![y5cdp7nhs2uy](https://i.redd.it/y5cdp7nhs2uy.jpg)

With thermal and frequency limits reached itâ€™s no longer possible to make a single core run twice as fast. But, if you add another cores you can provide twice the processing capacity â€” if the software can support it.

ç”±äºæ¸©åº¦å’Œé¢‘ç‡çš„é™åˆ¶ï¼Œæƒ³è®©å•æ ¸å¿ƒè¿è¡Œé€Ÿåº¦å˜å¿«ä¸¤å€å·²ç»ä¸å¤ªå®¹æ˜“äº†ã€‚
ä½†æ˜¯ï¼Œå‡å¦‚è½¯ä»¶èƒ½åŒæ—¶åˆ©ç”¨å¥½ä¸¤ä¸ª core ï¼Œé‚£åªè¦å†åŠ ä¸€ä¸ª CPU å°±èƒ½è½»æ¾è®©è¿è¡Œé€Ÿåº¦å¿«ä¸¤å€ã€‚

In truth, the core count of a CPU is dominated by heat dissipation. The end of Dennard scaling means that the clock speed of a CPU is some arbitrary number between 1 and 4 Ghz depending on how hot it is. Weâ€™ll see this shortly when we talk about benchmarking.

CPU çš„æ ¸å¿ƒæ•°é‡ä¸»è¦ç”±æ•£çƒ­æƒ…å†µå†³å®šã€‚
ç”± Dennardâ€™s Scaling å®šå¾‹å¯çŸ¥ï¼ŒCPU æ—¶é’Ÿé€Ÿåº¦è‚¯å®šæ˜¯åœ¨ 1 åˆ° 4 Ghz ä¹‹é—´ï¼Œå…·ä½“å¤§å°ç”±å®ƒçš„çƒ­åº¦å†³å®šã€‚
ä¸€ä¼šè®¨è®ºåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å°±èƒ½çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚


### 1.8. Amdahlâ€™s law (é˜¿å§†è¾¾å°”å®šå¾‹)

CPUs are not getting faster, but they are getting wider with hyper threading and multiple cores. Dual core on mobile parts, quad core on desktop parts, dozens of cores on server parts. Will this be the future of computer performance? Unfortunately not.

CPUè™½ç„¶æ²¡æœ‰å˜å¾—æ›´å¿«ï¼Œä½†ç”±äºè¶…çº¯ç§å’Œå¤šæ ¸å¿ƒæŠ€æœ¯çš„å‘å±•ï¼ŒCPUå˜çš„æ›´â€œå®½â€äº†ã€‚
åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä½¿ç”¨åŒæ ¸å¤„ç†å™¨ï¼Œæ¡Œé¢è®¾å¤‡ä¸Šä½¿ç”¨å››æ ¸å¤„ç†å™¨ï¼Œåœ¨æœåŠ¡å™¨ä¸­ä½¿ç”¨å‡ åæ ¸å¿ƒçš„å¤„ç†å™¨ã€‚
åœ¨ä»¥åçš„æ—¥å­ï¼Œåªè¦å¢åŠ æ ¸å¿ƒæ•°é‡ ï¼Œè®¡ç®—æœºæ€§èƒ½å°±èƒ½ä¸€ç›´æå‡å—ï¼Ÿ
å½“ç„¶ä¸å¯èƒ½äº†ã€‚

Amdahlâ€™s law, named after the Gene Amdahl the designer of the IBM/360, is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved.

Amdahl å®šå¾‹ï¼Œæ˜¯ä»¥ IBM/360 çš„è®¾è®¡å¸ˆ Gene Amdahl åå­—å‘½åçš„ã€‚
æ­¤å®šå¾‹ä¸­çš„å…¬å¼èƒ½è®¡ç®—å‡ºï¼Œåœ¨ä»»åŠ¡å·¥ä½œé‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œèƒ½æ— é™å¢åŠ ç³»ç»Ÿèµ„æºï¼Œæœ€å¿«èƒ½æå‰å¤šä¹…å®Œæˆä»»åŠ¡ã€‚

> NOTE: â€œæå‰å¤šä¹…å®Œæˆä»»åŠ¡â€ è¡¨è¾¾çš„å«ä¹‰ä¸ â€œèƒ½æå‡å·¥ä½œæ•ˆç‡å¤šå°‘å€â€ â€œæé€Ÿå¤šå°‘å€â€ ä¸€æ ·ã€‚

![AmdahlsLaw](https://upload.wikimedia.org/wikipedia/commons/e/ea/AmdahlsLaw.svg)

Amdahlâ€™s law tells us that the maximum speedup of a program is limited by the sequential parts of the program. If you write a program with 95% of its execution able to be run in parallel, even with thousands of processors the maximum speedup in the programs execution is limited to 20x.

Amdahl å®šå¾‹å‘Šè¯‰æˆ‘ä»¬ï¼Œèƒ½æé€Ÿå¤šå°‘ï¼Œå–å†³äºç¨‹åºå½“ä¸­èƒ½å¤Ÿé¡ºåºæ‰§è¡Œçš„éƒ¨åˆ†æœ‰å¤šå°‘ã€‚
å‡è®¾ä½ çš„ç¨‹åºä¸­æœ‰ 95% çš„ä»£ç éƒ½èƒ½å¹¶è¡Œæ‰§è¡Œï¼Œå³ä½¿æœ‰ä¸Šåƒä¸ªå¤„ç†å™¨ï¼Œæœ€å¤šä¹Ÿåªèƒ½æé€Ÿ 20 å€ã€‚

Think about the programs that you work on every day, how much of their execution is parralisable?

æƒ³æƒ³ä½ æ¯å¤©å†™çš„ç¨‹åºï¼Œå®ƒä»¬å½“ä¸­æœ‰å¤šå°‘æ˜¯å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„å‘¢ï¼Ÿ


> TODO è¶…çº¿ç¨‹ã€æ•´æ•°è¿ç®—å•å…ƒã€æµ®ç‚¹æ•°è¿ç®—å•å…ƒ integer performance  floating point performance
> 
> å‚è€ƒé“¾æ¥ï¼šhttps://www.zhihu.com/question/20277695/answer/14588735
>
> Intelçš„è¶…çº¿ç¨‹æŠ€æœ¯ï¼Œç›®çš„æ˜¯ä¸ºäº†æ›´å……åˆ†åœ°åˆ©ç”¨ä¸€ä¸ªå•æ ¸CPUçš„èµ„æºã€‚CPUåœ¨æ‰§è¡Œä¸€æ¡æœºå™¨æŒ‡ä»¤æ—¶ï¼Œå¹¶ä¸ä¼šå®Œå…¨åœ°åˆ©ç”¨æ‰€æœ‰çš„CPUèµ„æºï¼Œè€Œä¸”å®é™…ä¸Šï¼Œæ˜¯æœ‰å¤§é‡èµ„æºè¢«é—²ç½®ç€çš„ã€‚è¶…çº¿ç¨‹æŠ€æœ¯å…è®¸ä¸¤ä¸ªçº¿ç¨‹åŒæ—¶ä¸å†²çªåœ°ä½¿ç”¨CPUä¸­çš„èµ„æºã€‚æ¯”å¦‚ä¸€æ¡æ•´æ•°è¿ç®—æŒ‡ä»¤åªä¼šç”¨åˆ°æ•´æ•°è¿ç®—å•å…ƒï¼Œæ­¤æ—¶æµ®ç‚¹è¿ç®—å•å…ƒå°±ç©ºé—²äº†ï¼Œè‹¥ä½¿ç”¨äº†è¶…çº¿ç¨‹æŠ€æœ¯ï¼Œä¸”å¦ä¸€ä¸ªçº¿ç¨‹åˆšå¥½æ­¤æ—¶è¦æ‰§è¡Œä¸€ä¸ªæµ®ç‚¹è¿ç®—æŒ‡ä»¤ï¼ŒCPUå°±å…è®¸å±äºä¸¤ä¸ªä¸åŒçº¿ç¨‹çš„æ•´æ•°è¿ç®—æŒ‡ä»¤å’Œæµ®ç‚¹è¿ç®—æŒ‡ä»¤åŒæ—¶æ‰§è¡Œï¼Œè¿™æ˜¯çœŸçš„å¹¶è¡Œã€‚æˆ‘ä¸äº†è§£å…¶å®ƒçš„ç¡¬ä»¶å¤šçº¿ç¨‹æŠ€æœ¯æ˜¯æ€ä¹ˆæ ·çš„ï¼Œä½†å•å°±è¶…çº¿ç¨‹æŠ€æœ¯è€Œè¨€ï¼Œå®ƒæ˜¯å¯ä»¥å®ç°çœŸæ­£çš„å¹¶è¡Œçš„ã€‚ä½†è¿™ä¹Ÿå¹¶ä¸æ„å‘³ç€ä¸¤ä¸ªçº¿ç¨‹åœ¨åŒä¸€ä¸ªCPUä¸­ä¸€ç›´éƒ½å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œåªæ˜¯æ°å¥½ç¢°åˆ°ä¸¤ä¸ªçº¿ç¨‹å½“å‰è¦æ‰§è¡Œçš„æŒ‡ä»¤ä¸ä½¿ç”¨ç›¸åŒçš„CPUèµ„æºæ—¶æ‰å¯ä»¥çœŸæ­£åœ°å¹¶è¡Œæ‰§è¡Œã€‚




### 1.9. Dynamic Optimisations åŠ¨æ€ä¼˜åŒ–

With clock speeds stalled and limited returns from throwing extra cores at the problem, where are the speedups coming from? They are coming from architectural improvements in the chips themselves. These are the big five to seven year projects with names like  [Nehalem, Sandy Bridge, and Skylake](https://en.wikipedia.org/wiki/List_of_Intel_CPU_microarchitectures#Pentium_4_/_Core_Lines).

æ—¢ç„¶æ—¶é’Ÿé€Ÿåº¦åœæ»ä¸å‰ï¼Œé€šè¿‡å¢åŠ  CPU core æ•°é‡å¸¦æ¥çš„æé€Ÿåˆååˆ†æœ‰é™ï¼Œé‚£ä¹ˆè¿‘å¹´æ¥çš„æ€§èƒ½æå‡åˆæ¥è‡ªå“ªé‡Œå‘¢ï¼Ÿ
è¿™ä¸»è¦æ˜¯ç”±äºèŠ¯ç‰‡æœ¬èº«çš„æ¶æ„æ”¹è¿›ã€‚åƒ Nehalem, Sandy Bridge, Skylake è¿™äº›å¾®å¤„ç†å™¨æ¶æ„é¡¹ç›®ä¸€èˆ¬éƒ½è¦æŒä¹…äº”åˆ°ä¸ƒå¹´æ—¶é—´ã€‚

Much of the improvement in performance in the last two decades has come from architectural improvements:

å¯ä»¥è¯´ï¼Œè¿‡å»äºŒåå¹´é—´çš„æ€§èƒ½æå‡å¤§éƒ½æ¥æºäºæ¶æ„çš„æ”¹è¿›ã€‚



#### 1.9.1. Out of order execution ä¹±åºæ‰§è¡Œ

Out of Order, also known as super scalar, execution is a way of extracting so called  _Instruction level parallelism_  from the code the CPU is executing. Modern CPUs effectively do SSA at the hardware level to identify data dependencies between operations, and where possible run independent instructions in parallel.

ä¹±åºï¼Œä¹Ÿç§°ä¸ºè¶…æ ‡é‡ï¼Œæ˜¯ä¸€ç§èƒ½åœ¨è¿è¡Œä¸­çš„ CPU ä»£ç ä¸­ï¼Œæ‰§è¡Œè¡ŒæŒ‡ä»¤çº§å¹¶è¡Œä¼˜åŒ–çš„æ–¹æ³•ã€‚
ç°ä»£ CPU èƒ½é«˜æ•ˆæ‰§è¡Œ SSA è¿‡ç¨‹ï¼Œå› ä¸ºå®ƒèƒ½åœ¨ç¡¬ä»¶å±‚è¯†åˆ«å„ç§æ•°æ®æ“ä½œä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å°½å¯èƒ½å¹¶è¡Œæ‰§è¡Œã€‚

TODO [SSA Static single assignment é™æ€å•èµ‹å€¼](https://en.wikipedia.org/wiki/Static_single_assignment_form)


However there is a limit to the amount of parallelism inherent in any piece of code. Itâ€™s also tremendously power hungry. Most modern CPUs have settled on six execution units per core as there is an n squared cost of connecting each execution unit to all others at each stage of the pipeline.

ä½†æ˜¯æ¯æ®µä»£ç çš„å¹¶è¡Œé‡æ˜¯æœ‰é™çš„ã€‚è€Œä¸”ååˆ†è´¹ç”µã€‚
ç°ä»£CPUä¸­ï¼Œæ¯ä¸ª core é…ç½®å…­ä¸ªæ‰§è¡Œå•å…ƒï¼Œåœ¨æ‰§è¡ŒæŒ‡ä»¤æµæ°´çº¿çš„è¿‡ç¨‹ä¸­ï¼Œå°†æ¯ä¸ªæ‰§è¡Œå•å…ƒä¸å…¶ä»–æ‰§è¡Œå•å…ƒè¿æ¥åˆ°ä¸€èµ·çš„æˆæœ¬æ˜¯ N^2 ã€‚

#### 1.9.2. Speculative execution é¢„æµ‹æ‰§è¡Œ

Save the smallest micro controllers, all CPUs utilise an  _instruction pipeline_  to overlap parts of in the instruction fetch/decode/execute/commit cycle.

é™¤äº†æœ€å°çš„å¾®å‹æ§åˆ¶å™¨å¤–ï¼Œæ‰€æœ‰çš„ CPU éƒ½èƒ½åœ¨æ‰§è¡Œ fetch/decode/execute/commit æŒ‡ä»¤å‘¨æœŸçš„è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ _æŒ‡ä»¤æµæ°´çº¿_ é‡å ï¼ˆå¹¶è¡Œï¼‰æ‰§è¡Œå…¶ä¸­éƒ¨åˆ†æŒ‡ä»¤ã€‚

> NOTE [What are the smallest microcontrollers?](https://electronics.stackexchange.com/questions/84800/what-are-the-smallest-microcontrollers)


![800px Fivestagespipeline](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Fivestagespipeline.png/800px-Fivestagespipeline.png)

The problem with an instruction pipeline is branch instructions, which occur every 5-8 instructions on average. When a CPU reaches a branch it cannot look beyond the branch for additional instructions to execute and it cannot start filling its pipeline until it knows where the program counter will branch too. Speculative execution allows the CPU to "guess" which path the branch will take  _while the branch instruction is still being processed!_

å¯æ˜¯ï¼ŒæŒ‡ä»¤æµæ°´çº¿çš„åˆ†æ”¯æŒ‡ä»¤å¹³å‡æ¯ 5-8 ä¸ªæŒ‡ä»¤å‘¨æœŸæ‰ä¼šæ‰§è¡Œä¸€æ¬¡ã€‚
å½“ CPU è¾¾åˆ°åˆ†æ”¯æ—¶ï¼Œå®ƒä¸èƒ½ä»å½“å‰åˆ†æ”¯ä¹‹å¤–å¯»æ‰¾è¦æ‰§è¡Œçš„æŒ‡ä»¤ï¼Œå¿…é¡»ä» program counter è·å–åˆ°ä¸‹ä¸€ä¸ªè¦åˆ‡æ¢çš„åˆ†æ”¯åï¼Œæ‰èƒ½å¡«å……æµæ°´çº¿ã€‚
é¢„æµ‹æ‰§è¡ŒåŠŸèƒ½ï¼Œèƒ½è®© CPU åœ¨æ‰§è¡Œ åˆ†æ”¯æŒ‡ä»¤ çš„è¿‡ç¨‹ä¸­ï¼ŒçŒœæµ‹ä¸‹ä¸€æ¬¡æ‰§è¡Œçš„åˆ†æ”¯è·¯å¾„ã€‚

> NOTE Program counter ç¨‹åºè®¡æ•°å™¨ï¼Œä¹Ÿå«æŒ‡ä»¤æŒ‡é’ˆï¼Œç”¨äºä¿å­˜ç¨‹åºä¸‹ä¸€æ¬¡è¦æ‰§è¡Œçš„æŒ‡ä»¤ã€‚
> A program counter is a register in a computer processor that contains the address (location) of the instruction being executed at the current time. As each instruction gets fetched, the program counter increases its stored value by 1.
> [What is program counter? And how it work?](https://www.quora.com/What-is-program-counter-And-how-it-work)


If the CPU predicts the branch correctly then it can keep its pipeline of instructions full. If the CPU fails to predict the correct branch then when it realises the mistake it must roll back any change that were made to its  _architectural state_. As weâ€™re all learning through Spectre style vulnerabilities, sometimes this rollback isnâ€™t as seamless as hoped.

å¦‚æœ CPU é¢„æµ‹åˆ°æ­£ç¡®çš„åˆ†æ”¯ï¼Œå°±èƒ½ä¿æŒå®ƒçš„åˆ†æ”¯æµæ°´çº¿ä¸€ç›´æ˜¯æ»¡çš„ã€‚
å¦‚æœ CPU é¢„æµ‹é”™äº†åˆ†æ”¯ï¼Œå®ƒå°±å¿…é¡»åœ¨å‘ç°é”™è¯¯æ—¶ï¼Œç«‹å³å›æ»šä¹‹å‰å¯¹ _architectural state_ çš„æ”¹åŠ¨ã€‚
åƒæˆ‘ä»¬åœ¨ Spectre style vulnerabilities å­¦ä¹ åˆ°çš„é‚£æ ·ï¼Œæœ‰æ—¶è¿™ç§å›æ»šå¹¶éæ— ç¼çš„ã€‚


Speculative execution can be very power hungry when branch prediction rates are low. If the branch is misprediction, not only must the CPU backtrace to the point of the misprediction, but the energy expended on the incorrect branch is wasted.

å¦‚æœåˆ†æ”¯é¢„æµ‹çš„æ­£ç¡®ç‡å¾ˆä½æ—¶ï¼Œæ˜¯ååˆ†è´¹ç”µçš„ã€‚
åˆ†æ”¯é¢„æµ‹å¤±è´¥æ—¶ï¼Œä¸ä»… CPU è¦å›æº¯åˆ°ä¹‹å‰çš„çŠ¶æ€ï¼ŒèŠ±åœ¨åˆ†æ”¯åœ¨çš„èƒ½é‡ä¹Ÿæµªè´¹äº†ã€‚


All these optimisations lead to the improvements in single threaded performance weâ€™ve seen, at the cost of huge numbers of transistors and power.

è¿™äº›åœ¨å•çº¿ç¨‹æ€§èƒ½ä¸Šçš„æå‡ï¼Œéƒ½æ˜¯ä»¥æ¶ˆè€—å¤§é‡æ™¶ä½“ç®¡å’Œç”µåŠ›ä¸ºä»£ä»·çš„ã€‚

> Cliff Click has a  [wonderful presentation](https://www.youtube.com/watch?v=OFgxAFdxYAQ)  that argues out of order and speculative execution is most useful for starting cache misses early thereby reducing observed cache latency.

> Cliff Click æœ‰ä¸€ä¸ªç²¾å½©çš„æ¼”ç¤ºï¼Œè®ºè¯äº† ä¹±åºæ‰§è¡Œå’Œåˆ†æ”¯é¢„æµ‹ èƒ½é™ä½ cache latency .

TODO starting cache misses early å¦‚ä½•ç†è§£ï¼Ÿ å› ä¸ºæå‰é¢„æµ‹æ‰§è¡Œäº†ä»£ç ï¼Œæ‰€ä»¥æŠŠç›¸å…³æ•°æ®å’Œä»£ç ä¹Ÿéƒ½ç¼“å­˜åˆ° L1 L2 L3 ç­‰å¤šçº§ç¼“å­˜ä¸­è¾ƒè¿‘çš„ä¸€çº§ä¸­äº†ã€‚æ‰€ä»¥å½“çœŸæ­£ä½¿ç”¨ç›¸å…³æŒ‡ä»¤æ—¶ï¼Œä¹Ÿå°±èƒ½æ›´å¿«æ‰¾åˆ°æ‰€éœ€è¦çš„æ•°æ®å’ŒæŒ‡ä»¤ï¼Œè¿›è€Œå‡å°‘ cache latency ã€‚

TODO [CPU ä¸ºä½•éå¾—è¦ç”¨ä¹±åºæ‰§è¡Œå’Œé¢„æµ‹æ‰§è¡Œå‘¢ï¼Ÿ](https://www.v2ex.com/t/420690)

TODO [CPU Cache æœºåˆ¶ä»¥åŠ Cache miss](https://www.cnblogs.com/jokerjason/p/10711022.html)

NOTE CoolShell CPU Cache [^CPUCache]


### 1.10. Modern CPUs are optimised for bulk operations ç°ä»£ CPU å·²é’ˆå¯¹æ‰¹é‡æ“ä½œè¿›è¡Œäº†ä¼˜åŒ–

> Modern processors are a like nitro fuelled funny cars, they excel at the quarter mile. Unfortunately modern programming languages are like Monte Carlo, they are full of twists and turns.â€‰â€”â€‰David Ungar

> ç°ä»£å¤„ç†å™¨å°±åƒæ˜¯ç¡åŸºç‡ƒæ–™çš„æ±½è½¦ï¼Œå®ƒä»¬åœ¨å››åˆ†ä¹‹ä¸€è‹±é‡Œå¤„è¡¨ç°å‡ºè‰²ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°ä»£ç¼–ç¨‹è¯­è¨€å°±åƒè’™ç‰¹å¡æ´›ä¸€æ ·ï¼Œå……æ»¡äº†æ›²æŠ˜ã€‚â€”å¤§å«Â·æ˜‚åŠ ï¼ˆDavid Ungarï¼‰

NOTE [Monte Carlo è’™ç‰¹å¡ç½—æ–¹æ³•æ˜¯ä¸€ç§è®¡ç®—æ–¹æ³•ã€‚åŸç†æ˜¯é€šè¿‡å¤§é‡éšæœºæ ·æœ¬ï¼Œå»äº†è§£ä¸€ä¸ªç³»ç»Ÿï¼Œè¿›è€Œå¾—åˆ°æ‰€è¦è®¡ç®—çš„å€¼ã€‚](http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html)

This a quote from David Ungar, an influential computer scientist and the developer of the SELF programming language that was referenced in a very old presentation [I found online](http://www.ai.mit.edu/projects/dynlangs/wizards-panels.html).

è¿™å¥è¯å¼•ç”¨è‡ªæœ‰å½±å“åŠ›çš„è®¡ç®—æœºç§‘å­¦å®¶ï¼ŒSELFç¼–ç¨‹è¯­è¨€çš„å¼€å‘äººå‘˜ David Ungar ï¼Œæˆ‘åœ¨ç½‘ä¸Šæ‰¾åˆ°äº†ä¸€ä¸ªå¾ˆæ—§çš„æ¼”ç¤ºæ–‡ç¨¿ï¼Œå¹¶å¼•ç”¨äº†è¯¥å¼•ç”¨ã€‚

Thus, modern CPUs are optimised for bulk transfers and bulk operations. At every level, the setup cost of an operation encourages you to work in bulk. Some examples include

-   memory is not loaded per byte, but per multiple of cache lines, this is why alignment is becoming less of an issue than it was in earlier computers.
    
-   Vector instructions like MMX and SSE allow a single instruction to execute against multiple items of data concurrently providing your program can be expressed in that form.
    
ç°ä»£ CPU é’ˆå¯¹æ‰¹é‡ä¼ è¾“å’Œè¿ç®—æ“ä½œè¿›è¡Œäº†ä¼˜åŒ–ã€‚
å»ºè®®å°½é‡æŠŠæ“ä½œåˆå¹¶åˆ°ä¸€æ¬¡è°ƒç”¨æ¥æ‰§è¡Œã€‚

- å†…å­˜æ˜¯æŒ‰ç¼“å­˜è¡Œçš„å€æ•°åŠ è½½ï¼Œä¸å†åƒä»¥å‰ä¸€æ ·æŒ‰å­—èŠ‚åŠ è½½ã€‚
- ç±»ä¼¼ MMX SSE ç­‰å‘é‡æŒ‡ä»¤å…è®¸ä¸€æ¡æŒ‡ä»¤åŒæ—¶é’ˆå¯¹å¤šä¸ªæ•°æ®é¡¹å¹¶å‘æ‰§è¡Œã€‚å½“ç„¶ï¼Œè¿™ä¹Ÿéœ€è¦ä¸Šå±‚ç¨‹åºæ”¯æŒã€‚



### 1.11. Modern processors are limited by memory latency not memory capacity ç°ä»£å¤„ç†å™¨çš„ä¸»è¦ç“¶é¢ˆåœ¨å†…å­˜å»¶è¿Ÿï¼Œè€Œéå†…å­˜å¤§å°

If the situation in CPU land wasnâ€™t bad enough, the news from the memory side of the house doesnâ€™t get much better.

å¦‚æœ CPU è´Ÿè½½ä¸æ˜¯ç‰¹åˆ«é«˜ï¼Œé‚£å†…å­˜å»¶è¿Ÿçš„å½±å“ä¹Ÿå°±æ²¡é‚£ä¹ˆå¤§ã€‚

Physical memory attached to a server has increased geometrically. My first computer in the 1980â€™s had kilobytes of memory. When I went through high school I wrote all my essays on a 386 with 1.8 megabytes of ram. Now its commonplace to find servers with tens or hundreds of gigabytes of ram, and the cloud providers are pushing into the terabytes of ram.

æœåŠ¡å™¨ä¸Šçš„ç‰©ç†å†…å­˜å·²ç»ç¨‹å‡ ä½•çº§æ•°å¢é•¿ã€‚
æˆ‘åœ¨1980å¹´çš„ç¬¬ä¸€å°ç”µè„‘åªæœ‰å‡ åƒå­—èŠ‚å†…å­˜ã€‚
é«˜ä¸­æ—¶æ‰€æœ‰è®ºæ–‡éƒ½æ˜¯åœ¨åªæœ‰ 1.8 MB å†…å­˜çš„ 386 æœºå™¨ä¸Šç¼–å†™ã€‚
ç°åœ¨å¾ˆå®¹æ˜“æ‰¾åˆ°æ‹¥æœ‰å‡ åä¸Šç™¾ GB å†…åœ¨çš„æœåŠ¡å™¨ï¼Œäº‘æœåŠ¡æä¾›å•†ç”šè‡³ä½¿ç”¨äº† TB å¤§å°çš„å†…å­˜ã€‚

> NOTE gemoetrically â€œå‡ ä½•çº§æ•°â€ï¼Œåˆç§°â€œç­‰æ¯”çº§æ•°â€ã€‚è·Ÿç®—æ³•è¯¾ç¨‹ä¸­çš„â€œå¤æ‚åº¦é‡çº§â€ä¸åŒã€‚

> å¸¸é‡é˜¶O(1) å¯¹æ•°é˜¶O(logn) çº¿æ€§é˜¶O(n) çº¿æ€§å¯¹æ•°é˜¶O(n logn) å¹³æ–¹é˜¶O(n^2) ç«‹æ–¹é˜¶O(n^3) kæ¬¡æ–¹é˜¶O(n^k) æŒ‡æ•°é˜¶O(2^n) é˜¶ä¹˜é˜¶O(n!) ã€‚
> [æ•°æ®ç»“æ„ä¸ç®—æ³•ä¹‹ç¾](https://time.geekbang.org/column/article/40036)


![mem gap](https://www.extremetech.com/wp-content/uploads/2018/01/mem_gap.png)

However, the gap between processor speeds and memory access time continues to grow.

å¤„ç†å™¨é€Ÿåº¦ä¸å†…å­˜è®¿é—®å»¶è¿Ÿä¹‹é—´çš„å·®å¼‚è¶Šæ¥è¶Šå¤§ã€‚ï¼ˆä¸Šå›¾ä¸­2000åˆ°2010å¹´é—´ï¼ŒCPUé€Ÿåº¦æé«˜10å€é€Ÿï¼ŒMemoryè®¿é—®å»¶è¿Ÿä»…æé«˜2å€ï¼‰

[Table 2.2 Example Time Scale of System Latencies](https://pbs.twimg.com/media/BmBr2mwCIAAhJo1.png)

Event        | Latency | Scaled |
------------ | ---------------- |
1 CPU cycle  | 0.3 ns  | 1s     |
Level 1 cache access | 0.9 ns | 3s |
Level 2 cache access | 2.8 ns | 9s |
Level 3 cache access | 12.9 ns | 43 s |
Main memory access (DRAM, from CPU) | 120 ns | 6 min |
Solid-state disk I/O (flash memory) | S0- -150 us | 2- -6 days |
Rotational disk I/O | 1-10 ms | 1-12 months |
Internet: San Francisco to New York | 40 ms | 4 years |
Internet: San Francisco to United Kingdom | 81 ms | 8 years |
Internet: San Francisco to Australia | 183 ms | 19 years |
TCP packet retransmit | 1-3 s | 105- 317 years |
OS virtualization system reboot | 4S | 423 years |
SCSI command time-out | 30 s | 3 millennia |
Hardware (HW) virtualization system reboot | 40 s | 4 millennia |
Physical system reboot | 5m | 32 millennia |



But, in terms of processor cycles lost waiting for memory, physical memory is still as far away as ever because memory has not kept pace with the increases in CPU speed.

Memory è·Ÿä¸ä¸Š CPU é€Ÿåº¦å¿«é€Ÿå¢é•¿çš„æ­¥ä¼ï¼Œæ‰€ä»¥å¤„ç†å™¨è¦ç©ºè½¬å¤šä¸ªæ—¶é’Ÿå‘¨æœŸæ¥ç­‰å¾…è®¿é—®å†…å­˜çš„è¿‡ç¨‹ã€‚

So, most modern processors are limited by memory latency not capacity.

æ‰€ä»¥è¯´ï¼Œç°ä»£å¤„ç†å™¨ä¸»è¦ç“¶é¢ˆåœ¨å†…å­˜å»¶è¿Ÿï¼Œè€Œä¸æ˜¯å†…å­˜å¤§å°ã€‚



### 1.12. Cache rules everything around è‡³å…³é‡è¦çš„ç¼“å­˜

![latency](https://www.extremetech.com/wp-content/uploads/2014/08/latency.png)

> data access range  æ•°æ®è®¿é—®èŒƒå›´ï¼Œå¤§å°

> memory latency å†…å­˜å»¶è¿Ÿ


For decades the solution to the processor/memory cap was to add a cache-- a piece of small fast memory located closer, and now directly integrated onto, the CPU.

è¿‘å‡ åå¹´æ¥ï¼Œæå‡å¤„ç†å™¨/å†…å­˜ç“¶é¢ˆçš„ä¸»è¦æ–¹æ³•å°±æ˜¯åŠ ç¼“å­˜ ï¼ï¼ æœ€å¼€å§‹æ˜¯åœ¨CPUé™„è¿‘å¢åŠ ä¸€å°å—é«˜é€Ÿå†…å­˜ï¼Œç°åœ¨ç›´æ¥æŠŠé«˜é€Ÿå†…å­˜é›†æˆåˆ°CPUå†…äº†ã€‚

But;

-   L1 has been stuck at 32kb per core for decades
    
-   L2 has slowly crept up to 512kb on the largest intel parts
    
-   L3 is now measured in 4-32mb range, but its access time is variable


ä½†æ˜¯ï¼š

- ä¸€çº§ç¼“å­˜ä¸€ç›´ä¿æŒåœ¨æ¯æ ¸å¿ƒ 32kb å¤§å°ï¼Œå‡ åå¹´æ²¡ä»€ä¹ˆå˜åŒ–

- äºŒçº§ç¼“å­˜å¢é•¿ç¼“æ…¢ï¼Œåœ¨ interl ä¸­æœ€å¤§ 512kb

- ä¸‰çº§ç¼“å­˜åœ¨ 4-32mb ä¹‹é—´ï¼Œä½†å®ƒçš„è®¿é—®æ—¶é—´æ˜¯å˜åŒ–çš„

> TODO ä¸‰çº§ç¼“å­˜è®¿é—®æ—¶é—´å¯å˜ï¼Œæ˜¯æŒ‡ä¸åŒå‚å•†ä½¿ç”¨çš„ä¸‰çº§ç¼“å­˜é€Ÿåº¦ä¸åŒï¼Ÿè¿˜æ˜¯è¯´ä¸å‡¬æ—¶é—´ç‚¹è®¿é—®ä¸‰çº§ç¼“å­˜æ‰€éœ€è¦çš„æ—¶é—´æ˜¯å˜åŒ–çš„ï¼Ÿ
    

![E5v4blockdiagram](https://i3.wp.com/computing.llnl.gov/tutorials/linux_clusters/images/E5v4blockdiagram.png)

By caches are limited in size because they are  [physically large on the CPU die](http://www.itrs.net/Links/2000UpdateFinal/Design2000final.pdf), consume a lot of power. To halve the cache miss rate you must  _quadruple_  the cache size.

å› ä¸ºè¿™å—é«˜é€Ÿå†…å­˜å ç”¨CPUçš„ç‰©ç†ç©ºé—´å¤ªå¤šï¼Œä¸”æ¶ˆè´¹ç”µé‡è¾ƒé«˜ï¼Œæ‰€ä»¥ç¼“å­˜ä¸€ç›´ä¸å¤§ã€‚
å¦‚æœæƒ³è®©ç¼“å­˜ä¸¢å¤±ç‡å‡å°ä¸€åŠï¼Œè‡³å°‘è¦è®©ç¼“å­˜å¤§å°å¢åŠ å››å€ã€‚




### 1.13. The free lunch is over å…è´¹åˆé¤çš„æ—¶ä»£ç»“æŸäº†

In 2005 Herb Sutter, the C++ committee leader, wrote an article entitled  [The free lunch is over](http://www.gotw.ca/publications/concurrency-ddj.htm). In his article Sutter discussed all the points I covered and asserted that future programmers will not longer be able to rely on faster hardware to fix slow programs or slow programming languages.

 C+ï¼‹ å§”å‘˜ä¼šé¢†å¯¼äºº Herb Sutter äº 2005 å¹´å†™è¿‡ä¸€ç¯‡åä¸ºã€ŠThe free lunch is overã€‹çš„æ–‡ç« ã€‚
 æ–‡ä¸­è®¨è®ºäº†æˆ‘åˆšæ‰è®²çš„æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼Œå¹¶ä¸”æ–­è¨€ï¼Œä»¥åçš„ç¨‹åºå‘˜å†ä¹Ÿä¸èƒ½ä»…ä»…é å‡çº§æ›´å¿«çš„ç¡¬ä»¶æ¥ç»™åº”ç”¨ç¨‹åºæˆ–ç¼–ç¨‹è¯­è¨€æå‡æ€§èƒ½äº†ã€‚


Now, more than a decade later, there is no doubt that Herb Sutter was right. Memory is slow, caches are too small, CPU clock speeds are going backwards, and the simple world of a single threaded CPU is long gone.

åå‡ å¹´åçš„å¦‚ä»Šï¼Œå¯ä»¥è‚¯å®š Herb Sutter æ˜¯æ­£ç¡®çš„ã€‚
å†…å­˜å¤ªæ…¢ï¼Œç¼“å­˜åˆå¤ªå°ï¼ŒCPUæ—¶é’Ÿé€Ÿç‡è¿˜æ›´æ…¢äº†ï¼Œå•çº¿ç¨‹CPUçš„ä¸–ç•Œå·²ç»è¿‡å»äº†ã€‚

Mooreâ€™s Law is still in effect, but for all of us in this room, the free lunch is over.

æ‘©å°”å®šå¾‹ä»ç„¶æœ‰æ•ˆï¼Œä½†å¯¹åœ¨åº§å„ä½æ¥è¯´ï¼Œå…è´¹åˆé¤æ—¶ä»£å·²ç»è¿‡å»äº†ã€‚



### 1.14. Conclusion ç»“è®º

> The numbers I would cite would be by 2010: 30GHz, 10billion transistors, and 1 tera-instruction per second.â€‰â€”â€‰[Pat Gelsinger, Intel CTO, April 2002](https://www.cnet.com/news/intel-cto-chip-heat-becoming-critical-issue/)

> Intel CTO  Pat Gelsinger åœ¨ 2002 4æœˆæ›¾é¢„æµ‹2010å¹´çš„CPUæ€§èƒ½æŒ‡æ ‡ï¼š 30GHz, 100äº¿ä¸ªæ™¶ä½“ç®¡ï¼Œæ¯ç§’ç§æ‰§è¡Œ ä¸€åƒäº¿ æ¡æŒ‡ä»¤ã€‚
>
> NOTE å¼•ç”¨çš„æ–‡ç« ä¸­ï¼ŒPat Gelsinger æ˜¯è¯´åœ¨æ²¡æœ‰æ•£çƒ­é—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½è¾¾åˆ° 30GHz çš„é¢‘ç‡ã€‚è™½ç„¶ 2004 å¹´æ—¶å·²ç»ç”Ÿäº§å‡º 3.8GHz ä¸»é¢‘çš„ [Prescott](https://zh.wikipedia.org/wiki/%E5%A5%94%E8%85%BE4#cite_note-3) ï¼Œä½†è‡³ä»Š 2020 å¹´ï¼Œå•æ ¸å¿ƒCPUä¸»é¢‘æ™®éä½äº 4GHz [^InterlCPUList] ã€‚
> 
> - 1k = 1000
> - 1m = 1000k = 100 ä¸‡
> - 1g = 1000m = 1 äº¿
> - 1t = 1000g = ä¸€åƒäº¿ 
> 
> NOTE å…¶ä»–ç›¸å…³æ–‡ç«  [Favorite Forecast Fallacies](https://semiengineering.com/favorite-forecast-fallacies/)  [è¯·é—®ç›®å‰ä¸»æµCPUçš„æ¯ç§’è®¡ç®—æ¬¡æ•°èƒ½è¾¾åˆ°å¤šå°‘ï¼Ÿèƒ½å¤Ÿä¸Šäº¿å—ï¼Ÿ](https://www.zhihu.com/question/39604940)


Itâ€™s clear that without a breakthrough in material science the likelihood of a return to the days of 52% year on year growth in CPU performance is vanishingly small. The common consensus is that the fault lies not with the material science itself, but how the transistors are being used. The logical model of sequential instruction flow as expressed in silicon has lead to this expensive endgame.

æ˜¾ç„¶ï¼Œå¦‚æœææ–™ç§‘å­¦æ–¹é¢æ²¡æœ‰æŠ€æœ¯çªç ´ï¼Œæƒ³è®© CPU æ€§èƒ½æ¢å¤åˆ°åŒæ¯” 52ï¼… å¢é•¿çš„å¯èƒ½æ€§å¾ˆå°ã€‚
æ™®éçš„å…±è¯†æ˜¯ï¼Œé—®é¢˜ä¸åœ¨äºææ–™ç§‘å­¦æœ¬èº«ï¼Œè€Œåœ¨äºæ™¶ä½“ç®¡çš„ä½¿ç”¨æ–¹å¼ã€‚
åªè¦ä½¿ç”¨ç¡…è¡¨ç¤ºçš„é¡ºåºæŒ‡ä»¤æµçš„é€»è¾‘æ¨¡å‹ï¼Œå¿…ç„¶å¯¼è‡´è¿™ç§ä»£ä»·ã€‚

There are many presentations online that rehash this point. They all have the same predictionâ€‰â€”â€‰computers in the future will not be programmed like they are today. Some argue itâ€™ll look more like graphics cards with hundreds of very dumb, very incoherent processors. Others argue that Very Long Instruction Word (VLIW) computers will become predominant. All agree that our current sequential programming languages will not be compatible with these kinds of processors.

äº’è”ç½‘ä¸Šæœ‰å¾ˆå¤šèµ„æ–™å¼ºè°ƒè¿‡è¿™ä¸€è§‚ç‚¹ã€‚
å®ƒä»¬éƒ½é¢„ï¼Œæœªæ¥çš„è®¡ç®—æœºå°†ä¸ä¼šåƒä»Šå¤©è¿™æ ·ç¼–ç¨‹ã€‚
æœ‰äººè®¤ä¸ºæœªæ¥çš„å¤„ç†å™¨å°†ç”±ä¸Šç™¾ä¸ªå‹å·ä¸ä¸€çš„ä½ç«¯å¤„ç†å™¨ç»„æˆã€‚ä¹Ÿæœ‰äººè®¤ä¸ºè¶…é•¿æŒ‡ä»¤å­—(VLIW)å°†æˆä¸ºä¸»æµã€‚
ä½†å¤§å®¶ä¸€è‡´åŒæ„ï¼Œç°åœ¨çš„é¡ºåºç¼–ç¨‹è¯­è¨€æ— æ³•é€‚åº”æœªæ¥çš„å¤„ç†å™¨ã€‚

My view is that these predictions are correct, the outlook for hardware manufacturers saving us at this point is grim. However, there is  _enormous_  scope to optimise the programs today we write for the hardware we have today. Rick Hudson spoke at GopherCon 2015 about  [reengaging with a "virtuous cycle"](https://talks.golang.org/2015/go-gc.pdf)  of software that works  _with_  the hardware we have today, not indiferent of it.

æˆ‘è®¤ä¸ºè¿™äº›é¢„æµ‹æ˜¯å¯¹çš„ï¼Œé ç¡¬ä»¶å‚å•†æå‡æˆ‘ä»¬çš„è½¯ä»¶æ€§èƒ½æ˜¯ä¸å¤ªé è°±äº†ã€‚
ä½†ç°æœ‰ç¡¬ä»¶ä¸Šçš„ç¨‹åºè¿˜æœ‰å¾ˆå¤§ä¼˜åŒ–ç©ºé—´ã€‚
Rick Hudson (é‡Œå…‹Â·å“ˆå¾·æ£®)åœ¨2015å¹´GopherConè°ˆåˆ°â€œè‰¯æ€§å¾ªç¯â€çš„æ¦‚å¿µã€‚
å³ç¡¬ä»¶å’Œè½¯ä»¶åº”è¯¥åº”è¯¥äº’ç›¸é…åˆï¼Œè¿­ä»£ä¼˜åŒ–å‡çº§ã€‚

> NOTE: ä»¥å‰çš„ç¡¬ä»¶é¢‘ç‡å¢é•¿å¿«ï¼Œé‚£è½¯ä»¶åº”è¯¥å……åˆ†å‘æŒ‰é«˜é¢‘çš„ç‰¹æ€§ï¼›ä»¥åçš„ç¡¬ä»¶ CPU æ ¸å¿ƒæ•°é‡è¶Šæ¥è¶Šå¤šï¼Œé‚£è½¯ä»¶åº”è¯¥å……åˆ†åˆ©ç”¨å¤šæ ¸å¿ƒçš„ä¼˜åŠ¿ã€‚


Looking at the graphs I showed earlier, from 2015 to 2018 with at best a 5-8% improvement in integer performance and less than that in memory latency, the Go team have decreased the garbage collector pause times by  [two orders of magnitude](https://blog.golang.org/ismmkeynote). A Go 1.11 program exhibits significantly better GC latency than the same program on the same hardware using Go 1.6. None of this came from hardware.

å›é¡¾ä¹‹å‰çš„å›¾æ ‡å¯çœ‹å‡ºï¼Œä» 2015 å¹´åˆ° 2018 å¹´é—´ï¼Œ integer performance æ€§èƒ½ä»…æé«˜äº† 5-8% ï¼Œ memory latency æé«˜å¾—æ›´å°‘ã€‚å³ä½¿è¿™ç§æƒ…å†µä¸‹ï¼Œ Go å¼€å‘å›¢é˜Ÿä»ç„¶å°† garbage collector æš‚åœæ—¶é—´æé«˜äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚
åŒæ ·çš„ä»£ç ï¼Œåœ¨åŒæ ·çš„ç¡¬ä»¶ä¸­ï¼Œä½¿ç”¨ Go 1.11 ç¼–è¯‘æ—¶å…¶ GC latency æ˜æ˜¾ä¼˜äº Go 1.6 ç‰ˆæœ¬ã€‚
è¿™äº›æå‡å¯ä¸æ¥è‡ªç¡¬ä»¶ã€‚

![intgraph](https://dave.cheney.net/high-performance-go-workshop/images/int_graph.png)


So, for best performance on todayâ€™s hardware in todayâ€™s world, you need a programming language which:

ä¸ºäº†åœ¨ç°ä»Šä¸–ç•Œçš„ç¡¬ä»¶ä¸­è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä½ éœ€è¦çš„ç¼–ç¨‹è¯­è¨€åº”è¯¥æ˜¯ä¸‹é¢è¿™æ ·ï¼š

-   Is compiled, not interpreted, because interpreted programming languages interact poorly with CPU branch predictors and speculative execution.
    
-   You need a language which permits efficient code to be written, it needs to be able to talk about bits and bytes, and the length of an integer efficiently, rather than pretend every number is an ideal float.
    
-   You need a language which lets programmers talk about memory effectively, think structs vs java objects, because all that pointer chasing puts pressure on the CPU cache and cache misses burn hundreds of cycles.
    
-   A programming language that scales to multiple cores as performance of an application is determined by how efficiently it uses its cache and how efficiently it can parallelise work over multiple cores.

- å®ƒåº”è¯¥æ˜¯ç¼–è¯‘å‹è¯­è¨€ï¼Œè€Œéè§£é‡Šå‹è¯­è¨€ï¼Œå› ä¸ºè§£é‡Šå‹è¯­è¨€æ²¡æ³•å‘æŒ¥ CPU çš„åˆ†æ”¯é¢„æµ‹å’Œä¹±åºæ‰§è¡ŒåŠŸèƒ½ä¼˜åŠ¿ã€‚

- è¿™ç§è¯­è¨€åº”è¯¥æ”¯æŒç¼–å†™æ›´é«˜æ•ˆç‡çš„ä»£ç ï¼Œå®ƒåº”è¯¥èƒ½æ“ä½œ bit å’Œ byte ï¼Œè€Œä¸”èƒ½åŒºåˆ† integer å’Œ float æ•°å€¼ç±»å‹ï¼Œä»¥ä¾¿æ›´é«˜æ•ˆç‡åœ°å¤„ç† integer 

- è¿™ç§è¯­è¨€åº”è¯¥èƒ½è®©ç¨‹åºå‘˜è®¨è®ºå†…å­˜ï¼Œæ€è€ƒ struct ä¸ java object çš„åŒºåˆ«ï¼Œå› ä¸ºæ‰€æœ‰çš„ pointer chasing éƒ½ä¼šç»™ CPU  cache å¸¦æ¥å¾ˆå¤§å‹åŠ›ï¼Œè€Œ cache miss ä¼šæ¶ˆè€—ä¸Šç™¾ä¸ªæ—¶é’Ÿå‘¨æœŸã€‚

- è¿™ç§ç¼–ç¨‹è¯­è¨€åº”è¯¥æ”¯æŒï¼Œé€šè¿‡å¢åŠ CPUæ ¸å¿ƒæ•°é‡æ¥æå‡ç¨‹åºæ€§èƒ½ã€‚æ‰€ä»¥å®ƒè¦èƒ½é«˜æ•ˆåœ°åˆ©ç”¨ cache ï¼Œå¹¶é«˜æ•ˆåœ°åˆ©ç”¨å¤šæ ¸å¿ƒå¹¶è¡Œå·¥ä½œã€‚
    

Obviously weâ€™re here to talk about Go, and I believe that Go inherits many of the traits I just described.

æˆ‘ä»¬æ¥åˆ°è¿™é‡Œè®¨è®º Go è¯­è¨€ï¼Œæ˜¾ç„¶æ˜¯å› ä¸º Go å…·å¤‡å¾ˆå¤šæˆ‘åˆšæ‰æè¿°çš„é‚£äº›ç‰¹ç‚¹ã€‚


#### 1.14.1. What does that mean for us?

> There are only three optimizations: Do less. Do it less often. Do it faster.
> 
> The largest gains come from 1, but we spend all our time on 3.â€‰â€”â€‰[Michael Fromberger](https://twitter.com/creachadair/status/1039602865831010305)

> æœ‰ä¸‰ç§ä¼˜åŒ–æ‰‹æ®µï¼šå°‘åšã€‚å†å°‘åšä¸€ç‚¹ã€‚åšå¿«ç‚¹ã€‚

> æ”¶ç›Šæœ€å¤§çš„æ˜¯ç¬¬ä¸€ç§ï¼Œä½†æˆ‘ä»¬æŠŠæ—¶é—´éƒ½èŠ±åˆ°ç¬¬ä¸‰ç§æ‰‹æ®µä¸Šäº†ã€‚  â€”â€‰Michael Fromberger


The point of this lecture was to illustrate that when youâ€™re talking about the performance of a program or a system is entirely in the software. Waiting for faster hardware to save the day is a foolâ€™s errand.

è¿™æ¬¡è®²åº§çš„ç›®çš„æ˜¯æƒ³è¯´æ˜ï¼Œå½“æˆ‘ä»¬è°ˆè®ºè½¯ä»¶æˆ–ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ—¶ï¼Œè‚¯å®šæ˜¯åœ¨è¯´å®Œå…¨åŸºäºè½¯ä»¶çš„ä¼˜åŒ–æ‰‹æ®µã€‚
å¦„æƒ³ç­‰ç¡¬ä»¶å˜å¿«å¤§å¹…æé«˜è½¯ä»¶æ€§èƒ½çš„æƒ³æ³•æ˜¯æ„šè ¢çš„ã€‚

But there is good news, there is a tonne of improvements we can make in software, and that is what weâ€™re going to talk about today.

å¥½æ¶ˆæ¯æ˜¯ï¼Œè½¯ä»¶ä¸Šè¿˜æœ‰éå¸¸å¤§çš„ä¼˜åŒ–ç©ºé—´ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©è¦è®²çš„å†…å®¹ã€‚


#### 1.14.2. Further reading å»¶ä¼¸é˜…è¯»

-   [The Future of Microprocessors, Sophie Wilson](https://www.youtube.com/watch?v=zX4ZNfvw1cw)  JuliaCon 2018
    
-   [50 Years of Computer Architecture: From Mainframe CPUs to DNN TPUs, David Patterson](https://www.youtube.com/watch?v=HnniEPtNs-4)
    
-   [The Future of Computing, John Hennessy](https://web.stanford.edu/~hennessy/Future%20of%20Computing.pdf)
    
-   [The future of computing: a conversation with John Hennessy](https://www.youtube.com/watch?v=Azt8Nc-mtKM)  (Google I/O '18)
    

## 2. Benchmarking åŸºå‡†æµ‹è¯•

> Measure twice and cut once.â€‰â€”â€‰Ancient proverb

> æµ‹é‡ä¸¤æ¬¡ï¼Œåˆ‡ä¸€æ¬¡ã€‚ â€‰â€” è°šè¯­

> NOTE è¿™æ˜¯å®ƒçš„ç›´æ¥ç¿»è¯‘ã€‚å®ƒçš„æ·±åˆ»å«ä¹‰æ˜¯æŒ‡ï¼Œåšäº‹æƒ…è¦ç²¾å¿ƒå‡†å¤‡ï¼Œç‰¹åˆ«æ˜¯å½“ä½ åªæœ‰ä¸€æ¬¡æœºä¼šçš„æ—¶å€™ã€‚ä¾‹å¦‚ï¼Œå½“äººä»¬åˆ‡å‰²æœ¨å¤´çš„æ—¶å€™ï¼Œå¿…é¡»ä»”ç»†æµ‹é‡å°ºå¯¸ï¼Œå› ä¸ºä½ åªæœ‰ä¸€æ¬¡æœºä¼šï¼Œå¦åˆ™ä½ æœ€åçš„å°ºå¯¸ä¸æ˜¯å¤§äº†å°±æ˜¯å°äº†ã€‚æ‰€ä»¥ï¼Œå½“æˆ‘ä»¬æé†’åˆ«äººåšäº‹æƒ…è¦ä¸‰æ€åè¡Œæ—¶ï¼Œåœ¨è‹±æ–‡ä¸­å°±è¯´â€œMeasure Twice, Cut Onceâ€ã€‚å……è¶³çš„å‡†å¤‡æ˜¯æˆåŠŸæ‰€å¿…éœ€çš„ã€‚æœ‰å‡†å¤‡æœªå¿…æˆåŠŸï¼Œä½†æ˜¯æ²¡å‡†å¤‡ï¼Œå¤±è´¥çš„å¯èƒ½æ€§å¾ˆå¤§ã€‚[http://learn-english-writing.blogspot.com/](http://learn-english-writing.blogspot.com/2011/12/measure-twice-cut-once.html)


Before we attempt to improve the performance of a piece of code, first we must know its current performance.

åœ¨æˆ‘ä»¬è¯•å›¾æé«˜ä¸€æ®µä»£ç çš„æ€§èƒ½æ—¶ï¼Œå¿…é¡»å…ˆäº†è§£å®ƒå½“å‰çš„æ€§èƒ½ã€‚

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the pitfalls.

æœ¬èŠ‚å°†é‡ç‚¹ä»‹ç»å¦‚ä½•ä½¿ç”¨ Go æµ‹è¯•æ¡†æ¶æ„å»ºåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç»™å‡ºé¿å‘å®è·µæŒ‡å—ã€‚


### 2.1. Benchmarking ground rules åŸºå‡†æµ‹è¯•è§„åˆ™

Before you benchmark, you must have a stable environment to get repeatable results.

è¿›è¡ŒåŸºå‡†æµ‹è¯•å‰ï¼Œä½ å¿…é¡»æœ‰ä¸€ä¸ªç¨³å®šçš„è¿è¡Œç¯å¢ƒï¼Œæ‰èƒ½å¾—åˆ°å¯é‡å¤çš„ç»“æœã€‚


-   The machine must be idle -- donâ€™t profile on shared hardware, donâ€™t browse the web while waiting for a long benchmark to run.
    
-   Watch out for power saving and thermal scaling. These are almost unavoidable on modern laptops.
    
-   Avoid virtual machines and shared cloud hosting; they can be too noisy for consistent measurements.
    

- æœºå™¨å¿…é¡»æ˜¯ç©ºé—²çš„ -- ä¸è¦ä½¿ç”¨å…¬ç”¨çš„ç¡¬ä»¶ç¯å¢ƒï¼Œä¹Ÿä¸è¦åœ¨ç­‰å¾…è¿è¡Œè¾ƒé•¿æ—¶é—´çš„åŸºå‡†æµ‹è¯•è¿‡ç¨‹æµè§ˆç½‘é¡µã€‚

- æ³¨æ„ç³»ç»Ÿçš„èŠ‚èƒ½é…ç½®å’Œçƒ­åŠ›ç¼©æ”¾ã€‚è¿™äº›é—®é¢˜åœ¨ç°ä»£ç¬”è®°æœ¬ä¸­å‡ ä¹æ— æ³•é¿å…ã€‚

- é¿å…ä½¿ç”¨è™šæ‹Ÿæœºå’Œå…¬å…±çš„äº‘ä¸»æœºï¼›è¿™äº›ç¯å¢ƒå¹²æ‰°å› ç´ å¤ªå¤šï¼Œæ— æ³•ä¿è¯æµ‹é‡ç»“æœä¸€è‡´æ€§ã€‚


If you can afford it, buy dedicated performance test hardware. Rack it, disable all the power management and thermal scaling, and never update the software on those machines. The last point is poor advice from a system adminstration point of view, but if a software update changes the way the kernel or library performs â€”-think the Spectre patchesâ€”- this will invalidate any previous benchmarking results.

å¦‚æœæœ‰é’±ï¼Œå°±ä¹°ä¸“é—¨ç”¨äºæ€§èƒ½æµ‹è¯•çš„ç¡¬ä»¶ã€‚æ”¾æœºæ¶ä¸Šï¼Œç¦ç”¨æ‰€æœ‰ç”µæºç®¡ç†å’Œçƒ­åŠ›ç¼©æ”¾åŠŸèƒ½ï¼Œå¹¶ä¸”æ°¸è¿œä¸è¦å‡çº§è¿™å°æœºå™¨çš„è½¯ä»¶ã€‚
ä»ç³»ç»Ÿç®¡ç†å‘˜çš„è§’åº¦æ¥è¯´ï¼Œæœ€åä¸€æ¡å»ºè®®éå¸¸ç³Ÿç³•ã€‚ä½†å¦‚æœå‡çº§è½¯ä»¶åï¼Œæ”¹å˜äº†ç³»ç»Ÿå†…æ ¸æˆ–ç¬¬ä¸‰æ–¹åº“ï¼Œé‚£ä¹ˆåœ¨æ­¤ä¹‹å‰æ‰€æœ‰åŸºå‡†æµ‹è¯•ç»“æœéƒ½æ— æ•ˆäº†ï¼Œæ¯”å¦‚ Spectre æ¼æ´çš„è¡¥ä¸å°±ä¼šé™¤ä½ç³»ç»Ÿæ€§èƒ½ã€‚

> NOTE Spectre æ¼æ´çš„ä¿®å¤è¡¥ä¸ä¼šé™ä½ç³»ç»Ÿæ€§èƒ½ï¼Œå‚è€ƒ [å¦‚ä½•çœ‹å¾… 2018 å¹´ 1 æœˆ 2 æ—¥çˆ†å‡ºçš„ Intel CPU è®¾è®¡æ¼æ´ï¼Ÿ](https://www.zhihu.com/question/265012502/answer/288239171)
> èŠ¯ç‰‡å¾®ç æ›´æ–°ä¸è¶³ä»¥ä¿®å¤æ¼æ´ï¼Œå¿…é¡»ä¿®æ”¹ç³»ç»Ÿæˆ–è€…è´­ä¹°æ–°è®¾è®¡çš„ CPUã€‚
ç›®å‰ Linux å†…æ ¸çš„è§£å†³æ–¹æ¡ˆæ˜¯é‡æ–°è®¾è®¡é¡µè¡¨ï¼ˆKPTI æŠ€æœ¯ï¼Œå‰èº«ä¸º KAISERï¼‰ã€‚ä¹‹å‰æ™®é€šç¨‹åºå’Œå†…æ ¸ç¨‹åºå…±ç”¨é¡µè¡¨ï¼Œé  CPU æ¥é˜»æ­¢æ™®é€šç¨‹åºçš„è¶Šæƒè®¿é—®ã€‚æ–°æ–¹æ¡ˆè®©å†…æ ¸ä½¿ç”¨å¦å¤–ä¸€ä¸ªé¡µè¡¨ï¼Œè€Œæ™®é€šç¨‹åºçš„é¡µè¡¨ä¸­åªä¿ç•™ä¸€äº›å¿…è¦çš„å†…æ ¸ä¿¡æ¯ï¼ˆä¾‹å¦‚è°ƒç”¨å†…æ ¸çš„åœ°å€ï¼‰ã€‚è¿™ä¸ªæ–¹æ¡ˆä¼šå¯¼è‡´æ¯æ¬¡æ™®é€šç¨‹åºå’Œå†…æ ¸ç¨‹åºä¹‹é—´çš„åˆ‡æ¢ï¼ˆä¾‹å¦‚ç³»ç»Ÿå†…æ ¸è°ƒç”¨æˆ–è€…ç¡¬ä»¶ä¸­æ–­ï¼‰éƒ½éœ€è¦åˆ‡æ¢é¡µè¡¨ï¼Œå¼•èµ· CPU çš„ TLB ç¼“å­˜åˆ·æ–°ã€‚TLB ç¼“å­˜åˆ·æ–°ç›¸å¯¹æ¥è¯´æ˜¯éå¸¸è€—æ—¶çš„ï¼Œå› æ­¤ä¼šé™ä½ç³»ç»Ÿçš„æ•ˆç‡ã€‚
> KAISER æŠ€æœ¯å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ä¸€èˆ¬æ˜¯ 5%ï¼Œæœ€é«˜å¯è¾¾ 30%ã€‚ä¸€äº›é«˜çº§çš„èŠ¯ç‰‡åŠŸèƒ½ï¼ˆä¾‹å¦‚ PCIDï¼‰å¯ä»¥æ”¯æŒå…¶ä»–æŠ€æœ¯ï¼Œä»è€Œå‡å°‘æ€§èƒ½å½±å“ã€‚Linux å·²ç»åœ¨ 4.14 ç‰ˆæœ¬çš„å¼€å‘è¿‡ç¨‹ä¸­æ·»åŠ äº†å¯¹ PCID çš„æ”¯æŒã€‚
> åœ¨ Linux ç³»ç»Ÿä¸­ï¼ŒKPTI åªæœ‰åœ¨è‹±ç‰¹å°”èŠ¯ç‰‡ä¸Šæ‰ä¼šå¯ç”¨ï¼Œå› æ­¤ AMD èŠ¯ç‰‡ä¸å—å½±å“ï¼Œä¸”ç”¨æˆ·å¯ä»¥é€šè¿‡æ‰‹åŠ¨ä¿®æ”¹å¼€å…³çš„æ–¹å¼å…³é—­ KPTI ã€‚  


For the rest of us, have a before and after sample and run them multiple times to get consistent results.

ä¼˜åŒ–å‰ï¼Œä¼˜åŒ–åï¼Œéƒ½è¦è¿è¡Œå¤šæ¬¡åŸºå‡†æµ‹è¯•ï¼Œæ¥ä¿è¯å‰åæ ·æœ¬ç»“æœçš„ä¸€è‡´æ€§ã€‚


### 2.2. Using the testing package for benchmarking ä½¿ç”¨ testing package è¿›è¡ŒåŸºå‡†æµ‹è¯•

The  `testing`  package has built in support for writing benchmarks. If we have a simple function like this:

`testing` package ä¸“é—¨ç¼–å†™åŸºå‡†æµ‹è¯•çš„å†…ç½®åŒ…ã€‚
å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸‹é¢è¿™æ ·ç®€å•çš„å‡½æ•°è¦æµ‹è¯•ï¼š

```go
func Fib(n int) int {
	switch n {
	case 0:
		return 0
	case 1:
		return 1
	case 2:
		return 2
	default:
		return Fib(n-1) + Fib(n-2)
	}
}
```

The we can use the  `testing`  package to write a  _benchmark_  for the function using this form.

å¯ä»¥åƒä¸‹é¢è¿™æ ·ï¼Œç”¨ `testing` package  ç¼–å†™ _åŸºå‡†æµ‹è¯•_  æµ‹è¯•åˆšæ‰å‡½æ•°ã€‚

```go
func BenchmarkFib20(b *testing.B) {
	for n := 0; n < b.N; n++ {
		Fib(20) // run the Fib function b.N times
	}
}
```

The benchmark function lives alongside your tests in a  `_test.go`  file.

è¿™ä¸ªåŸºå‡†æµ‹è¯•å‡½æ•°ä¸€èˆ¬æ”¾åœ¨ä»¥ `_test.go` ç»“å°¾çš„å•å…ƒæµ‹è¯•ä¸­ã€‚

Benchmarks are similar to tests, the only real difference is they take a  `*testing.B`  rather than a  `*testing.T`. Both of these types implement the  `testing.TB`  interface which provides crowd favorites like  `Errorf()`,  `Fatalf()`, and  `FailNow()`.

åŸºå‡†æµ‹è¯•ä¸å•å…ƒæµ‹è¯•å¾ˆåƒï¼Œå”¯ä¸€åŒºåˆ«æ˜¯ï¼ŒåŸºå‡†æµ‹è¯•å‡½æ•°å‚æ•°æ˜¯ `*testing.B`ï¼Œè€Œå•å…ƒæµ‹è¯•å‡½æ•°å‚æ•°æ˜¯ `*testing.T`ã€‚
ä¸¤è€…éƒ½å®ç°äº† `testing.TB` æ¥å£å®šä¹‰çš„ `Errorf()`,  `Fatalf()`, å’Œ  `FailNow()` æ–¹æ³•ã€‚




#### 2.2.1. Running a packageâ€™s benchmarks è¿è¡Œ package çš„åŸºå‡†æµ‹è¯•

As benchmarks use the  `testing`  package they are executed via the  `go test`  subcommand. However, by default when you invoke  `go test`, benchmarks are excluded.

å› ä¸º benchmark ä½¿ç”¨ `testing` package ï¼Œè‡ªç„¶æ˜¯ä½¿ç”¨ `go test` çš„å­å‘½ä»¤è¿è¡ŒåŸºå‡†æµ‹è¯•çš„ã€‚
ä½†ï¼Œæ‰§è¡Œ `go test` å‘½ä»¤æ—¶ï¼Œé»˜è®¤ä¸ä¼šæ‰§è¡ŒåŸºå‡†æµ‹è¯•ã€‚

To explicitly run benchmarks in a package use the  `-bench`  flag.  `-bench`  takes a regular expression that matches the names of the benchmarks you want to run, so the most common way to invoke all benchmarks in a package is  `-bench=.`. Here is an example:

ä½¿ç”¨ `-bench` æ ‡å¿—å¯æ˜ç¡®è¿è¡Œ package ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚
`-bench` ä½¿ç”¨æ­£åˆ™åŒ¹é…è¦è¿è¡ŒåŸºå‡†æµ‹è¯•çš„ package åç§°ã€‚
ä¸€èˆ¬å¸¸ç”¨ `-bench=.` è¿è¡Œ package ä¸­çš„æ‰€æœ‰åŸºå‡†æµ‹è¯•ã€‚
ä¸‹é¢æ˜¯ç¤ºä¾‹ï¼š

```shell
% go test -bench=. ./examples/fib/
goos: darwin
goarch: amd64
BenchmarkFib20-8           30000             40865 ns/op
PASS
ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     1.671s
```

`go test`  will also run all the tests in a package before matching benchmarks, so if you have a lot of tests in a package, or they take a long time to run, you can exclude them by providing  go testâ€™s `-run`  flag with a regex that matches nothing; ie.

`go test` åœ¨åŒ¹é…åŸºå‡†æµ‹è¯•å‰ï¼Œä¼šå…ˆè¿è¡Œ package ä¸­çš„æ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œå¦‚æœ package ä¸­çš„å•å…ƒæµ‹è¯•å¾ˆå¤šï¼Œæˆ–è€…è¿è¡Œå•å…ƒæµ‹è¯•éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå¯ä»¥ç»™ go test çš„ `-run` æ ‡è®°åŠ ä¸€ä¸ªåŒ¹é…ç»“æœä¸ºç©ºçš„å‚æ•°ï¼Œå°±èƒ½è·³è¿‡å•å…ƒæµ‹è¯•ã€‚


```shell
go test -run=^$
```


#### 2.2.2. How benchmarks work åŸºå‡†æµ‹è¯•æ˜¯å¦‚ä½•å·¥ä½œçš„

Each benchmark function is called with different value for  `b.N`, this is the number of iterations the benchmark should run for.

`b.N` å‚æ•°æ˜¯åŸºå‡†çš„è¿­ä»£æ¬¡æ•°ï¼Œæ¯ä¸ªåŸºå‡†æµ‹è¯•å‡½æ•°éƒ½ä¼šä½¿ç”¨ä¸åŒçš„ `b.N` å€¼è¢«è°ƒç”¨å¤šæ¬¡ã€‚

`b.N`  starts at 1, if the benchmark function completes in under 1 second --the default-- then  `b.N`  is increased and the benchmark function run again.

`b.N` é»˜è®¤å–å€¼æ˜¯1ï¼Œå¦‚æœåŸºå‡†æµ‹è¯•å‡½æ•°åœ¨1ç§’å†…å®Œæˆï¼Œä¼šå¢åŠ  `b.N` çš„å€¼ï¼Œç„¶åå†æ¬¡è¿è¡ŒåŸºå‡†æµ‹è¯•ã€‚


`b.N`  increases in the approximate sequence; 1, 2, 3, 5, 10, 20, 30, 50, 100, and so on. The benchmark framework tries to be smart and if it sees small values of  `b.N`  are completing relatively quickly, it will increase the the iteration count faster.

`b.N` çš„å¢åŠ è¿‡ç¨‹ç±»ä¼¼è¿™ä¸ªæ•°åˆ—ï¼š1, 2, 3, 5, 10, 20, 30, 50, 100, ç­‰ç­‰ã€‚
åŸºå‡†æµ‹è¯•æ¡†æ¶å¦‚æœå‘ç°å¾ˆå°çš„ `b.N` å€¼èƒ½å¾ˆå¿«å®Œæˆï¼Œä¼šå¢åŠ è¿­ä»£ `b.N` æ•°æ—¶çš„å–å€¼(NOTE:è·³è¿‡è¾ƒå°çš„æ•°)ã€‚


Looking at the example above,  `BenchmarkFib20-8`  found that around 30,000 iterations of the loop took just over a second. From there the benchmark framework computed that the average time per operation was 40865ns.

ä»ä¸Šé¢çš„ä¾‹å­å¯ä»¥çœ‹å‡º `BenchmarkFib20-8` è¿‡ç¨‹å¤§æ¦‚ 30000 æ¬¡è¿­ä»£åªéœ€è¦ 1 ç§’é’Ÿã€‚
æ‰€ä»¥åŸºå‡†æµ‹è¯•æ¡†æ¶è®¡ç®—å‡ºå¹³å‡æ¯æ¬¡æ“ä½œæ˜¯ 40865ns ã€‚

The  `-8`  suffix relates to the value of  `GOMAXPROCS`  that was used to run this test. This number, like  `GOMAXPROCS`, defaults to the number of CPUs visible to the Go process on startup. You can change this value with the  `-cpu`  flag which takes a list of values to run the benchmark with.

åç¼€ `-8` çš„å–å€¼ä¸è¿è¡Œæµ‹è¯•æ—¶çš„ `GOMAXPROCS` æœ‰å…³ã€‚
è·Ÿ `GOMAXPROCS` ç¯å¢ƒå˜é‡ä¸€æ ·ï¼Œå®ƒçš„é»˜è®¤å–å€¼æ˜¯ Go è¿›ç¨‹å¯åŠ¨æ—¶çš„ CPU æ•°é‡ã€‚
ä½ èƒ½ç”¨ `-cpu` å‚æ•°æ”¹å˜è¿è¡ŒåŸºå‡†æµ‹è¯•æ—¶çš„å–å€¼

```shell
% go test -bench=. -cpu=1,2,4 ./examples/fib/
goos: darwin
goarch: amd64
BenchmarkFib20             30000             39115 ns/op
BenchmarkFib20-2           30000             39468 ns/op
BenchmarkFib20-4           50000             40728 ns/op
PASS
ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     5.531s
```

This shows running the benchmark with 1, 2, and 4 cores. In this case the flag has little effect on the outcome because this benchmark is entirely sequential.

ä»¥ä¸Šæ˜¯åˆ†åˆ«ä½¿ç”¨ 1ï¼Œ2 å’Œ 4 ä¸ªCPUæ ¸å¿ƒè¿è¡ŒåŸºå‡†æµ‹è¯•çš„ç»“æœã€‚
å› ä¸ºè¢«æµ‹å‡½æ•°æ˜¯é¡ºåºæ‰§è¡Œçš„ (NOTE:Fibæ²¡æœ‰åˆ©ç”¨å¤šæ ¸å¿ƒä¼˜åŒ–)  ï¼Œæ‰€ä»¥è¿™é‡Œçš„åŸºå‡†æµ‹è¯•ç»“æœå˜åŒ–ä¸å¤§ã€‚


#### 2.2.3. Improving benchmark accuracy æé«˜åŸºå‡†æµ‹è¯•çš„ç²¾åº¦

The  `fib`  function is a slightly contrived example â€”-unless your writing a TechPower web server benchmarkâ€”- itâ€™s unlikely your business is going to be gated on how quickly you can compute the 20th number in the Fibonaci sequence. But, the benchmark does provide a faithful example of a valid benchmark.

è¿™ä¸ª `fib` æ˜¯äººä¸ºçš„ä¾‹å­ --é™¤éä½ å†™ TechPower ç½‘ç»œæœåŠ¡çš„åŸºå‡†æµ‹è¯•-- å¦åˆ™ä½ çš„ä¸šåŠ¡ç“¶é¢ˆä¸åœ¨å¯èƒ½é™åˆ¶åœ¨è®¡ç®—ç¬¬ 20 ä¸ªæ–æ³¢é‚£å¥‘åºåˆ—çš„å€¼ã€‚
ä½†è¿™ä¸ªç¤ºä¾‹å¯¹äºæ¼”ç¤ºåŸºå‡†æµ‹è¯•è¶³å¤Ÿäº†ã€‚

Specifically you want your benchmark to run for several tens of thousand iterations so you get a good average per operation. If your benchmark runs for only 100â€™s or 10â€™s of iterations, the average of those runs may have a high standard deviation. If your benchmark runs for millions or billions of iterations, the average may be very accurate, but subject to the vaguaries of code layout and alignment.

å¦‚æœä½ çš„åŸºå‡†æµ‹è¯•è¿è¡Œæˆåƒä¸Šä¸‡æ¬¡ï¼Œåº”è¯¥èƒ½å¾—åˆ°ä¸€ä¸ªè¾ƒçœŸå®çš„å¹³å‡æ“ä½œæ—¶é—´ã€‚
å¦‚æœä½ çš„åŸºå‡†æµ‹è¯•åªè¿è¡Œå‡ åå‡ ç™¾æ¬¡ï¼Œé‚£ä¹ˆå¾—åˆ°çš„å¹³å‡æ“ä½œæ—¶é—´åº”è¯¥æœ‰è¾ƒå¤§çš„è¯¯å·®ã€‚
å¦‚æœä½ çš„åŸºå‡†æµ‹è¯•è¿è¡Œå‡ ç™¾ä¸‡ç”šè‡³å‡ åäº¿æ¬¡ï¼Œç»“æœä¼šéå¸¸ç²¾ç¡®ã€‚ä½†è¿™æ—¶æœ‰å¯èƒ½å—åˆ°ä»£ç å¸ƒå±€å’Œå¯¹é½çš„å½±å“ã€‚

> NOTE è¿™é‡Œ vaguaries of code layout and alignment å¯èƒ½æ˜¯æŒ‡æ•°æ®å¯¹é½ã€‚æ•°æ®å¯¹é½æœ‰å¯èƒ½å¯¹CPUæ€§èƒ½äº§ç”Ÿå½±å“ï¼Œå½“æ•°æ®ç»“æ„å¤§å°åˆšæ‰å¥½ Cache Line å¯¹é½ï¼Œæœ‰å¯èƒ½æé«˜æ€§èƒ½ã€‚ [^CPUCache] [^MemoryAndNativeCodePerformance]


To increase the number of iterations, the benchmark time can be increased with the  `-benchtime`  flag. For example:

å¯ä»¥ä½¿ç”¨ `-benchtime` å‚æ•°å¢åŠ åŸºå‡†æµ‹è¯•æ—¶é—´ï¼Œè¿›è€Œå¢åŠ è¿­ä»£æ¬¡æ•°ã€‚æ¯”å¦‚ï¼š

```shell
% go test -bench=. -benchtime=10s ./examples/fib/
goos: darwin
goarch: amd64
BenchmarkFib20-8          300000             39318 ns/op
PASS
ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     20.066s
```

Ran the same benchmark until it reached a value of  `b.N`  that took longer than 10 seconds to return. As weâ€™re running for 10x longer, the total number of iterations is 10x larger. The result hasnâ€™t changed much, which is what we expected.

æ­¤æ—¶ä¼šé€‰å–ä¸€ä¸ªä½¿åŸºå‡†æµ‹è¯•çš„è¿è¡Œæ—¶é—´è‡³å°‘è¶…è¿‡ 10s çš„ `b.N` å€¼ã€‚ 
æˆ‘ä»¬çš„è¿è¡Œæ—¶é—´å¢åŠ äº†10å€ï¼Œè¿è¡Œæ¬¡æ•°ä¹Ÿå¢åŠ äº†10å€ã€‚
ä½†å¹³å‡æ“ä½œæ—¶é—´çš„ç»“æœå¹¶æ²¡æœ‰å˜åŒ–ï¼Œè¿™ä¹Ÿæ­£æ˜¯æˆ‘ä»¬æœŸæœ›çœ‹åˆ°çš„ç»“æœã€‚


Why is the total time reporteded to be 20 seconds, not 10?

ä½†ä¸ºä»€ä¹ˆæ€»è€—æ—¶æ˜¯ 20s è€Œä¸æ˜¯ 10s å‘¢ï¼Ÿ

If you have a benchmark which runs for millons or billions of iterations resulting in a time per operation in the micro or nano second range, you may find that your benchmark numbers are unstable because thermal scaling, memory locality, background processing, gc activity, etc.

å¦‚æœä½ çš„åŸºå‡†æµ‹è¯•ä¼šè¿è¡Œä¸Šç™¾ä¸‡ç”šè‡³æ•°äº¿æ¬¡ï¼Œè€Œæ¯æ¬¡æ“ä½œåœ¨å¾®ç§’å’Œçº³ç§’èŒƒå›´å†…ã€‚
ä½ ä¼šå‘ç°ï¼ŒåŸºå‡†æµ‹è¯•ç»“æœä¼šå› ä¸º thermal scaling, memory locality, background processing, gc æ´»åŠ¨ç­‰å˜å¾—ååˆ†ä¸ç¨³å®šã€‚

For times measured in 10 or single digit nanoseconds per operation the relativistic effects of instruction reordering and code alignment will have an impact on your benchmark times.

å¯¹äºå•æ¬¡æ“ä½œè€—æ—¶åœ¨ 10çº³ç§’ä»¥å†…çš„æƒ…å†µï¼ŒåŸºå‡†æµ‹è¯•å—æŒ‡ä»¤é‡æ’å’Œä»£ç å¯¹é½çš„å½±å“å¾ˆå¤§ã€‚

To address this run benchmarks multiple times with the  `-count`  flag:

ä¸ºè§£å†³è¿™ç§é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ `-count` å‚æ•°æŒ‡å®šè¿è¡ŒåŸºå‡†æµ‹è¯•çš„æ•°æ¬¡ã€‚

```shell
% go test -bench=Fib1 -count=10 ./examples/fib/
goos: darwin
goarch: amd64
BenchmarkFib1-8         2000000000               1.99 ns/op
BenchmarkFib1-8         1000000000               1.95 ns/op
BenchmarkFib1-8         2000000000               1.99 ns/op
BenchmarkFib1-8         2000000000               1.97 ns/op
BenchmarkFib1-8         2000000000               1.99 ns/op
BenchmarkFib1-8         2000000000               1.96 ns/op
BenchmarkFib1-8         2000000000               1.99 ns/op
BenchmarkFib1-8         2000000000               2.01 ns/op
BenchmarkFib1-8         2000000000               1.99 ns/op
BenchmarkFib1-8         1000000000               2.00 ns/op
```

A benchmark of  `Fib(1)`  takes around 2 nano seconds with a variance of +/- 2%.

å‡½æ•° `Fib(1)` è€—æ—¶å¤§æ¦‚2çº³ç§’ï¼Œæ–¹å·®ä¸º +/- 2% ã€‚ 

New in Go 1.12 is the  `-benchtime`  flag now takes a number of iterations, eg.  `-benchtime=20x`  which will run your code exactly  `benchtime`  times.

Go 1.12 ç‰ˆæœ¬ä¸­ `-benchtime` å‚æ•°æ”¯æŒè®¾ç½®è¿­ä»£çš„æ¬¡æ•°ï¼Œæ¯”å¦‚ `-benchtime=20x` å¯ä»¥è®©å‡†ç¡®çš„è®©åŸºå‡†æµ‹è¯•è¿è¡Œ20æ¬¡ã€‚

Try running the fib bench above with a  `-benchtime`  of 10x, 20x, 50x, 100x, and 300x. What do you see?

å°è¯•ä½¿ç”¨ 10x, 20x, 50x, 100x, å’Œ 300x ä¸º `-benchtime` åˆ†åˆ«è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œçœ‹çœ‹ç»“æœæ˜¯ä»€ä¹ˆæ ·ï¼Ÿ

If you find that the defaults that  `go test`  applies need to be tweaked for a particular package, I suggest codifying those settings in a  `Makefile`  so everyone who wants to run your benchmarks can do so with the same settings.

å¦‚æœä½ å¸Œæœ›è°ƒæ•´æ‰§è¡Œ `go test` æ‰€ç”¨çš„é»˜è®¤å‚æ•°ï¼Œå»ºè®®æŠŠè¿™äº›é…ç½®å†™åˆ° `Makefile` ä¸­ï¼Œä»¥ä¾¿æ‰€æœ‰äººè¿è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„é…ç½®ã€‚


### 2.3. Comparing benchmarks with benchstat ä½¿ç”¨ benchstat æ¯”è¾ƒåŸºå‡†æµ‹è¯•

In the previous section I suggested running benchmarks more than once to get more data to average. This is good advice for any benchmark because of the effects of power management, background processes, and thermal management that I mentioned at the start of the chapter.

ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘å»ºè®®å¤šè¿è¡Œå‡ æ¬¡åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¾¿å¾—åˆ°æ›´å‡†ç¡®çš„å¹³å‡ç»“æœã€‚
ä¸ºé˜²æ­¢ power management, background process å’Œ thermal management çš„å½±å“ï¼Œè¿™æ˜¯ä¸ªå¾ˆå¥½çš„å»ºè®®ã€‚

Iâ€™m going to introduce a tool by Russ Cox called  [benchstat](https://godoc.org/golang.org/x/perf/cmd/benchstat).

ä¸‹é¢æˆ‘è¦ä»‹ç»çš„æ˜¯ Russ Cox çš„ benchstat å·¥å…·ã€‚

```shell
% go get golang.org/x/perf/cmd/benchstat
```

Benchstat can take a set of benchmark runs and tell you how stable they are. Here is an example of  `Fib(20)`  on battery power.

Benchstat å¯ä»¥åˆ†æå‡ºä¸€ç»„åŸºå‡†æµ‹è¯•ç»“æœçš„ç¨³å®šæ€§å¦‚ä½•ã€‚
ä¸‹é¢æ˜¯åœ¨ battery power ä¸Šæ‰§è¡Œ `Fib(20)` çš„åŸºå‡†æµ‹è¯•ç»“æœã€‚

```shell
% go test -bench=Fib20 -count=10 ./examples/fib/ | tee old.txt
goos: darwin
goarch: amd64
BenchmarkFib20-8           50000             38479 ns/op
BenchmarkFib20-8           50000             38303 ns/op
BenchmarkFib20-8           50000             38130 ns/op
BenchmarkFib20-8           50000             38636 ns/op
BenchmarkFib20-8           50000             38784 ns/op
BenchmarkFib20-8           50000             38310 ns/op
BenchmarkFib20-8           50000             38156 ns/op
BenchmarkFib20-8           50000             38291 ns/op
BenchmarkFib20-8           50000             38075 ns/op
BenchmarkFib20-8           50000             38705 ns/op
PASS
ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     23.125s
% benchstat old.txt
name     time/op
Fib20-8  38.4Âµs Â± 1%
```

`benchstat`  tells us the mean is 38.8 microseconds with a +/- 2% variation across the samples. This is pretty good for battery power.

`benchstat` å‘ç°æ ·æœ¬å‡å€¼åœ¨ 38.8ms ï¼Œæ–¹å·® +/- 1% ã€‚
è¿™ä¸ªç»“æœå¯¹ battery power æ¥è¯´å¾ˆä¸é”™äº†ã€‚

> NOTE: æè¿°æœ‰è¯¯ï¼Œä»¥å‘½ä»¤è¾“å‡ºçš„å†…å®¹ä¸ºå‡†ï¼Œå‡å€¼åº”è¯¥æ˜¯ 38.4Âµs Â± 1%ã€‚ 38.8 microseconds æ¯” 38.4Âµs å¤§å¤ªå¤šäº†ã€‚


-   The first run is the slowest of all because the operating system had the CPU clocked down to save power.
    
-   The next two runs are the fastest, because the operating system as decided that this isnâ€™t a transient spike of work and it has boosted up the clock speed to get through the work as quick as possible in the hope of being able to go back to sleep.
    
-   The remaining runs are the operating system and the bios trading power consumption for heat production.

- ç¬¬ä¸€æ¬¡è¿è¡Œç»“æœæ˜¯æœ€æ…¢çš„ï¼Œå› ä¸ºæ“ä½œç³»ç»Ÿä¸ºäº†çœç”µï¼ŒæŠŠ CPU æ—¶é’Ÿé€Ÿç‡é™åˆ°æœ€ä½äº†ã€‚

- ç´§æ¥ä¸‹æ¥çš„ä¸¤æ¬¡æ˜¯æœ€å¿«çš„ï¼Œå› ä¸ºæ“ä½œç³»ç»Ÿå‘ç°è¿™ä¸æ˜¯ä¸€ä¸ªçŸ­æš‚çš„ä¸´æ—¶ä»»åŠ¡ï¼Œä¸ºäº†å°½å¿«å®Œæˆä»»åŠ¡ï¼Œå›åˆ°ç¡çœ çŠ¶æ€ï¼Œå®ƒè°ƒé«˜äº†CPUæ—¶é’Ÿé€Ÿç‡ã€‚

-  å‰©ä¸‹çš„ç»“æœéƒ½æ˜¯ä¼´éšæ“ä½œç³»ç»Ÿä¸ BIOS é—´åè°ƒèƒ½è€—ä¸æ•£çƒ­çš„æƒ…å†µæ‰§è¡Œçš„ã€‚

> TODO: åé¢è¿™æ¬¡ç»“æœæ—¶é«˜æ—¶ä½ï¼Œæ˜¯æƒ³è¯´æ˜ä»€ä¹ˆå‘¢ï¼Ÿ 
> å› ä¸ºä¸»é¢‘è°ƒé«˜åï¼Œèƒ½è€—é«˜ï¼Œçƒ­é‡é«˜ï¼Œå¯¼è‡´é™é¢‘ï¼Œç„¶åèƒ½è€—å˜ä½ï¼Œçƒ­é‡ä½ï¼Œæ‰€ä»¥ç»“æœæ—¶é«˜æ—¶ä½å—ï¼Ÿ ä½†ä¸»é¢‘å‡é«˜åï¼Œåº”è¯¥ä¼šç»´æŠ¤ä¸€æ®µæ—¶é—´æ‰å¯¹ã€‚æ‰€ä»¥è¿™é‡Œç»“æœæ—¶é«˜æ—¶ä½åº”è¯¥å’Œèƒ½è€—å¤„ç†æ— å…³ï¼Œ
> ä¹Ÿè®¸æ˜¯è¿è¡Œå¤šæ¬¡åï¼Œç”±äºæ¯æ¬¡è¿ç®—éƒ½æ˜¯é‡å¤çš„ï¼Œå¯¼è‡´æ›´å¥½å¾—åˆ©ç”¨äº† CacheLine ç­‰ç‰¹æ€§ï¼Œæ‰äº§ç”Ÿæ—¶é«˜æ—¶ä½çš„ç»“æœå—ï¼Ÿæ²¡æƒ³é€šã€‚

    

#### 2.3.1. Improve  `Fib` æ”¹å–„ `Fib`

Determining the performance delta between two sets of benchmarks can be tedious and error prone. Benchstat can help us with this.

ç¡®å®šä¸¤ç»„åŸºå‡†æµ‹è¯•ä¹‹é—´çš„æ€§èƒ½åå·®å¯èƒ½ä¼šå¾ˆç¹çï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ã€‚Benchstat å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

Saving the output from a benchmark run is useful, but you can also save the  _binary_  that produced it. This lets you rerun benchmark previous iterations. To do this, use the  `-c`  flag to save the test binaryã€‚I often rename this binary from  `.test`  to  `.golden`.

å°†åŸºå‡†æµ‹è¯•çš„è¾“å‡ºç»“æœä¿å­˜èµ·æ¥ä¹Ÿè®¸ä¼šå¾ˆæœ‰ç”¨çš„ï¼Œä½†ä¹Ÿå¯ä»¥ä¿å­˜äº§ç”Ÿæµ‹è¯•ç»“æœçš„äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ã€‚
è¿™å¯ä»¥è®©ä½ é‡æ–°è¿è¡Œä¹‹å‰çš„åŸºå‡†æµ‹è¯•ã€‚
è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨ `-c` æ ‡å¿—æ¥ä¿å­˜äº§ç”Ÿæµ‹è¯•ç»“æœçš„äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ã€‚
æˆ‘ç»å¸¸æŠŠè¿™ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ä» `.test` é‡å‘½åä¸º `.golden` ã€‚

> NOTE golden çè´µ

```shell
% go test -c
% mv fib.test fib.golden
```

The previous  `Fib`  fuction had hard coded values for the 0th and 1st numbers in the fibonaci series. After that the code calls itself recursively. Weâ€™ll talk about the cost of recursion later today, but for the moment, assume it has a cost, especially as our algorithm uses exponential time.

å‰é¢çš„ Fib å‡½æ•°ç¡¬ç¼–ç äº† fibonacci ä¸­çš„ç¬¬0å’Œç¬¬1ä¸ªæ•°å­—ã€‚ä¹‹åçš„ä»£ç ä¼šè¿›è¡Œé€’å½’è°ƒç”¨ã€‚
æˆ‘ä»¬ç¨åä¼šè®²åˆ°é€’å½’çš„æˆæœ¬ï¼Œæš‚æ—¶å…ˆå‡è®¾å®ƒæ˜¯æœ‰æˆæœ¬çš„ï¼Œè€Œä¸”æ—¶é—´å¤æ‚åº¦æ˜¯æŒ‡æ•°é˜¶çš„ã€‚

As simple fix to this would be to hard code another number from the fibonacci series, reducing the depth of each recusive call by one.

æœ€ç®€å•çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ï¼Œç›´æ¥ç¡¬ç¼–ç å…¶ä»–å‡ ä¸ª fibonacci æ•°é‡ï¼Œç›´æ¥å‡å°‘é€’å½’çš„å°è¯•ã€‚

```go
func Fib(n int) int {
	switch n {
	case 0:
		return 0
	case 1:
		return 1
	case 2:
		return 1
	default:
		return Fib(n-1) + Fib(n-2)
	}
}
```

This file also includes a comprehensive test for  `Fib`. Donâ€™t try to improve your benchmarks without a test that verifies the current behaviour.

ä»£ç ä¸­è¿˜æœ‰ `Fib` ç›¸å…³çš„åŸºå‡†æµ‹è¯•ã€‚
åœ¨æ²¡æœ‰éªŒè¯è¿‡æ”¹è¿›åçš„å‡½æ•°ä»£ç å‰ï¼Œä¸è¦è°ƒæ•´åŸºå‡†æµ‹è¯•çš„ä»£ç ã€‚

To compare our new version, we compile a new test binary and benchmark both of them and use  `benchstat`  to compare the outputs.

ä¸ºäº†æ¯”è¾ƒæˆ‘ä»¬æ–°ç‰ˆæœ¬çš„å‡½æ•°ï¼Œå¯ä»¥åˆ†åˆ«ç¼–è¯‘ä¸¤ä¸ªåŸºå‡†æµ‹è¯•çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ benchstat æ¥æ¯”è¾ƒä¸¤ä¸ªåŸºå‡†æµ‹è¯•çš„è¾“å‡ºç»“æœã€‚

```shell
% go test -c
% ./fib.golden -test.bench=. -test.count=10 > old.txt
% ./fib.test -test.bench=. -test.count=10 > new.txt
% benchstat old.txt new.txt
name     old time/op  new time/op  delta
Fib20-8  44.3Âµs Â± 6%  25.6Âµs Â± 2%  -42.31%  (p=0.000 n=10+10)
```

There are three things to check when comparing benchmarks

æ¯”è¾ƒåŸºå‡†æµ‹è¯•ç»“æœæ—¶ï¼Œä¸»è¦å…³æ³¨è¿™ä¸‰ç‚¹

-   The variance Â± in the old and new times. 1-2% is good, 3-5% is ok, greater than 5% and some of your samples will be considered unreliable. Be careful when comparing benchmarks where one side has a high variance, you may not be seeing an improvement.

- æ–°æ—§ç»“æœä¸­å„è‡ªçš„æ–¹å·®å€¼ã€‚æœ€å¥½çš„æ–¹å·®æ˜¯ 1-2% ï¼Œå…¶æ¬¡æ˜¯ 3-5% ï¼Œ è¶…è¿‡ 5% çš„æ–¹å·®ç»“æœï¼Œè¯´æ˜è¿™ä¸ªæ ·æœ¬ç»“æœä¸å¯ä¿¡ã€‚æ— è®ºæ–°æ—§åŸºå‡†æµ‹è¯•ç»“æœä¸­å“ªä¸€ä¸ªæ–¹å·®å€¼åé«˜ï¼Œè¿™æ¬¡ä¼˜åŒ–æå‡çš„ç»“æœ(delta)éƒ½æ˜¯ä¸å‡†ç¡®çš„äº†ã€‚
    
-   p value. p values lower than 0.05 are good, greater than 0.05 means the benchmark may not be statistically significant.

- p å€¼ã€‚ä½äº 0.05 çš„ p å€¼æ˜¯æœ‰æ„ä¹‰çš„ï¼Œåªè¦åŸºå‡†æµ‹è¯•ç»“æœ p å€¼è¶…è¿‡ 0.05 ï¼Œéƒ½æ²¡æœ‰æ„ä¹‰ã€‚
    
-   Missing samples. benchstat will report how many of the old and new samples it considered to be valid, sometimes you may find only, say, 9 reported, even though you did  `-count=10`. A 10% or lower rejection rate is ok, higher than 10% may indicate your setup is unstable and you may be comparing too few samples.

- æ ·æœ¬æ•°é‡ä¸è¶³ã€‚ benchstat ä¼šæŠ¥å‘Šæ–°æ—§åŸºå‡†æµ‹è¯•ç»“æœä¸­å®ƒè®¤ä¸ºæœ‰æ•ˆçš„æ ·æœ¬æ•°é‡ï¼Œæœ‰æ—¶ä½ ä¼šå‘ç°ï¼Œå³ä½¿æŒ‡å®šäº† `-count=10` å‚æ•°ï¼Œå´ä»…æ˜¾ç¤º 9 ä¸ªæ ·æœ¬æ•°é‡ã€‚ç¼ºå°‘çš„æ ·æœ¬æ•°é‡åœ¨ 10% ä»¥ä¸‹ï¼Œéƒ½æ˜¯å¯ä»¥æ¥å—çš„ã€‚ç¼ºå¤±çš„æ ·æœ¬æ•°é‡å¤§äº 10% ï¼Œå¯èƒ½å°±æ˜¯ä½ å‚æ•°é…ç½®ä¸æ­£ç¡®ã€‚
    

### 2.4. Avoiding benchmarking start up costs å‡å°‘åŸºå‡†æµ‹è¯•çš„å¯åŠ¨æˆæœ¬

Sometimes your benchmark has a once per run setup cost.  `b.ResetTimer()`  will can be used to ignore the time accrued in setup.

æœ‰æ—¶ï¼Œåœ¨è¿è¡ŒåŸºå‡†æµ‹è¯•å‰ï¼Œå¯èƒ½éœ€è¦æ‰§è¡Œä¸€äº›æ¯”è¾ƒè€—æ—¶çš„åˆå§‹åŒ–é…ç½®ã€‚
ä½¿ç”¨ `b.ResetTimer()` å¯ä»¥å¿½ç•¥è¿™äº›åˆå§‹åŒ–é…ç½®æµªè´¹çš„æ—¶é—´ã€‚

```go
func BenchmarkExpensive(b *testing.B) {
        boringAndExpensiveSetup()
        b.ResetTimer() 
        for n := 0; n < b.N; n++ {
                // function under test
        }
}
```

Reset the benchmark timer

é‡ç½®åŸºå‡†æµ‹è¯•å®šæ—¶å™¨

If you have some expensive setup logic  _per loop_  iteration, use  `b.StopTimer()`  and  `b.StartTimer()`  to pause the benchmark timer.

å¦‚æœåœ¨å¾ªç¯è¿­ä»£ `b.N` çš„è¿‡ç¨‹æ¯æ¬¡éƒ½è¦æ‰§è¡Œä¸€äº›è€—æ—¶çš„æ“ä½œï¼Œå¯ä»¥æ­é…ä½¿ç”¨ `b.StopTimer()`  å’Œ `b.StartTimer()`  æš‚åœåŸºå‡†æµ‹è¯•è¿‡ç¨‹çš„è®¡æ—¶å™¨ã€‚

```go
func BenchmarkComplicated(b *testing.B) {
        for n := 0; n < b.N; n++ {
                b.StopTimer() 
                complicatedSetup()
                b.StartTimer() 
                // function under test
        }
}
```

Pause benchmark timer

æš‚åœè®¡æ—¶å™¨ä½¿ç”¨ `b.StopTimer()` 

Resume timer

æ¢å¤è®¡æ—¶å™¨ä½¿ç”¨ `b.StartTimer()` 



### 2.5. Benchmarking allocations åŸºå‡†æµ‹è¯•è¿‡ç¨‹çš„å†…å­˜åˆ†é…

Allocation count and size is strongly correlated with benchmark time. You can tell the  `testing`  framework to record the number of allocations made by code under test.

å†…å­˜åˆ†é…çš„æ¬¡æ•°å’Œå¤§å°ä¸åŸºå‡†æµ‹è¯•çš„è€—æ—¶å¯†åˆ‡ç›¸å…³ã€‚
ä½ è®© `testing` æ¡†æ¶è®°å½•è®°å½•è¢«æµ‹è¯•ä»£ç æ‰§è¡Œå†…å­˜åˆ†é…çš„æ¬¡æ•°ã€‚

```go
func BenchmarkRead(b *testing.B) {
        b.ReportAllocs()
        for n := 0; n < b.N; n++ {
                // function under test
        }
}
```

Here is an example using the  `bufio`  packageâ€™s benchmarks.

ä»¥ä¸‹ç¤ºä¾‹æ˜¯è¿è¡Œæ ‡å‡†åº“ä¸­ `bufio` package çš„åŸºå‡†æµ‹è¯•çš„ç»“æœã€‚

```shell
% go test -run=^$ -bench=. bufio
goos: darwin
goarch: amd64
pkg: bufio
BenchmarkReaderCopyOptimal-8            20000000               103 ns/op
BenchmarkReaderCopyUnoptimal-8          10000000               159 ns/op
BenchmarkReaderCopyNoWriteTo-8            500000              3644 ns/op
BenchmarkReaderWriteToOptimal-8          5000000               344 ns/op
BenchmarkWriterCopyOptimal-8            20000000                98.6 ns/op
BenchmarkWriterCopyUnoptimal-8          10000000               131 ns/op
BenchmarkWriterCopyNoReadFrom-8           300000              3955 ns/op
BenchmarkReaderEmpty-8                   2000000               789 ns/op            4224 B/op          3 allocs/op
BenchmarkWriterEmpty-8                   2000000               683 ns/op            4096 B/op          1 allocs/op
BenchmarkWriterFlush-8                  100000000               17.0 ns/op             0 B/op          0 allocs/op
```

You can also use the  `go test -benchmem`  flag to force the testing framework to report allocation statistics for all benchmarks run.

è¿˜å¯ä»¥ç”¨ `go test -benchmem` æ ‡å¿—å¼ºåˆ¶  testing æ¡†æ¶è®°å½•åŸºå‡†æµ‹è¯•è¿‡ç¨‹çš„å†…å­˜åˆ†é…æƒ…å†µã€‚

```shell
% go test -run=^$ -bench=. -benchmem bufio
goos: darwin
goarch: amd64
pkg: bufio
BenchmarkReaderCopyOptimal-8            20000000                93.5 ns/op            16 B/op          1 allocs/op
BenchmarkReaderCopyUnoptimal-8          10000000               155 ns/op              32 B/op          2 allocs/op
BenchmarkReaderCopyNoWriteTo-8            500000              3238 ns/op           32800 B/op          3 allocs/op
BenchmarkReaderWriteToOptimal-8          5000000               335 ns/op              16 B/op          1 allocs/op
BenchmarkWriterCopyOptimal-8            20000000                96.7 ns/op            16 B/op          1 allocs/op
BenchmarkWriterCopyUnoptimal-8          10000000               124 ns/op              32 B/op          2 allocs/op
BenchmarkWriterCopyNoReadFrom-8           500000              3219 ns/op           32800 B/op          3 allocs/op
BenchmarkReaderEmpty-8                   2000000               748 ns/op            4224 B/op          3 allocs/op
BenchmarkWriterEmpty-8                   2000000               662 ns/op            4096 B/op          1 allocs/op
BenchmarkWriterFlush-8                  100000000               16.9 ns/op             0 B/op          0 allocs/op
PASS
ok      bufio   20.366s
```



### 2.6. Watch out for compiler optimisations å°å¿ƒç¼–è¯‘å™¨çš„ä¼˜åŒ–

This example comes from  [issue 14813](https://github.com/golang/go/issues/14813#issue-140603392).

è¿™ä¸‹ä¾‹å­æ¥è‡ª [issue 14813](https://github.com/golang/go/issues/14813#issue-140603392) ã€‚

```go
const m1 = 0x5555555555555555
const m2 = 0x3333333333333333
const m4 = 0x0f0f0f0f0f0f0f0f
const h01 = 0x0101010101010101

func popcnt(x uint64) uint64 {
	x -= (x >> 1) & m1
	x = (x & m2) + ((x >> 2) & m2)
	x = (x + (x >> 4)) & m4
	return (x * h01) >> 56
}

func BenchmarkPopcnt(b *testing.B) {
	for i := 0; i < b.N; i++ {
		popcnt(uint64(i))
	}
}
```

How fast do you think this function will benchmark? Letâ€™s find out.

ä½ è§‰å¾—è¿™ä¸ªå‡½æ•°çš„åŸºå‡†æµ‹è¯•ç»“æœæœ‰å¤šå¿«ï¼Ÿæˆ‘ä»¬çœ‹çœ‹ç»“æœå§ã€‚

```shell
% go test -bench=. ./examples/popcnt/
goos: darwin
goarch: amd64
BenchmarkPopcnt-8       2000000000               0.30 ns/op
PASS
```

0.3 of a nano second; thatâ€™s basically one clock cycle. Even assuming that the CPU may have a few instructions in flight per clock tick, this number seems unreasonably low. What happened?

åªç”¨äº† 0.3 çº³ç§’ï¼Œä¹Ÿå°±æ˜¯åªæœ‰ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸã€‚
å³ä½¿ CPU åœ¨ä¸€ä¸ªæ—¶é’Ÿæ»´ç­”å†…èƒ½æ‰§è¡Œå¤šä¸ªæŒ‡ä»¤ï¼Œè¿™ä¸ªç»“æœä¹Ÿå¤ªå°äº†ã€‚
åˆ°åº•ä¸ºä»€ä¹ˆä¼šè¿™æ ·å‘¢ï¼Ÿ

> CPUæ—¶é’Ÿå‘¨æœŸè€—æ—¶ï¼Œå†…å­˜è®¿é—®è€—æ—¶ç­‰ï¼Œå¯å‚è€ƒæœ¬æ–‡ 1.11 èŠ‚ Table 2.2 Example Time Scale of System Latencies

> [Concept of clock tick and clock cycles](https://stackoverflow.com/questions/25743995/concept-of-clock-tick-and-clock-cycles)
> 
> clock tick æ—¶é’Ÿæ»´ç­” æŒ‡ç³»ç»Ÿæ—¶é’Ÿï¼Œæ˜¯ç³»ç»Ÿèƒ½è¯†åˆ«çš„æœ€å°æ—¶é—´å•ä½ã€‚
> 
> clock cycle æ—¶é’Ÿå‘¨æœŸ åˆ™æ˜¯ CPU æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„å¤„ç†å™¨è„‰å†²æ‰€èŠ±è´¹çš„æ—¶é—´ã€‚è¿™æ˜¯èƒ½ä» CPU ä¸»é¢‘è®¡ç®—å‡ºæ¥çš„ã€‚æ¯”å¦‚ 2GHz çš„å¤„ç†å™¨ï¼Œæ¯ç§’é’Ÿèƒ½æ‰§è¡Œ 2,000,000,000 clock cycles ã€‚ 


To understand what happened, we have to look at the function under benchmake,  `popcnt`.  `popcnt`  is a leaf functionâ€‰â€”â€‰it does not call any other functionsâ€‰â€”â€‰so the compiler can inline it.

è¦äº†è§£åŸå› ï¼Œæˆ‘ä»¬å¾—çœ‹çœ‹åŸºå‡†æµ‹è¯•æ—¶ `popcnt` åˆ°åº•åšäº†ä»€ä¹ˆã€‚
`popcnt`æ˜¯å¶å­å‡½æ•° - å®ƒä¸è°ƒç”¨ä»»æ„å­å‡½æ•° - æ‰€ä»¥ç¼–è¯‘å™¨ä¼šå†…è”æ­¤å‡½æ•°ã€‚

Because the function is inlined, the compiler now can see it has no side effects.  `popcnt`  does not affect the state of any global variable. Thus, the call is eliminated. This is what the compiler sees:

å› ä¸ºè¿™ä¸ªå‡½æ•°æ˜¯å†…è”çš„ï¼Œè€Œç¼–è¯‘å™¨å‘ç°å®ƒæ²¡æœ‰ä»»ä½•å‰¯ä½œç”¨ã€‚`popcnt`æ²¡æœ‰ä¿®æ”¹ä»»ä½•å…¨å±€å˜é‡çš„å€¼ã€‚å› æ­¤è¿™ä¸ªè°ƒç”¨è¢«çœç•¥äº†ã€‚
ç¼–è¯‘å™¨çœ‹åˆ°çš„ä»£ç å®é™…æ˜¯ä¸‹é¢è¿™æ ·ã€‚

```go
func BenchmarkPopcnt(b *testing.B) {
	for i := 0; i < b.N; i++ {
		// optimised away
	}
}
```

On all versions of the Go compiler that iâ€™ve tested, the loop is still generated. But Intel CPUs are really good at optimising loops, especially empty ones.

è™½ç„¶æˆ‘æ‰€æµ‹è¯•è¿‡çš„æ‰€æœ‰ Go ç¼–è¯‘å™¨ä¸­ï¼Œéƒ½ä¼šäº§ç”Ÿå¾ªç¯ä»£ç ã€‚ä½† Intel CPU å¤ªæ“…é•¿ä¼˜åŒ–å¾ªç¯è¯­è¨€äº†ï¼Œå°¤å…¶æ˜¯ç©ºå¾ªç¯ã€‚



#### 2.6.1. Exercise, look at the assembly ç»ƒä¹ ï¼Œçœ‹çœ‹æ±‡ç¼–è¯­è¨€ 

Before we go on, lets look at the assembly to confirm what we saw

ç»§ç»­è®²å…¶ä»–å†…å®¹å‰ï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹æ±‡ç¼–è¯­è¨€ç¡®è®¤ä¸‹åˆšæ‰çš„åˆ¤æ–­ã€‚

```go
% go test -gcflags=-S
```

Use `gcflags="-l -S"` to disable inlining, how does that affect the assembly output

å¯ä»¥ä½¿ç”¨ `gcflags="-l -S"` å…³é—­å†…è”ï¼Œè¿™ä¼šå½±å“ç¼–è¯‘å‡ºçš„æ±‡ç¼–ä»£ç ã€‚

> Optimisation is a good thing
> 
> The thing to take away is the same optimisations that  _make real code fast_, by removing unnecessary computation, are the same ones that remove benchmarks that have no observable side effects.
> 
> This is only going to get more common as the Go compiler improves.


> ä¼˜åŒ–æ˜¯ä¸€ä»¶å¥½äº‹
> 
> ä¼˜åŒ–æ˜¯ä¸ºäº† _æ›´å¿«æ‰§è¡ŒçœŸæ­£æœ‰ç”¨çš„ä»£ç _ ã€‚
> ç§»é™¤æ— ç”¨çš„è®¡ç®—æ˜¯ä¸€ç§ä¼˜åŒ–ï¼›åƒåŸºå‡†æµ‹è¯•è¿™æ ·ï¼Œç§»é™¤æ²¡æœ‰å‰¯ä½œç”¨çš„ä»£ç ä¹Ÿæ˜¯ä¸€ç§ä¼˜åŒ–ã€‚
>
> éšç€Goç¼–è¯‘å™¨çš„æ”¹è¿›ï¼Œè¿™ç§ä¼˜åŒ–ä¼šæ›´åŠ å¸¸è§ã€‚



#### 2.6.2. Fixing the benchmark ä¿®å¤åŸºå‡†æµ‹è¯•

Disabling inlining to make the benchmark work is unrealistic; we want to build our code with optimisations on.

åœ¨åŸºå‡†æµ‹è¯•ä¸­å…³é—­å†…è”ä¸å¤ªç°å®ï¼›å› ä¸ºæˆ‘ä»¬ç¼–è¯‘ä»£ç æ—¶ï¼Œè‚¯å®šå¸Œæœ›èƒ½å¯ç”¨å†…è”åŠŸèƒ½ä¼˜åŒ–ä»£ç ã€‚

> NOTE å¦‚æœåŸºå‡†æµ‹è¯•å…³å†…è”ï¼Œä½†ç¼–è¯‘ä»£ç æ—¶åˆå¼€å†…è”ï¼Œé‚£åŸºå‡†æµ‹è¯•ç»“æœå°±æ²¡æœ‰å‚è€ƒä»·å€¼äº†ã€‚


To fix this benchmark we must ensure that the compiler cannot  _prove_  that the body of  `BenchmarkPopcnt`  does not cause global state to change.

ä¸ºäº†ä¿®å¤è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬åªéœ€è¦è®©ç¼–è¯‘å™¨æ— æ³• _è¯æ˜_ `BenchmarkPopcnt` æ²¡æœ‰ä¿®æ”¹å…¨å±€å˜é‡å³å¯ã€‚

```go
var Result uint64

func BenchmarkPopcnt(b *testing.B) {
	var r uint64
	for i := 0; i < b.N; i++ {
		r = popcnt(uint64(i))
	}
	Result = r
}
```

This is the recommended way to ensure the compiler cannot optimise away body of the loop.

è¿™æ ·å°±èƒ½ä¿è¯ç¼–è¯‘å™¨ä¸ä¼šä¼˜åŒ–å¾ªç¯ä½“å†…çš„ä»£ç äº†ã€‚


First we  _use_  the result of calling  `popcnt`  by storing it in  `r`. Second, because  `r`  is declared locally inside the scope of  `BenchmarkPopcnt`  once the benchmark is over, the result of  `r`  is never visible to another part of the program, so as the final act we assign the value of  `r`  to the package public variable  `Result`.

é¦–å…ˆï¼Œæˆ‘ä»¬æŠŠè°ƒç”¨ `popcnt` è¿”å›çš„ç»“æœä¿å­˜åœ¨å˜é‡ `r` ä¸­ã€‚
å› ä¸º`r`æ˜¯`BenchmarkPopcnt`æ˜¯å±€éƒ¨å˜é‡ï¼Œæ‰€ä»¥ä¸€æ—¦åŸºå‡†æµ‹è¯•å®Œæ¯•ï¼Œå˜é‡`r`çš„å€¼å¯¹ç¨‹åºå†…å…¶ä»–ä»£ç éƒ½ä¸å¯è§ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬è¿˜è¦æŠŠ`r`çš„å€¼åˆ†é…åˆ° package å…¬å¼€å˜é‡ `Result` ã€‚

Because  `Result`  is public the compiler cannot prove that another package importing this one will not be able to see the value of  `Result`  changing over time, hence it cannot optimise away any of the operations leading to its assignment.

å› ä¸º`Result`æ˜¯å…¬å¼€å˜é‡ï¼Œæ‰€ä»¥ç¼–è¯‘å™¨æ— æ³•åˆ¤æ–­å…¶ä»– package ä½•æ—¶ä¼šåœ¨å¯¼å…¥å½“å‰ package åè®¿é—®`Result`çš„å–å€¼ï¼Œå› æ­¤ç¼–è¯‘å™¨ä¸ä¼šéšæ„å»é™¤å…¬å…±å˜é‡çš„èµ‹å€¼è¯­å¥è¿›è¡Œä¼˜åŒ–çš„ã€‚


What happens if we assign to  `Result`  directly? Does this affect the benchmark time? What about if we assign the result of  `popcnt`  to  `_`?

å¦‚æœæˆ‘ä»¬ä½¿ç”¨å±€éƒ¨å˜é‡`r`ï¼Œç›´æ¥èµ‹å€¼ç»™`Result`ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿè¿™ä¼šå½±å“åŸºå‡†æµ‹è¯•çš„æ—¶é—´å—ï¼Ÿ
å¦‚æœæˆ‘ä»¬æŠŠ `popcnt` çš„ç»“æœèµ‹å€¼ç»™ `_`  åˆä¼šæ€æ ·å‘¢ï¼Ÿ

In our earlier  `Fib`  benchmark we didnâ€™t take these precautions, should we have done so?

åœ¨ä»¥å‰çš„ `Fib` åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰é˜²èŒƒè¿™äº›æƒ…å†µï¼Œé‚£æˆ‘ä»¬æ˜¯å¦åº”è¯¥è€ƒè™‘å¹¶é˜²èŒƒè¿™äº›é—®é¢˜å‘¢ï¼Ÿ


### 2.7. Benchmark mistakes é”™è¯¯çš„åŸºå‡†æµ‹è¯•

The  `for`  loop is crucial to the operation of the benchmark.

`for`å¾ªç¯æ˜¯åŸºå‡†æµ‹è¯•çš„å…³ç³»éƒ¨åˆ†ã€‚

Here are two incorrect benchmarks, can you explain what is wrong with them?

ä¸‹é¢ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½ èƒ½è§£é‡Šä¸€ä¸‹ä»–ä»¬åˆ†åˆ«é”™åœ¨å“ªé‡Œå—ï¼Ÿ

```go
func BenchmarkFibWrong(b *testing.B) {
	Fib(b.N)
}
```

```go
func BenchmarkFibWrong2(b *testing.B) {
	for n := 0; n < b.N; n++ {
		Fib(n)
	}
}
```

TODO éªŒè¯ï¼š1.åº”è¯¥æ˜¯æ²¡æœ‰ä¿®æ”¹å…¨å±€çŠ¶æ€ï¼Œæ‰€ä»¥è¢«ä¼˜åŒ–äº†ã€‚ 2. b.N è¡¨ç¤ºæ‰§è¡Œ N æ¬¡å‡½æ•°ï¼Œè€Œéè°ƒç”¨ Fib æ—¶ä¼ å‚æ•°ä¸ºN ã€‚

Run these benchmarks, what do you see?

è¿è¡Œè¿™äº›åŸºå‡†æµ‹è¯•ï¼Œä½ çœ‹åˆ°äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼Ÿ



### 2.8. Profiling benchmarks å¸¦æ€§èƒ½åˆ†æçš„åŸºå‡†æµ‹è¯•

> Profiling æ˜¯æŒ‡åœ¨ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæ”¶é›†èƒ½å¤Ÿåæ˜ ç¨‹åºæ‰§è¡ŒçŠ¶æ€çš„æ•°æ®ã€‚åœ¨è½¯ä»¶å·¥ç¨‹ä¸­ï¼Œæ€§èƒ½åˆ†æï¼ˆperformance analysisï¼Œä¹Ÿç§°ä¸º profilingï¼‰ï¼Œæ˜¯ä»¥æ”¶é›†ç¨‹åºè¿è¡Œæ—¶ä¿¡æ¯ä¸ºæ‰‹æ®µç ”ç©¶ç¨‹åºè¡Œä¸ºçš„åˆ†ææ–¹æ³•ï¼Œæ˜¯ä¸€ç§åŠ¨æ€ç¨‹åºåˆ†æçš„æ–¹æ³•ã€‚[^qcraoPPROF]

The  `testing`  package has built in support for generating CPU, memory, and block profiles.

`testing` package æ”¯æŒç”Ÿæˆ CPU memory block profile ç­‰ç±»å‹çš„æ€§èƒ½åˆ†ææ–‡ä»¶ã€‚

-   `-cpuprofile=$FILE`  writes a CPU profile to  `$FILE`.
    
-   `-memprofile=$FILE`, writes a memory profile to  `$FILE`,  `-memprofilerate=N`  adjusts the profile rate to  `1/N`.  è®¾ç½®å†…å­˜é‡‡æ ·é€Ÿç‡ã€‚ [golang.org MemProfileRate](https://golang.org/pkg/runtime/#pkg-variables)
    
-   `-blockprofile=$FILE`, writes a block profile to  `$FILE`.
    

Using any of these flags also preserves the binary.

```
% go test -run=XXX -bench=. -cpuprofile=c.p bytes
% go tool pprof c.p
```

### 2.9. Discussion è®¨è®º

Are there any questions?

Perhaps it is time for a break.



## 3. Performance measurement and profiling æ€§èƒ½è¯„ä¼°ä¸åˆ†æ

In the previous section we looked at benchmarking individual functions which is useful when you know ahead of time where the bottlekneck is. However, often you will find yourself in the position of asking

> Why is this program taking so long to run?


ä¸Šä¸€èŠ‚ä»‹ç»çš„åŸºå‡†æµ‹è¯•ç”¨äºåˆ†æå•ä¸ªå‡½æ•°çš„æ€§èƒ½ç“¶é¢ˆã€‚
ä½†æˆ‘ä»¬ç»å¸¸é‡åˆ°çš„é—®é¢˜çš„

> ä¸ºä»€ä¹ˆè¿™ä¸ªç¨‹åºè¦è¿è¡Œè¿™ä¹ˆé•¿æ—¶é—´ï¼Ÿ


Profiling  _whole_  programs which is useful for answering high level questions like. In this section weâ€™ll use profiling tools built into Go to investigate the operation of the program from the inside.

åªæœ‰å¯¹ _æ•´ä¸ª_ ç¨‹åºè¿›è¡Œåˆ†æï¼Œæ‰èƒ½å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ Go å†…ç½®çš„ profile tool æ¥è°ƒæ•´ç¨‹åºå†…éƒ¨çš„è¿è¡Œæƒ…å†µã€‚



### 3.1. pprof

The first tool weâ€™re going to be talking about today is  _pprof_.  [pprof](https://github.com/google/pprof)  descends from the  [Google Perf Tools](https://github.com/gperftools/gperftools)  suite of tools and has been integrated into the Go runtime since the earliest public releases.

`pprof`  consists of two parts:

-   `runtime/pprof`  package built into every Go program
    
-   `go tool pprof`  for investigating profiles.


ç¬¬ä¸€ä¸ªè¦è®¨è®ºçš„å·¥å…·æ˜¯ _pprof_ ã€‚  [pprof](https://github.com/google/pprof)  æ¥è‡ª [Google Perf Tools](https://github.com/gperftools/gperftools) ï¼Œåœ¨é¦–æ¬¡å…¬å¼€å‘ç‰ˆæ—¶ï¼Œå°±åŒ…é›†æˆåˆ°äº† Go è¿è¡Œæ—¶å†…ã€‚

`pprof` åŒ…å«ä»¥ä¸‹ä¸¤éƒ¨åˆ†ï¼š

-   `runtime/pprof`  æ˜¯ä¸€ä¸ªå¯åœ¨ Go ç¨‹åºä»£ç ä¸­å¼•ç”¨ package ã€‚
    
-   `go tool pprof`  æ˜¯è¿›è¡Œæ€§èƒ½åˆ†æçš„å·¥å…·ã€‚



### 3.2. Types of profiles æ€§èƒ½åˆ†æçš„ç§ç±»

pprof supports several types of profiling, weâ€™ll discuss three of these today:

-   CPU profiling.
    
-   Memory profiling.
    
-   Block (or blocking) profiling.
    
-   Mutex contention profiling. 
    
pprof æ”¯æŒåˆ†æä»¥ä¸‹å‡ ç§ç±»å‹ï¼š

- CPU æ€§èƒ½å‰–æ
- å†…å­˜æ€§èƒ½å‰–æ
- é˜»å¡å‰–æ
- é”ï¼ˆäº’æ–¥é‡ï¼‰äº‰ç”¨å‰–æ




#### 3.2.1. CPU profiling [^qcraoPPROF]

CPU profiling is the most common type of profile, and the most obvious.

When CPU profiling is enabled the runtime will interrupt itself every 10ms and record the stack trace of the currently running goroutines.

Once the profile is complete we can analyse it to determine the hottest code paths.

The more times a function appears in the profile, the more time that code path is taking as a percentage of the total runtime.

æœ€å¸¸ç”¨çš„æ˜¯ CPU profileã€‚

å¯ç”¨ CPU profile åï¼Œruntime æ¯éš” 10ms ä¸­æ–­ä¸€æ¬¡ï¼Œç„¶åè®°å½•å½“å‰çš„å †æ ˆã€‚

profile å®Œæ¯•åï¼Œå°±èƒ½ analyseï¼ˆåˆ†æï¼‰ å‡º hottest code path ï¼ˆçƒ­ç‚¹ä»£ç ï¼‰ã€‚

åœ¨ profile ä¸­å‡ºç°æ¬¡æ•°è¶Šå¤šçš„å‡½æ•°ï¼Œåœ¨ code path ä¸­æ‰€å æ¯”é‡å°±æœ€å¤§ã€‚




#### 3.2.2. Memory profiling [^qcraoPPROF]

Memory profiling records the stack trace when a  _heap_  allocation is made.

Stack allocations are assumed to be free and are  _not tracked_  in the memory profile.

Memory profiling, like CPU profiling is sample based, by default memory profiling samples 1 in every 1000 allocations. This rate can be changed.

Because of memory profiling is sample based and because it tracks  _allocations_  not  _use_, using memory profiling to determine your applicationâ€™s overall memory usage is difficult.

> Personal Opinion:  I do not find memory profiling useful for finding memory leaks. There are better ways to determine how much memory your application is using. We will discuss these later in the presentation.



Memory profile æ˜¯åœ¨å †(Heap)åˆ†é…çš„æ—¶å€™ï¼Œè®°å½•ä¸€ä¸‹è°ƒç”¨å †æ ˆã€‚

æ ˆ(Stack)åˆ†é…ç”±äºä¼šéšæ—¶é‡Šæ”¾ï¼Œå› æ­¤ä¸ä¼šè¢«å†…å­˜åˆ†ææ‰€è®°å½•ã€‚

ä¸ CPU profile ç±»ä¼¼ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒMemory profile æ¯ 1000 æ¬¡åˆ†é…ï¼Œå–æ ·ä¸€æ¬¡ï¼Œè¿™ä¸ªæ•°å€¼å¯ä»¥æ”¹å˜ã€‚

ç”±äºå†…å­˜åˆ†ææ˜¯å–æ ·æ–¹å¼ï¼Œå¹¶ä¸”ä¹Ÿå› ä¸ºå…¶è®°å½•çš„æ˜¯åˆ†é…çš„å†…å­˜ï¼Œè€Œä¸æ˜¯ä½¿ç”¨çš„å†…å­˜ã€‚å› æ­¤ä½¿ç”¨å†…å­˜æ€§èƒ½åˆ†æå·¥å…·æ¥å‡†ç¡®åˆ¤æ–­ç¨‹åºå…·ä½“çš„å†…å­˜ä½¿ç”¨æ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚

> ä¸ªäººè§‚ç‚¹:  Memory profile ä¸èƒ½ç”¨äºæŸ¥æ‰¾å†…å­˜æ³„æ¼ã€‚æœ‰å…¶ä»–æ›´å¥½çš„æ–¹æ³•æ¥è·Ÿè¸ªç¨‹åºå ç”¨çš„å†…å­˜å¤§å°ã€‚æˆ‘ä»¬åé¢è®¨è®ºã€‚



#### 3.2.3. Block profiling

Block profiling is quite unique to Go.

A block profile is similar to a CPU profile, but it records the amount of time a goroutine spent waiting for a shared resource.

This can be useful for determining  _concurrency_  bottlenecks in your application.

Block profiling can show you when a large number of goroutines  _could_  make progress, but were  _blocked_. Blocking includes:

-   Sending or receiving on a unbuffered channel.
    
-   Sending to a full channel, receiving from an empty one.
    
-   Trying to  `Lock`  a  `sync.Mutex`  that is locked by another goroutine.

Block profiling is a very specialised tool, it should not be used until you believe you have eliminated all your CPU and memory usage bottlenecks.


Block profile æ˜¯ Go è¯­è¨€ä¸­ç‰¹æœ‰çš„ä¸€ç§åˆ†ææ–¹æ³•ã€‚

Block profile ä¸ CPU profile å¾ˆåƒï¼Œä½†å®ƒè®°å½•çš„æ˜¯ goroutine ç­‰å¾…å…±äº«èµ„æºæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚

åœ¨åˆ†æç¨‹åº _å¹¶å‘_ ç“¶é¢ˆæ—¶ï¼Œååˆ†æœ‰ç”¨ã€‚

Block profile å¯åˆ†æå‡ºå“ªäº›æ—¶é—´ï¼Œå‡ºç°äº†å¤§é‡ goroutine åŒæ—¶å¤„äº block çŠ¶æ€çš„æƒ…å†µã€‚

å¯èƒ½å¼•èµ· block çš„åŸå› å¦‚ä¸‹ï¼š

- å‘é€æˆ–æ¥æ”¶æ— ç¼“å†²çš„ channel æ—¶ã€‚
- å‘å·²æ»¡çš„ channel ä¸­å†™æ•°æ®ï¼Œæˆ–ä»ç©ºçš„ channel ä¸­è¯»æ•°æ®æ—¶ã€‚
- å°è¯• Lock ä¸€ä¸ªå·²ç»è¢«å…¶ä»– goroutine é”ä½çš„ sync.Mutex æ—¶ã€‚

Block profile å¾ˆç‰¹æ®Šï¼Œåœ¨æ’é™¤ CPU å’Œ Memory çš„æ€§èƒ½ç“¶é¢ˆå‰ï¼Œä¸è¦ä½¿ç”¨å®ƒæ¥åˆ†æã€‚



#### 3.2.4. Mutex profiling

Mutex profiling is simlar to Block profiling, but is focused exclusively on operations that lead to delays caused by mutex contention.

I donâ€™t have a lot of experience with this type of profile but I have built an example to demonstrate it. Weâ€™ll look at that example shortly.

Mutex profile ä¸ Block profile ç±»ä¼¼ï¼Œä½†å®ƒä¸“é—¨åˆ†æäº’æ–¥é‡äº‰ç”¨æ‰€å¯¼è‡´çš„å»¶è¿Ÿã€‚

æˆ‘æ²¡æœ‰å¤šå°‘ Mutext profile ç›¸å…³çš„ä½¿ç”¨ç»éªŒï¼Œä½†åé¢ä¼šæœ‰ä¸€ä¸ªç¤ºä¾‹æ¼”ç¤ºå…·ä½“ç”¨æ³•ã€‚



### 3.3. One profile at at time æ¯æ¬¡åªåˆ†æä¸€ç§ç±»å‹

Profiling is not free.

profile æ˜¯æœ‰ä»£ä»·çš„ã€‚

Profiling has a moderate, but measurable impact on program performanceâ€”especially if you increase the memory profile sample rate.

æ‰§è¡Œ profile çš„è¿‡ç¨‹å¯¹ç¨‹åºæœ‰ä¸€å®šæ€§èƒ½æŸè€—ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜ memory profile é‡‡æ ·ç‡æ—¶ã€‚

Most tools will not stop you from enabling multiple profiles at once.

ä½†æ˜¯å¤šæ•°å·¥å…·éƒ½ä¸ç¦æ­¢ä½ åŒæ—¶å¼€å¯å¤šä¸ª profile ã€‚

Do not enable more than one kind of profile at a time.

If you enable multiple profileâ€™s at the same time, they will observe their own interactions and throw off your results.

åŒæ—¶å¼€å¯å¤šç§ profile æ—¶ï¼Œåœ¨åˆ†æç»“æœä¼šäº’ç›¸å½±å“ã€‚


### 3.4. Collecting a profile æ”¶é›†åˆ†æç»“æœ

The Go runtimeâ€™s profiling interface lives in the  `runtime/pprof`  package.  `runtime/pprof`  is a very low level tool, and for historic reasons the interfaces to the different kinds of profile are not uniform.

Go è¿è¡Œæ—¶ profile æ¥å£ä½äº `runtime/pprof` package ä¸­ã€‚
è¿™æ˜¯ä¸€ä¸ªå¾ˆä½å±‚çš„æ¥å£ã€‚
ç”±äºå†å²åŸå› ï¼Œä¸åŒç±»å‹çš„ profile æ¥å£ä¹Ÿä¸ç»Ÿä¸€ã€‚

As we saw in the previous section, pprof profiling is built into the  `testing`  package, but sometimes its inconvenient, or difficult, to place the code you want to profile in the context of at  `testing.B`  benchmark and must use the  `runtime/pprof`  API directly.

pprof profile æ˜¯å†…ç½®åœ¨ `testing` package ä¸­çš„ï¼Œå¦‚æœä¸æ–¹ä¾¿åœ¨ `testing.B` åŸºå‡†æµ‹è¯•ä¸­æ”¾ç½® profile ä»£ç æ—¶ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ `runtime/pprof` API ã€‚


A few years ago I wrote a [pkg/profile](https://github.com/pkg/profile) package, to make it easier to profile an existing application.

å‡ å¹´å‰ï¼Œæˆ‘å®ç°äº†ä¸€ä¸ªç”Ÿæˆ profile çš„ package [pkg/profile](https://github.com/pkg/profile) ã€‚

```go
import "github.com/pkg/profile"

func main() {
	defer profile.Start().Stop()
	// ...
}
```

Weâ€™ll use the profile package throughout this section. Later in the day weâ€™ll touch on using the  `runtime/pprof`  interface directly.

æœ¬èŠ‚æˆ‘ä»¬å°±ä¼šç”¨è¿™ä¸ª pkg/profile package è¿›è¡Œæ¼”ç¤ºã€‚åé¢å‡ å¤©ï¼Œæ‰ä¼šç›´æ¥ä½¿ç”¨ `runtime/pprof` æ¥å£ã€‚




### 3.5. Analysing a profile with pprof ä½¿ç”¨ pprof åˆ†æå‰–æç»“æœ 

Now that weâ€™ve talked about what pprof can measure, and how to generate a profile, letâ€™s talk about how to use pprof to analyse a profile.

åˆšæ‰è®¨è®ºè¿‡ï¼Œpprof èƒ½å‰–æå“ªäº›å†…å®¹ï¼Œä»¥æœ‰å¦‚ä½•ç”Ÿæˆ profile æ–‡ä»¶ã€‚
ç°åœ¨æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ç”¨ pprof åˆ†æ profile æ–‡ä»¶å§ã€‚

The analysis is driven by the  `go pprof`  subcommand

go tool pprof /path/to/your/profile

This tool provides several different representations of the profiling data; textual, graphical, even flame graphs.

æ­¤å·¥å…·å¯ä»¥ï¼šæ–‡æœ¬ï¼Œå›¾å½¢ï¼Œç”šè‡³ç«ç„°å›¾ç­‰å‡ ç§æ–¹å¼å±•ç° profile æ•°æ®ã€‚


If youâ€™ve been using Go for a while, you might have been told that  `pprof`  takes two arguments. Since Go 1.9 the profile file contains all the information needed to render the profile. You do no longer need the binary which produced the profile. ğŸ‰

å¦‚æœä½ ç”¨è¿‡è¾ƒæ—©ç‰ˆæœ¬çš„ Goï¼Œå¯èƒ½é‡åˆ° `pprof` å‘½ä»¤è¦æ±‚æä¾›ä¸¤ä¸ªå‚æ•°çš„æƒ…å†µã€‚å³åŒæ—¶æä¾›  profile æ–‡ä»¶ ä¸ ç”Ÿæˆ profile æ—¶è¿è¡Œçš„äºŒè¿›åˆ¶ç¨‹åºæ–‡ä»¶ï¼Œæ‰èƒ½è¾“å‡ºåˆ†æç»“æœã€‚
ä» Go 1.9 ç‰ˆæœ¬å¼€å§‹ï¼Œåªéœ€è¦ä¸€ä¸ª profile æ–‡ä»¶ä¸­å°±èƒ½æ‰§è¡Œåˆ†æï¼Œå¹¶è¾“å‡ºåˆ†æç»“æœã€‚



#### 3.5.1. Further reading å»¶ä¼¸é˜…è¯»

-   [Profiling Go programs](http://blog.golang.org/profiling-go-programs)  (Go Blog)
    
-   [Debugging performance issues in Go programs](https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs)
 


#### 3.5.2. CPU profiling (exercise)

Letâ€™s write a program to count words:

å†™ä¸€ä¸ªè®¡ç®—å•è¯ä¸ªæ•°çš„ç¨‹åºï¼š

```go
package main

import (
	"fmt"
	"io"
	"log"
	"os"
	"unicode"

	"github.com/pkg/profile"
)

func readbyte(r io.Reader) (rune, error) {
	var buf [1]byte
	_, err := r.Read(buf[:])
	return rune(buf[0]), err
}

func main() {
	defer profile.Start().Stop()

	f, err := os.Open(os.Args[1])
	if err != nil {
		log.Fatalf("could not open file %q: %v", os.Args[1], err)
	}

	words := 0
	inword := false
	for {
		r, err := readbyte(f)
		if err == io.EOF {
			break
		}
		if err != nil {
			log.Fatalf("could not read file %q: %v", os.Args[1], err)
		}
		if unicode.IsSpace(r) && inword {
			words++
			inword = false
		}
		inword = unicode.IsLetter(r)
	}
	fmt.Printf("%q: %d words\n", os.Args[1], words)
}
```

Letâ€™s see how many words there are in Herman Melvilleâ€™s classic  [Moby Dick](https://www.gutenberg.org/ebooks/2701)  (sourced from Project Gutenberg)

æˆ‘ä»¬çœ‹çœ‹ èµ«å°”æ›¼Â·æ¢…å°”ç»´å°”ç»å…¸å°è¯´ ã€ŠMoby Dickç™½é²¸è®°ã€‹ æœ‰å¤šå°‘ä¸ªå•è¯å§ã€‚

```txt
% go build && time ./words moby.txt
"moby.txt": 181275 words

real    0m2.110s
user    0m1.264s
sys     0m0.944s
```

Letâ€™s compare that to unixâ€™s  `wc -w`

å†ä¸ unix ä¸­çš„æ ‡å‡†å•è¯è®¡æ•°ç¨‹åº `wc -w` æ¯”è¾ƒä¸€ä¸‹

```txt
% time wc -w moby.txt
215829 moby.txt

real    0m0.012s
user    0m0.009s
sys     0m0.002s
```

So the numbers arenâ€™t the same.  `wc`  is about 19% higher because what it considers a word is different to what my simple program does. Thatâ€™s not important - both programs take the whole file as input and in a single pass count the number of transitions from word to non word.

Letâ€™s investigate why these programs have different run times using pprof.

`wc` å‘½ä»¤ç®—å‡ºçš„å•è¯æ•°é‡æ¯”æˆ‘ä»¬çš„å¤šäº† 19% ï¼Œ è¿™æ˜¯å› ä¸ºä¸¤ä¸ªç¨‹åºè¯†åˆ«å•è¯çš„æ ‡å‡†ä¸ä¸€æ ·å¯¼è‡´ã€‚
è¿™ä¸ªå•ç‹¬å¹¶ä¸é‡å¯ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨çš„é—®é¢˜æ˜¯ï¼šä¸¤ä¸ªç¨‹åºéƒ½ä»æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰æ•°æ®ç„¶åè®¡ç®—å•è¯æ•°ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬çš„ç¨‹åºèŠ±è´¹çš„æ—¶é—´æ›´é•¿å‘¢ï¼Ÿ

æˆ‘ä»¬ç”¨ pprof å·¥å…·åˆ†æä¸€ä¸‹çœ‹çœ‹ã€‚




#### 3.5.3. Add CPU profiling å¢åŠ  CPU profile

First, edit  `main.go`  and enable profiling

ç¼–è¾‘ `main.go` æ–‡ä»¶ï¼Œå¯ç”¨ profile 

```go
import (
        "github.com/pkg/profile"
)

func main() {
        defer profile.Start().Stop()
        // ...
```

Now when we run the program a  `cpu.pprof`  file is created.

ç°åœ¨ï¼Œæˆ‘ä»¬è¿è¡Œç¨‹åºï¼Œä¸€ä¸ª `cpu.pprof` æ–‡ä»¶å°±ä¼šè‡ªåŠ¨ç”Ÿæˆ ã€‚


```txt
% go run main.go moby.txt
2018/08/25 14:09:01 profile: cpu profiling enabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof
"moby.txt": 181275 words
2018/08/25 14:09:03 profile: cpu profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof
```

Now we have the profile we can analyse it with  `go tool pprof`

ç°åœ¨æˆ‘ä»¬å°±èƒ½ç”¨ `go tool pprof` å·¥å…·ç›´æ¥åˆ†æè¿™ä¸ª profile æ–‡ä»¶äº†ã€‚

```txt
% go tool pprof /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof
Type: cpu
Time: Aug 25, 2018 at 2:09pm (AEST)
Duration: 2.05s, Total samples = 1.36s (66.29%)
Entering interactive mode (type "help" for commands, "o" for options)
(pprof) top
Showing nodes accounting for 1.42s, 100% of 1.42s total
      flat  flat%   sum%        cum   cum%
     1.41s 99.30% 99.30%      1.41s 99.30%  syscall.Syscall
     0.01s   0.7%   100%      1.42s   100%  main.readbyte
         0     0%   100%      1.41s 99.30%  internal/poll.(*FD).Read
         0     0%   100%      1.42s   100%  main.main
         0     0%   100%      1.41s 99.30%  os.(*File).Read
         0     0%   100%      1.41s 99.30%  os.(*File).read
         0     0%   100%      1.42s   100%  runtime.main
         0     0%   100%      1.41s 99.30%  syscall.Read
         0     0%   100%      1.41s 99.30%  syscall.read
```

The  `top`  command is one youâ€™ll use the most. We can see that 99% of the time this program spends in  `syscall.Syscall`, and a small part in  `main.readbyte`.

æœ€å¸¸ç”¨çš„å‘½ä»¤æ˜¯ `top` ã€‚
è¿™ä¸ªç¨‹åº 99% çš„æ—¶é—´ç”¨èŠ±åœ¨ `syscall.Syscall` ä¸Šï¼Œ
å¦ä¸€éƒ¨åˆ†æ—¶é—´èŠ±åœ¨äº† `main.readbyte` ã€‚


We can also visualise this call the with the  `web`  command. This will generate a directed graph from the profile data. Under the hood this uses the  `dot`  command from Graphviz.

æˆ‘ä»¬è¿˜å¯ä»¥ç”¨ `web` å‘½ä»¤è§‚å¯Ÿå‰–æç»“æœã€‚è¿™ä¼šæ ¹æ® profile ä¸­çš„æ•°æ®ç”Ÿæˆä¸€å¼ å›¾è¡¨ã€‚å‘½ä»¤å†…éƒ¨ä¼šè°ƒç”¨ Graphviz å·¥å…·çš„ `dot` å‘½ä»¤ç”Ÿæˆçš„å›¾è¡¨ï¼ˆè¯‘ï¼šæ‰€ä»¥åœ¨æ‰§è¡Œ pprof åˆ†æçš„ä¸»æœºä¸­ï¼Œè¦å®‰è£… Graphviz ç›¸å…³å‘½ä»¤ï¼‰ã€‚


However, in Go 1.10 (possibly 1.11) Go ships with a version of pprof that natively supports a http sever

ä½†æ˜¯ï¼Œåœ¨ Go 1.10 (ä¹Ÿå¯èƒ½æ˜¯ 1.11ï¼‰ç‰ˆæœ¬ä¸­ï¼Œ pprof å†…ç½®äº†ä¸€ä¸ª HTTP æœåŠ¡å™¨ã€‚


```txt
% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof
```

Will open a web browser;

-   Graph mode
    
-   Flame graph mode

æ‰§è¡Œä¸Šé¢çš„å‘½ä»¤ï¼Œå°±ä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œæ”¯æŒ

- å›¾è¡¨æ¨¡å¼
- ç«ç„°å›¾æ¨¡å¼
    

On the graph the box that consumes the  _most_  CPU time is the largestâ€‰â€”â€‰we see  `syscall.Syscall`  at 99.3% of the total time spent in the program. The string of boxes leading to  `syscall.Syscall`  represent the immediate callersâ€‰â€”â€‰there can be more than one if multiple code paths converge on the same function. The size of the arrow represents how much time was spent in children of a box, we see that from  `main.readbyte`  onwards they account for near 0 of the 1.41 second spent in this arm of the graph.

åœ¨å›¾è¡¨ä¸­ï¼Œæ–¹æ¡†æœ€å¤§çš„å›¾å½¢å ç”¨CPUä¹Ÿæœ€å¤šã€‚
æ–¹æ¡†ä¸Šçš„è¿çº¿ï¼Œè¡¨ç¤ºæ­¤å‡½æ•°çš„è°ƒç”¨æ–¹ã€‚å¦‚æœæœ‰å¤šæ¡ä»£ç è·¯å¾„è°ƒç”¨ç›¸åŒçš„å‡½æ•°ï¼Œå°±ä¼šå‡ºç°å¤šæ¡çº¿ã€‚
ç®­å¤´çš„å¤§å°è¡¨ç¤ºè¢«è°ƒç”¨æ–¹ï¼ˆå­æ–¹æ¡†ï¼‰èŠ±è´¹çš„æ—¶é—´ã€‚

åœ¨æœ¬å›¾ä¸­ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼š
å ç”¨ç¨‹åº 99.3% æ—¶é—´çš„æ˜¯`syscall.Syscall` ã€‚
`main.readbyte`æ–¹æ¡†å ç”¨äº† 1.41 ç§’ï¼Œä½†`readbyte`å‡½æ•°æœ¬èº«åªå ç”¨äº† 0 ç§’ï¼Œå…¶å­å‡½æ•° `File.Read() å ç”¨äº† 1.41 ç§’ã€‚


_Question_: Can anyone guess why our version is so much slower than  `wc`?

_é—®é¢˜_ï¼š è°çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„ç¨‹åºæ¯” `wc` æ…¢äº†è¿™ä¹ˆå¤šå‘¢ï¼Ÿ


> æ·±åº¦è§£å¯†Goè¯­è¨€ä¹‹pprof[^qcraoPPROF]

 åˆ—å | å«ä¹‰
----- | -------------------------------------------
flat  | æœ¬å‡½æ•°çš„æ‰§è¡Œè€—æ—¶, the time in a function
flat% | flat å  CPU æ€»æ—¶é—´çš„æ¯”ä¾‹ã€‚ç¨‹åºæ€»è€—æ—¶ 16.22s, Eat çš„ 16.19s å äº† 99.82%
sum%  | å‰é¢æ¯ä¸€è¡Œçš„ flat å æ¯”æ€»å’Œ
cum   | ç´¯è®¡é‡ã€‚æŒ‡è¯¥å‡½æ•°åŠ ä¸Šè¯¥å‡½æ•°è°ƒç”¨çš„å‡½æ•°æ€»è€—æ—¶ã€‚cumulative time a function and everything below it.
cum%  | cum å  CPU æ€»æ—¶é—´çš„æ¯”ä¾‹

> æ–¹æ¡†ä¸­æ–‡å­—çš„å«ä¹‰ syscall.Syscall 760ms(84.44%) of 820ms(91.11%) 
> 
> 760ms è¡¨ç¤º flat æ—¶é—´ï¼› 820ms è¡¨ç¤º cumulate æ—¶é—´ï¼›



#### 3.5.4. Improving our version ä¼˜åŒ–æˆ‘ä»¬çš„ç¨‹åº

The reason our program is slow is not because Goâ€™s  `syscall.Syscall`  is slow. It is because syscalls in general are expensive operations (and getting more expensive as more Spectre family vulnerabilities are discovered).

Each call to  `readbyte`  results in a syscall.Read with a buffer size of 1. So the number of syscalls executed by our program is equal to the size of the input. We can see that in the pprof graph that reading the input dominates everything else.

æˆ‘ä»¬çš„ç¨‹åºæ…¢ï¼Œå¹¶éç”±äº Go çš„ `syscall.Syscall` æ¥å£æ…¢å¯¼è‡´çš„ã€‚
è€Œæ˜¯å› ä¸º syscall ç³»ç»Ÿè°ƒç”¨åŸæœ¬å°±æ˜¯å¼€é”€å·¨å¤§çš„ä¸€ç§æ“ä½œï¼ˆä¸ºäº†ä¿®å¤è¶Šæ¥è¶Šå¤šçš„å®‰å…¨æ¼æ´ï¼Œè¿™ç§å¼€é”€ä¹Ÿä¼šè¶Šæ¥è¶Šå¤§ï¼‰ã€‚

æ¯æ¬¡ `readbyte` éƒ½ä¼šè§¦å‘ä¸€æ¬¡ syscall.Read ç³»ç»Ÿè°ƒç”¨ï¼Œè€Œä¸” buffer size æ˜¯ 1 ï¼Œæ‰€ä»¥ç³»ç»Ÿè°ƒç”¨çš„æ¬¡æ•°å°±æ˜¯æ–‡ä»¶åœ¨å­—èŠ‚æ•°ã€‚
æ‰€ä»¥èƒ½ä» pprof å›¾è¡¨ä¸­çœ‹åˆ°ï¼Œä¸»è¦è€—æ—¶éƒ½åœ¨è¯»å–æ•°æ®çš„è¿‡ç¨‹ã€‚

> NOTE Spectre family vulnerabilities å¹½çµæ˜¯ä¸€ä¸ªå­˜åœ¨äºåˆ†æ”¯é¢„æµ‹å®ç°ä¸­çš„ç¡¬ä»¶ç¼ºé™·åŠå®‰å…¨æ¼æ´ï¼Œå«æœ‰é¢„æµ‹æ‰§è¡ŒåŠŸèƒ½çš„ç°ä»£å¾®å¤„ç†å™¨å‡å—å…¶å½±å“ï¼Œæ¼æ´åˆ©ç”¨æ˜¯åŸºäºæ—¶é—´çš„æ—è·¯æ”»å‡»ï¼Œå…è®¸æ¶æ„è¿›ç¨‹è·å¾—å…¶ä»–ç¨‹åºåœ¨æ˜ å°„å†…å­˜ä¸­çš„æ•°æ®å†…å®¹ã€‚ [ç»´åŸºç™¾ç§‘](https://zh.wikipedia.org/zh-cn/%E5%B9%BD%E7%81%B5%E6%BC%8F%E6%B4%9E)



```go
func main() {
	defer profile.Start(profile.MemProfile, profile.MemProfileRate(1)).Stop()
	// defer profile.Start(profile.MemProfile).Stop()

	f, err := os.Open(os.Args[1])
	if err != nil {
		log.Fatalf("could not open file %q: %v", os.Args[1], err)
	}

	b := bufio.NewReader(f)
	words := 0
	inword := false
	for {
		r, err := readbyte(b)
		if err == io.EOF {
			break
		}
		if err != nil {
			log.Fatalf("could not read file %q: %v", os.Args[1], err)
		}
		if unicode.IsSpace(r) && inword {
			words++
			inword = false
		}
		inword = unicode.IsLetter(r)
	}
	fmt.Printf("%q: %d words\n", os.Args[1], words)
}
```

By inserting a  `bufio.Reader`  between the input file and  `readbyte`  will

Compare the times of this revised program to  `wc`. How close is it? Take a profile and see what remains.

åœ¨æ–‡ä»¶å†…å®¹ä¸ `readbyte` ä¹‹é—´åŠ ä¸€ä¸ª `bufio.Reader` ç¼“å†²ï¼Œå‡å°‘ç³»ç»Ÿè°ƒç”¨çš„æ¬¡æ•°ã€‚
å°†ä¼˜åŒ–åçš„ç‰ˆæœ¬ä¸ `wc` æ¯”è¾ƒä¸‹ï¼Œçœ‹çœ‹è¿˜æœ‰å¤šå°‘å·®è·ï¼Ÿæ‰§è¡Œä¸€æ¬¡ profile çœ‹çœ‹è¿˜æœ‰å“ªäº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚


> NOTE è¿˜æ˜¯æ¯” wc æ…¢ï¼Œå¼€å¯ CPU profile æ˜¾ç¤º 10ms æ—¶é—´éƒ½åœ¨ main ï¼Œæ‰¾ä¸åˆ°ä¼˜åŒ–ç‚¹ã€‚éš¾é“æ˜¯ golang æœ¬èº«å°±æ˜¯æ…¢ï¼Ÿæ˜¯æ…¢åœ¨å†…å­˜ç®¡ç†ä¸Šå—ï¼Ÿ

```txt
~/tmp$ time wc -w ./2701-0.txt 
215830 ./2701-0.txt

real	0m0.018s
user	0m0.018s
sys	0m0.000s
~/tmp$ time wc -w ./2701-0.txt 
215830 ./2701-0.txt

real	0m0.018s
user	0m0.018s
sys	0m0.000s
~/tmp$ time wc -w ./2701-0.txt 
215830 ./2701-0.txt

real	0m0.018s
user	0m0.018s
sys	0m0.000s
~/tmp$ time wc -w ./2701-0.txt 
215830 ./2701-0.txt

real	0m0.018s
user	0m0.018s
sys	0m0.000s
~/tmp$ time wc -w ./2701-0.txt 
215830 ./2701-0.txt

real	0m0.032s
user	0m0.032s
sys	0m0.000s
~/tmp$ time ./t4 ./2701-0.txt 
"./2701-0.txt": 181276 words

real	0m0.020s
user	0m0.020s
sys	0m0.000s
~/tmp$ time ./t4 ./2701-0.txt 
"./2701-0.txt": 181276 words

real	0m0.022s
user	0m0.023s
sys	0m0.000s
~/tmp$ time ./t4 ./2701-0.txt 
"./2701-0.txt": 181276 words

real	0m0.022s
user	0m0.019s
sys	0m0.004s
~/tmp$ time ./t4 ./2701-0.txt 
"./2701-0.txt": 181276 words

real	0m0.033s
user	0m0.033s
sys	0m0.000s
~/tmp$ time ./t4 ./2701-0.txt 
"./2701-0.txt": 181276 words

real	0m0.022s
user	0m0.023s
sys	0m0.000s
```



#### 3.5.5. Memory profiling

The new  `words`  profile suggests that something is allocating inside the  `readbyte`  function. We can use pprof to investigate.

åœ¨ `readbyte` å‡½æ•°å†…éƒ¨è¿˜æœ‰å†…å­˜åˆ†é…çš„æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ pprof åˆ†æä¸‹ã€‚

```go
defer profile.Start(profile.MemProfile).Stop()
```

Then run the program as usual

```txt
% go run main2.go moby.txt
2018/08/25 14:41:15 profile: memory profiling enabled (rate 4096), /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile312088211/mem.pprof
"moby.txt": 181275 words
2018/08/25 14:41:15 profile: memory profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile312088211/mem.pprof
```

![pprof1](https://blog.zeromake.com/public/img/high-performance-go-workshop/pprof-1.svg)


As we suspected the allocation was coming from  `readbyte`â€‰â€”â€‰this wasnâ€™t that complicated, readbyte is three lines long:

ä»ä¸Šå›¾ä¸­å¯çœ‹å‡ºæ¥ï¼Œå†…å­˜åˆ†é…æ“ä½œç¡®å®æ¥è‡ª `readbyte` ï¼ è¿™ä¸ªå‡½æ•°ä»£ç åªæœ‰3è¡Œï¼Œæ‰€ä»¥åˆ†æèµ·æ¥ä¹Ÿå¾ˆå®¹æ˜“ã€‚

Use pprof to determine where the allocation is coming from.

é€šè¿‡ pprof å¾ˆå®¹æ˜“å‘ç°å†…å­˜åˆ†é…æ“ä½œæ¥è‡ªè¿™é‡Œã€‚

```go
func readbyte(r io.Reader) (rune, error) {
        var buf [1]byte // Allocation is here å°±æ˜¯è¿™é‡Œåœ¨åˆ†é…å†…å­˜
        _, err := r.Read(buf[:])
        return rune(buf[0]), err
}
```


Weâ€™ll talk about why this is happening in more detail in the next section, but for the moment what we see is every call to readbyte is allocating a new one byte long  _array_  and that array is being allocated on the heap.

ç°åœ¨æˆ‘ä»¬èƒ½ç¡®å®šï¼Œæ¯æ¬¡ readbyte è°ƒç”¨éƒ½ä¼šåœ¨ å †(heap) ä¸Šåˆ†é…ä¸€ä¸ªå­—èŠ‚é•¿çš„ æ•°ç»„(array) ã€‚
åé¢æˆ‘ä»¬ä¼šè¯¦ç»†è®¨è®ºä¸ºä»€ä¹ˆå‘ç”Ÿè¿™ç§ç°è±¡ã€‚


What are some ways we can avoid this? Try them and use CPU and memory profiling to prove it.

æœ‰ä»€ä¹ˆåŠæ³•èƒ½é¿å…è¿™æ¬¡å†…å­˜åˆ†é…æ“ä½œå‘¢ï¼Ÿ
è¯•è¯•ä¼˜åŒ–ä¸€ä¸‹ï¼Œç„¶åç”¨ CPU å’Œ Memory profile åˆ†æåˆ†æã€‚



##### Alloc objects vs. inuse objects

Memory profiles come in two varieties, named after their  `go tool pprof`  flags

-   `-alloc_objects`  reports the call site where each allocation was made.
    
-   `-inuse_objects`  reports the call site where an allocation was made  _iff_  it was reachable at the end of the profile.

Memory profile ä¸­å¤§æ¦‚æœ‰ä¸¤ä¸ªç±»å‹ï¼Œåœ¨ `go tool pprof` å°†å…¶å‘½åä¸º

-   `-alloc_objects`  ç»Ÿè®¡äº†æ‰€æœ‰æ‰§è¡Œå†…å­˜åˆ†é…æ“ä½œçš„è°ƒç”¨ç‚¹ã€‚
    
-   `-inuse_objects`  ç»Ÿè®¡äº†æ‰€æœ‰æ‰§è¡Œå†…å­˜åˆ†é…ä¸”"ç›´åˆ°ç”Ÿæˆ profile åï¼Œè¿˜èƒ½ç»§ç»­è¢«è®¿é—®çš„å†…å­˜"çš„è°ƒç”¨ç‚¹ã€‚


> æœ‰ä¸¤ç§å†…å­˜åˆ†æç­–ç•¥ï¼š[^qcraoPPROF]
> ä¸€ç§æ˜¯å½“å‰çš„ï¼ˆè¿™ä¸€æ¬¡é‡‡é›†ï¼‰å†…å­˜æˆ–å¯¹è±¡çš„åˆ†é…ï¼Œç§°ä¸º inuseï¼›
> å¦ä¸€ç§æ˜¯ä»ç¨‹åºè¿è¡Œåˆ°ç°åœ¨æ‰€æœ‰çš„å†…å­˜åˆ†é…ï¼Œä¸ç®¡æ˜¯å¦å·²ç»è¢« gc è¿‡äº†ï¼Œç§°ä¸º alloc
    

To demonstrate this, here is a contrived program which will allocate a bunch of memory in a controlled manner.

ä¸ºäº†æ–¹ä¾¿è¯´æ˜ï¼Œåˆ¶ä½œäº†ä¸‹é¢è¿™ä¸ªä»¥ç‰¹å®šæ–¹å¼æ§åˆ¶å†…å­˜åˆ†é…è¿‡ç¨‹çš„æ¼”ç¤ºç¨‹åºã€‚

```go
const count = 100000

var y []byte

func main() {
	defer profile.Start(profile.MemProfile, profile.MemProfileRate(1)).Stop()
	y = allocate()
	runtime.GC()
}

// allocate allocates count byte slices and returns the first slice allocated.
func allocate() []byte {
	var x [][]byte
	for i := 0; i < count; i++ {
		x = append(x, makeByteSlice())
	}
	return x[0]
}

// makeByteSlice returns a byte slice of a random length in the range [0, 16384).
func makeByteSlice() []byte {
	return make([]byte, rand.Intn(2^14))
}
```

The program is annotation with the  `profile`  package, and we set the memory profile rate to  `1`--that is, record a stack trace for every allocation. This is slows down the program a lot, but youâ€™ll see why in a minute.

æˆ‘ä»¬ä¼šç”¨ `profile` package æ¥è§£é‡Šè¿™ä¸ªç¨‹åºã€‚
å¦å¤–ï¼Œæˆ‘ä»¬å°† memory profile rate è®¾ç½®ä¸º 1 ï¼Œæ‰€ä»¥ä¼šè®°å½•æ¯ä¸€æ¬¡å†…å­˜åˆ†é…æ—¶çš„ stack trace ã€‚
è¿™ä¼šä½¿ç¨‹åºå˜æ…¢ï¼Œä½†ä½ å¾ˆå¿«å°±ä¼šæ˜ç™½ä¸ºä»€ä¹ˆè¦è¿™æ ·åšäº†ã€‚

```txt
% go run main.go
2018/08/25 15:22:05 profile: memory profiling enabled (rate 1), /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile730812803/mem.pprof
2018/08/25 15:22:05 profile: memory profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile730812803/mem.pprof
```

Lets look at the graph of allocated objects, this is the default, and shows the call graphs that lead to the allocation of every object during the profile.

æˆ‘ä»¬å…ˆçœ‹ allocate object ç±»å‹çš„åˆ†æç»“æœã€‚
è¿™é‡Œä¼šæ˜¾ç¤ºæ‰€æœ‰æ‰§è¡Œå†…å­˜åˆ†é…æ“ä½œçš„è°ƒç”¨å›¾è¡¨ï¼ˆè°ƒç”¨æ ˆå…³ç³»ï¼‰ã€‚


```txt
% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile891268605/mem.pprof
```

![](https://blog.zeromake.com/public/img/high-performance-go-workshop/pprof-2.svg)

Not surprisingly more than 99% of the allocations were inside  `makeByteSlice`. Now lets look at the same profile using  `-inuse_objects`

ä½ å¯èƒ½ä¼šæœ‰äº›æƒŠè®¶ï¼Œæœ‰è¶…è¿‡ 99% çš„å†…å­˜åˆ†é…éƒ½å‘ç”Ÿåœ¨ `makeByteSlice` ã€‚
ç°åœ¨å†çœ‹ inuse object ç±»å‹çš„åˆ†æç»“æœå§ã€‚


```txt
% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile891268605/mem.pprof
```

![](https://blog.zeromake.com/public/img/high-performance-go-workshop/pprof-3.svg)

What we see is not the objects that were  _allocated_  during the profile, but the objects that remain  _in use_, at the time the profile was takenâ€‰â€”â€‰this ignores the stack trace for objects which have been reclaimed by the garbage collector.

è¿™å¼ å›¾ä¸­çœ‹ä¸åˆ°å¤šå°‘ _allocated_ çš„å¯¹è±¡ï¼Œåªæœ‰ä¸€äº›è¿˜åœ¨ _in use_ çš„å¯¹è±¡ï¼ˆåœ¨ profile ç»“æŸå‰è¿˜åœ¨ä½¿ç”¨ä¸­çš„å¯¹è±¡ï¼‰ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œé‚£äº›å·²ç»è¢«åƒåœ¾å›æ”¶çš„å¯¹è±¡ï¼Œä¸ä¼šæ˜¾ç¤ºåœ¨è¿™é‡Œã€‚



#### 3.5.6. Block profiling

The last profile type weâ€™ll look at is block profiling. Weâ€™ll use the  `ClientServer`  benchmark from the  `net/http`  package

æœ€åä¸€ä¸ªä»‹ç»çš„æ˜¯ block profile ç±»å‹ã€‚
æˆ‘ä»¬ä½¿ç”¨ net/http package ä¸­çš„  ClientServer  åŸºå‡†æµ‹è¯•æ¥ä»‹ç»ã€‚

```txt
% go test -run=XXX -bench=ClientServer$ -blockprofile=/tmp/block.p net/http
% go tool pprof -http=:8080 /tmp/block.p
```

![](https://blog.zeromake.com/public/img/high-performance-go-workshop/pprof-4.svg)




#### 3.5.7. Thread creation profiling

Go 1.11 (?) added support for profiling the creation of operating system threads.

Add thread creation profiling to  `godoc`  and observe the results of profiling  `godoc -http=:8080 -index`.




#### 3.5.8. Framepointers

Go 1.7 has been released and along with a new compiler for amd64, the compiler now enables frame pointers by default.

Go 1.7 ä¸æ–°ç‰ˆ amd64 ç¼–è¯‘å™¨ä¸€èµ·å‘å¸ƒï¼Œæ–°ç‰ˆæœ¬ç¼–è¯‘å™¨é»˜è®¤æ”¯æŒ å¸§æŒ‡é’ˆã€‚

The frame pointer is a register that always points to the top of the current stack frame.

å¸§æŒ‡é’ˆæ˜¯ä¸€ç›´æŒ‡å‘å½“å‰å †æ ˆå¸§é¡¶éƒ¨çš„å¯„å­˜å™¨ã€‚

Framepointers enable tools like  `gdb(1)`, and  `perf(1)`  to understand the Go call stack.

å¸§æŒ‡é’ˆèƒ½å¸®åŠ© gdb(1) perf(1) è¿™æ ·çš„è°ƒè¯•å·¥å…·ç†è§£ Go çš„è°ƒç”¨æ ˆã€‚

We wonâ€™t cover these tools in this workshop, but you can read and watch a presentation I gave on seven different ways to profile Go programs.

è¿™é‡Œä¸ä¼šä»‹ç»è¿™äº›å·¥å…·ï¼Œä¸‹é¢çš„é“¾æ¥ä¸­æœ‰å¯¹ Go program è¿›è¡Œ profile çš„è¯¦ç»†ä»‹ç»ã€‚

-   [Seven ways to profile a Go program](https://talks.godoc.org/github.com/davecheney/presentations/seven.slide)  (slides)
    
-   [Seven ways to profile a Go program](https://www.youtube.com/watch?v=2h_NFBFrciI)  (video, 30 mins)
    
-   [Seven ways to profile a Go program](https://www.bigmarker.com/remote-meetup-go/Seven-ways-to-profile-a-Go-program)  (webcast, 60 mins)
    

#### 3.5.9. Exercise

-   Generate a profile from a piece of code you know well. If you donâ€™t have a code sample, try profiling  `godoc`.
    
    ```
    % go get golang.org/x/tools/cmd/godoc
    % cd $GOPATH/src/golang.org/x/tools/cmd/godoc
    % vim main.go
    ```
    
-   If you were to generate a profile on one machine and inspect it on another, how would you do it?

- å°è¯•å¯¹ç†Ÿæ‚‰çš„ä»£ç æ‰§è¡Œä¸€æ¬¡ profile ã€‚å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„ä»£ç è¿›è¡Œæµ‹è¯•ï¼Œå¯ä»¥è¯•è¯•å¯¹ godoc è¿›è¡Œ profile ã€‚
- å¦‚æœè¦åœ¨ä¸€å°æœºå™¨ä¸Šç”Ÿæˆ profile ï¼Œè€Œåœ¨å¦ä¸€å°æœºå™¨ä¸Šåˆ†æï¼Œå¦‚æœå®Œæˆï¼Ÿ
    

## 4. Compiler optimisations ç¼–è¯‘å™¨ä¼˜åŒ–

This section covers some of the optimisations that the Go compiler performs.

For example;

-   Escape analysis
    
-   Inlining
    
-   Dead code elimination

are all handled in the front end of the compiler, while the code is still in its AST form; then the code is passed to the SSA compiler for further optimisation.


ä¸‹é¢ä»‹ç» Go ç¼–è¯‘å™¨æ‰§è¡Œçš„ä¸€äº›ä¼˜åŒ–ã€‚

æ¯”å¦‚ï¼š

- é€ƒé€¸åˆ†æ
- å†…è”
- å»æ‰æ— æ•ˆä»£ç 


### 4.1. History of the Go compiler Go ç¼–è¯‘å™¨çš„å†å²

The Go compiler started as a fork of the Plan9 compiler tool chain circa 2007. The compiler at that time bore a strong resemblance to Aho and Ullmanâ€™s  [_Dragon Book_](https://www.goodreads.com/book/show/112269.Principles_of_Compiler_Design).

Go ç¼–è¯‘å™¨æºäº Plan9 ç¼–è¯‘å™¨å·¥å…·é“¾çš„åˆ†æ”¯ã€‚
è¿™æ—¶çš„ç¼–è¯‘å™¨ä¸[Dragon Book é¾™ä¹¦ ç¼–è¯‘å™¨è®¾è®¡åŸç†](https://www.goodreads.com/book/show/112269.Principles_of_Compiler_Design)ååˆ†ç›¸ä¼¼ã€‚

In 2015 the then Go 1.5 compiler was mechanically translated from  [C into Go](https://golang.org/doc/go1.5#c).

2015 å¹´æ—¶ï¼Œ Go 1.5 ç¼–è¯‘å™¨ï¼Œå·²ç»å®Œæˆ [ä» C ç¿»è¯‘åˆ° Go å®ç°è‡ªä¸¾](https://golang.org/doc/go1.5#c) ã€‚

A year later, Go 1.7 introduced a  [new compiler backend](https://blog.golang.org/go1.7)  based on  [SSA](https://en.wikipedia.org/wiki/Static_single_assignment_form)  techniques replaced the previous Plan 9 style code generation. This new backend introduced many opportunities for generic and architecture specific optimistions.

ä¸€å¹´å, Go 1.7 åŸºäº [SSA](https://en.wikipedia.org/wiki/Static_single_assignment_form) æŠ€æœ¯å®ç°äº† [ä¸€ä¸ªæ–°çš„ç¼–è¯‘å™¨åç«¯](https://blog.golang.org/go1.7)  ä»è€Œä»£æ›¿äº†æ­¤å‰ Plan 9 æ—¶ä»£çš„ä»£ç é£æ ¼ã€‚æ–°çš„ç¼–è¯‘å™¨åç«¯èƒ½ä¸ºé€šç”¨ä½“ç³»æ¶æ„åŠç‰¹å®šä½“ç³»æ¶æ„ä¸Šæä¾›å¾ˆå¤šä¼˜åŒ–ç©ºé—´ã€‚



### 4.2. Escape analysis é€ƒé€¸åˆ†æ

The first optimisation weâ€™re doing to discuss is  _escape analysis_.

é¦–å…ˆè¦è®¨è®ºçš„ä¼˜åŒ–æ‰‹æ®µæ˜¯ _é€ƒé€¸åˆ†æ_ ã€‚

To illustrate what escape analysis does recall that the  [Go spec](https://golang.org/ref/spec)  does not mention the heap or the stack. It only mentions that the language is garbage collected in the introduction, and gives no hints as to how this is to be achieved.

åœ¨å…·ä½“ä»‹ç»é€ƒé€¸åˆ†æå‰ï¼Œå¯å›é¡¾ä¸‹[GOè¯­è¨€ç¼–ç¨‹è§„èŒƒ Go spec](https://golang.org/ref/spec) ï¼Œä¼šå‘ç°å…¶ä¸­å®Œå…¨æ²¡æœ‰æåˆ° heap å’Œ stack ã€‚
å®ƒä»…åœ¨å¼•è¨€ä¸­æåˆ°è¿™ä¸€é—¨è‡ªåŠ¨åƒåœ¾å›æ”¶çš„è¯­è¨€ï¼Œè‡³äºå¦‚ä½•å®ç°è‡ªåŠ¨åƒåœ¾å›æ”¶ï¼Œåˆ™å…¨æœªæåŠã€‚

A compliant Go implementation of the Go spec  _could_  store every allocation on the heap. That would put a lot of pressure on the the garbage collector, but it is in no way incorrectâ€‰â€”â€‰for several years, gccgo had very limited support for escape analysis so could effectively be considered to be operating in this mode.

æŠŠæ‰€æœ‰åˆ†é…çš„å˜é‡éƒ½ä¿å­˜åˆ° heap ä¸­ï¼Œä¹Ÿæ˜¯ä¸€ç§ç¬¦åˆ Go è¯­æ–‡ç¼–ç¨‹è§„èŒƒçš„å®ç°ã€‚
è¿™ç§æ–¹æ¡ˆå¯¹åƒåœ¾å›æ”¶å™¨çš„å‹åŠ›å¾ˆå¤§ï¼Œä½†ä¹Ÿä¸èƒ½è¯´è¿™ç§æ–¹æ¡ˆä¸å¯¹ ï¼ï¼ å¾ˆå¤šå¹´ä»¥æ¥ gccgo å¯¹é€ƒé€¸åˆ†æçš„æ”¯æŒéå¸¸æœ‰é™ï¼ŒåŸºæœ¬å¯è®¤ä¸º Go è¯­æ–‡å°±æ˜¯è¿™æ ·å·¥ä½œçš„ã€‚

However, a goroutineâ€™s stack exists as a cheap place to store local variables; there is no need to garbage collect things on the stack. Therefore, where it is safe to do so, an allocation placed on the stack will be more efficient.

ä½†æ˜¯åœ¨ goroutine çš„ stack ç©ºé—´ä¿å­˜æœ¬åœ°å˜é‡å¤ªæ–¹ä¾¿äº†ï¼ˆå¼€é”€å°ï¼‰ï¼›stack ä¸­æ²¡æœ‰åƒåœ¾å›æ”¶ã€‚
å› æ­¤ï¼Œåœ¨ç¡®å®šå®‰å…¨çš„æƒ…å†µä¸‹ï¼Œåœ¨ stack ä¸­åˆ†é…å˜é‡ä¼šæ›´æœ‰æ•ˆç‡ã€‚

In some languages, for example C and C++, the choice of allocating on the stack or on the heap is a manual exercise for the programmerâ€”heap allocations are made with  `malloc`  and  `free`, stack allocation is via  `alloca`. Mistakes using these mechanisms are a common cause of memory corruption bugs.

åœ¨æŸäº›è¯­æ–‡ä¸­ï¼Œæ¯”å¦‚ C å’Œ C++ ï¼Œå˜é‡åœ¨ stack è¿˜æ˜¯åœ¨ heap ä¸­åˆ†é…ï¼Œæ˜¯ç”±ç¨‹åºå‘˜å†³å®šçš„ï¼š

è°ƒç”¨  `malloc` æˆ– `free` å‡½æ•°åˆ†é…çš„ç©ºé—´åœ¨ heap åˆ†é…ï¼›

è°ƒç”¨ `alloca` å‡½æ•°åˆ†é…çš„ç©ºé—´åœ¨ stack åˆ†é…ã€‚

å¤§é‡å†…å­˜æŸåçš„ bug éƒ½æ˜¯ç”±äºè¿™ä¸€æœºåˆ¶å¯¼è‡´çš„ã€‚



In Go, the compiler automatically moves a value to the heap if it lives beyond the lifetime of the function call. It is said that the value  _escapes_  to the heap.

åœ¨ Go è¯­æ–‡ä¸­ï¼Œä¸€ä¸ªå˜é‡çš„ç”Ÿå‘½å‘¨æœŸè¶…å‡ºå‡½æ•°è°ƒç”¨çš„èŒƒå›´åï¼Œç¼–è¯‘å™¨ä¼šè‡ªåŠ¨æŠŠå˜é‡ç§»åŠ¨åˆ° heap ä¸­ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œå˜é‡ _é€ƒé€¸_ åˆ° heap äº†ã€‚

```go
type Foo struct {
	a, b, c, d int
}

func NewFoo() *Foo {
	return &Foo{a: 3, b: 1, c: 4, d: 7}
}
```

In this example the  `Foo`  allocated in  `NewFoo`  will be moved to the heap so its contents remain valid after  `NewFoo`  has returned.

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œ `NewFoo` å‡½æ•°ä¸­åˆ†é…äº† `Foo` å˜é‡ï¼Œéšåå˜é‡åˆè¢«ç§»åŠ¨åˆ° heap ä¸­ï¼Œä»¥ä¾¿åœ¨å‡½æ•°è¿”å›åï¼Œç»§ç»­ä½¿ç”¨è¿™ä¸ªå˜é‡çš„å€¼ã€‚

This has been present since the earliest days of Go. It isnâ€™t so much an optimisation as an automatic correctness feature. Accidentally returning the address of a stack allocated variable is not possible in Go.

è¿™ç§ç‰¹æ€§åœ¨ Go è¯­è¨€å‘å±•çš„æ—©æœŸå°±å®ç°äº†ã€‚
ä¸å…¶è¯´è¿™æ˜¯ä¸€ç§ä¼˜åŒ–ï¼Œä¸å¦‚è¯´è¿™æ˜¯ä¸€ç§è‡ªåŠ¨çº é”™åŠŸèƒ½ã€‚
å› ä¸ºï¼Œåœ¨ Go è¯­è¨€ä¸­ï¼Œå†ä¹Ÿä¸ä¼šæ„å¤–è¿”å›ä¸€ä¸ª stack çš„å˜é‡åœ°å€ï¼ˆè€Œé€ æˆ bug äº†ï¼‰ã€‚


But the compiler can also do the opposite; it can find things which would be assumed to be allocated on the heap, and move them to stack.

å®é™…æ˜¯ï¼Œç¼–è¯‘å™¨è¿˜ä¼šåå…¶é“è€Œè¡Œä¹‹ï¼›
å®ƒä¹Ÿä¼šæ‰¾åˆ°é‚£äº›åŸæœ¬åˆ†é…åœ¨ heap çš„å˜é‡ï¼Œç§»åŠ¨åˆ° stack ä¸Šåˆ†é…ã€‚

Letâ€™s have a look at an example

æˆ‘ä»¬çœ‹ä¸ªä¾‹å­ï¼š

```go
func Sum() int {
	const count = 100
	numbers := make([]int, count)
	for i := range numbers {
		numbers[i] = i + 1
	}

	var sum int
	for _, i := range numbers {
		sum += i
	}
	return sum
}

func main() {
	answer := Sum()
	fmt.Println(answer)
}
```

`Sum`  adds the `int`s between 1 and 100 and returns the result.

`Sum` è¿”å› 1 åˆ° 100 çš„ int å€¼ç´¯åŠ åˆ°ä¸€èµ·çš„ å’Œã€‚

Because the  `numbers`  slice is only referenced inside  `Sum`, the compiler will arrange to store the 100 integers for that slice on the stack, rather than the heap. There is no need to garbage collect  `numbers`, it is automatically freed when  `Sum`  returns.

å› ä¸º `numbers` slice åªåœ¨ `Sum` å‡½æ•°ä¸­ä½¿ç”¨ï¼Œæ‰€ä»¥ç¼–è¯‘å™¨ä¼šæŠŠä¿å­˜è¿™ 100 ä¸ªæ•´æ•°çš„ slick å˜é‡åˆ†é…åˆ° stack ä¸Šï¼Œè€Œä¸æ˜¯ heap ä¸Šã€‚
æ‰€ä»¥ï¼Œè¿™é‡Œä¸éœ€è¦åƒåœ¾å›æ”¶ `numbers` ï¼Œå› ä¸ºåœ¨ `Sum` å‡½æ•°è¿”å›åï¼Œstack çš„å˜é‡ä¼šè‡ªåŠ¨é”€æ¯ã€‚



#### 4.2.1. Prove it!

To print the compilers escape analysis decisions, use the  `-m`  flag.

```
% go build -gcflags=-m examples/esc/sum.go
# command-line-arguments
examples/esc/sum.go:22:13: inlining call to fmt.Println
examples/esc/sum.go:8:17: Sum make([]int, count) does not escape
examples/esc/sum.go:22:13: answer escapes to heap
examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap
examples/esc/sum.go:22:13: main []interface {} literal does not escape
<autogenerated>:1: os.(*File).close .this does not escape
```

Line 8 shows the compiler has correctly deduced that the result of  `make([]int, 100)`  does not escape to the heap. The reason it did no

The reason line 22 reports that  `answer`  escapes to the heap is  `fmt.Println`  is a  _variadic_  function. The parameters to a variadic function are  _boxed_  into a slice, in this case a  `[]interface{}`, so  `answer`  is placed into a interface value because it is referenced by the call to  `fmt.Println`. Since Go 1.6 the garbage collector requires  _all_  values passed via an interface to be pointers, what the compiler sees is  _approximately_:

```
var answer = Sum()
fmt.Println([]interface{&answer}...)
```

We can confirm this using the  `-gcflags="-m -m"`  flag. Which returns

```
% go build -gcflags='-m -m' examples/esc/sum.go 2>&1 | grep sum.go:22
examples/esc/sum.go:22:13: inlining call to fmt.Println func(...interface {}) (int, error) { return fmt.Fprintln(io.Writer(os.Stdout), fmt.a...) }
examples/esc/sum.go:22:13: answer escapes to heap
examples/esc/sum.go:22:13:      from ~arg0 (assign-pair) at examples/esc/sum.go:22:13
examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap
examples/esc/sum.go:22:13:      from io.Writer(os.Stdout) (passed to call[argument escapes]) at examples/esc/sum.go:22:13
examples/esc/sum.go:22:13: main []interface {} literal does not escape
```

In short, donâ€™t worry about line 22, its not important to this discussion.

#### 4.2.2. Exercises

-   Does this optimisation hold true for all values of  `count`?
    
-   Does this optimisation hold true if  `count`  is a variable, not a constant?
    
-   Does this optimisation hold true if  `count`  is a parameter to  `Sum`?
    

#### 4.2.3. Escape analysis (continued)

This example is a little contrived. It is not intended to be real code, just an example.

```
type Point struct{ X, Y int }

const Width = 640
const Height = 480

func Center(p *Point) {
	p.X = Width / 2
	p.Y = Height / 2
}

func NewPoint() {
	p := new(Point)
	Center(p)
	fmt.Println(p.X, p.Y)
}
```

`NewPoint`  creates a new  `*Point`  value,  `p`. We pass  `p`  to the  `Center`  function which moves the point to a position in the center of the screen. Finally we print the values of  `p.X`  and  `p.Y`.

```
% go build -gcflags=-m examples/esc/center.go
# command-line-arguments
examples/esc/center.go:11:6: can inline Center
examples/esc/center.go:18:8: inlining call to Center
examples/esc/center.go:19:13: inlining call to fmt.Println
examples/esc/center.go:11:13: Center p does not escape
examples/esc/center.go:19:15: p.X escapes to heap
examples/esc/center.go:19:20: p.Y escapes to heap
examples/esc/center.go:19:13: io.Writer(os.Stdout) escapes to heap
examples/esc/center.go:17:10: NewPoint new(Point) does not escape
examples/esc/center.go:19:13: NewPoint []interface {} literal does not escape
<autogenerated>:1: os.(*File).close .this does not escape
```

Even though  `p`  was allocated with the  `new`  function, it will not be stored on the heap, because no reference  `p`  escapes the  `Center`  function.

_Question_: What about line 19, if  `p`  doesnâ€™t escape, what is escaping to the heap?

Write a benchmark to provide that  `Sum`  does not allocate.

### 4.3. Inlining

In Go function calls in have a fixed overhead; stack and preemption checks.

Some of this is ameliorated by hardware branch predictors, but itâ€™s still a cost in terms of function size and clock cycles.

Inlining is the classical optimisation that avoids these costs.

Until Go 1.11 inlining only worked on  _leaf functions_, a function that does not call another. The justification for this is:

-   If your function does a lot of work, then the preamble overhead will be negligible. Thatâ€™s why functions over a certain size (currently some count of instructions, plus a few operations which prevent inlining all together (eg. switch before Go 1.7)
    
-   Small functions on the other hand pay a fixed overhead for a relatively small amount of useful work performed. These are the functions that inlining targets as they benefit the most.
    

The other reason is that heavy inlining makes stack traces harder to follow.

#### 4.3.1. Inlining (example)

```
func Max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func F() {
	const a, b = 100, 20
	if Max(a, b) == b {
		panic(b)
	}
}
```

Again we use the  `-gcflags=-m`  flag to view the compilers optimisation decision.

```
% go build -gcflags=-m examples/inl/max.go
# command-line-arguments
examples/inl/max.go:4:6: can inline Max
examples/inl/max.go:11:6: can inline F
examples/inl/max.go:13:8: inlining call to Max
examples/inl/max.go:20:6: can inline main
examples/inl/max.go:21:3: inlining call to F
examples/inl/max.go:21:3: inlining call to Max
```

The compiler printed two lines.

-   The first at line 3, the declaration of  `Max`, telling us that it can be inlined.
    
-   The second is reporting that the body of  `Max`  has been inlined into the caller at line 12.
    

_Without_  using the  `//go:noinline`  comment, rewrite  `Max`  such that it still returns the right answer, but is no longer considered inlineable by the compiler.

#### 4.3.2. What does inlining look like?

Compile  `max.go`  and see what the optimised version of  `F()`  became.

```
% go build -gcflags=-S examples/inl/max.go 2>&1 | grep -A5 '"".F STEXT'
"".F STEXT nosplit size=2 args=0x0 locals=0x0
        0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11)     TEXT    "".F(SB), NOSPLIT|ABIInternal, $0-0
        0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11)     FUNCDATA        $0, gclocalsÂ·33cdeccccebe80329f1fdbee7f5874cb(SB)
        0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11)     FUNCDATA        $1, gclocalsÂ·33cdeccccebe80329f1fdbee7f5874cb(SB)
        0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11)     FUNCDATA        $3, gclocalsÂ·33cdeccccebe80329f1fdbee7f5874cb(SB)
        0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:13)     PCDATA  $2, $0
```

This is the body of  `F`  once  `Max`  has been inlined into itâ€‰â€”â€‰thereâ€™s nothing happening in this function. I know thereâ€™s a lot of text on the screen for nothing, but take my word for it, the only thing happening is the  `RET`. In effect  `F`  became:

```
func F() {
        return
}
```

What are FUNCDATA and PCDATA?

The output from  `-S`  is not the final machine code that goes into your binary. The linker does some processing during the final link stage. Lines like  `FUNCDATA`  and  `PCDATA`  are metadata for the garbage collector which are moved elsewhere when linking. If youâ€™re reading the output of  `-S`, just ignore  `FUNCDATA`  and  `PCDATA`  lines; theyâ€™re not part of the final binary.

#### 4.3.3. Discussion

Why did I declare  `a`  and  `b`  in  `F()`  to be constants?

Experiment with the output of What happens if  `a`  and  `b`  are declared as are variables? What happens if  `a`  and  `b`  are passing into  `F()`  as parameters?

`-gcflags=-S`  doesnâ€™t prevent the final binary being build in your working directory. If you find that subsequent runs of  `go build â€¦â€‹`  produce no output, delete the  `./max`  binary in your working directory.

#### 4.3.4. Adjusting the level of inlining

Adjusting the  _inlining level_  is performed with the  `-gcflags=-l`  flag. Somewhat confusingly passing a single  `-l`  will disable inlining, and two or more will enable inlining at more aggressive settings.

-   `-gcflags=-l`, inlining disabled.
    
-   nothing, regular inlining.
    
-   `-gcflags='-l -l'`  inlining level 2, more aggressive, might be faster, may make bigger binaries.
    
-   `-gcflags='-l -l -l'`  inlining level 3, more aggressive again, binaries definitely bigger, maybe faster again, but might also be buggy.
    
-   `-gcflags=-l=4`  (four `-l`s) in Go 1.11 will enable the experimental  [_mid stack_  inlining optimisation](https://github.com/golang/go/issues/19348#issuecomment-393654429).
    

#### 4.3.5. Mid Stack inlining

Since Go 1.12 so called  _mid stack_  inlining has been enabled (it was previously available in preview in Go 1.11 with the  `-gcflags='-l -l -l -l'`  flag).

We can see an example of mid stack inlining in the previous example. In Go 1.11 and earlier  `F`  would not have been a leaf functionâ€‰â€”â€‰it called  `max`. However because of inlining improvements  `F`  is now inlined into its caller. This is for two reasons; . When  `max`  is inlined into  `F`,  `F`  contains no other function calls thus it becomes a potential  _leaf function_, assuming its complexity budget has not been exceeded. . Because  `F`  is a simple functionâ€”â€‹inlining and dead code elimination has eliminated much of its complexity budgetâ€”â€‹it is eligable for  _mid stack_  inlining irrispective of calling  `max`.

Mid stack inlining can be used to inline the fast path of a function, eliminating the function call overhead in the fast path.  [This recent CL which landed in for Go 1.13](https://go-review.googlesource.com/c/go/+/152698)  shows this technique applied to  `sync.RWMutex.Unlock()`.

##### [](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#further_reading_3)[Further reading](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#further_reading_3)

-   [Mid-stack inlining in the Go compiler presentation by David Lazar](https://docs.google.com/presentation/d/1Wcblp3jpfeKwA0Y4FOmj63PW52M_qmNqlQkNaLj0P5o/edit#slide=id.p)
    
-   [Proposal: Mid-stack inlining in the Go compiler](https://github.com/golang/proposal/blob/master/design/19348-midstack-inlining.md)
    

### 4.4. Dead code elimination

Why is it important that  `a`  and  `b`  are constants?

To understand what happened lets look at what the compiler sees once its inlined  `Max`  into  `F`. We canâ€™t get this from the compiler easily, but itâ€™s straight forward to do it by hand.

Before:

```
func Max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func F() {
	const a, b = 100, 20
	if Max(a, b) == b {
		panic(b)
	}
}
```

After:

```
func F() {
	const a, b = 100, 20
	var result int
	if a > b {
		result = a
	} else {
		result = b
	}
	if result == b {
		panic(b)
	}
}
```

Because  `a`  and  `b`  are constants the compiler can prove at compile time that the branch will never be false;  `100`  is always greater than  `20`. So the compiler can further optimise  `F`  to

```
func F() {
	const a, b = 100, 20
	var result int
	if true {
		result = a
	} else {
		result = b
	}
	if result == b {
		panic(b)
	}
}
```

Now that the result of the branch is know then then the contents of  `result`  are also known. This is call  _branch elimination_.

```
func F() {
        const a, b = 100, 20
        const result = a
        if result == b {
                panic(b)
        }
}
```

Now the branch is eliminated we know that  `result`  is always equal to  `a`, and because  `a`  was a constant, we know that  `result`  is a constant. The compiler applies this proof to the second branch

```
func F() {
        const a, b = 100, 20
        const result = a
        if false {
                panic(b)
        }
}
```

And using branch elimination again the final form of  `F`  is reduced to.

```
func F() {
        const a, b = 100, 20
        const result = a
}
```

And finally just

```
func F() {
}
```

#### 4.4.1. Dead code elimination (cont.)

Branch elimination is one of a category of optimisations known as  _dead code elimination_. In effect, using static proofs to show that a piece of code is never reachable, colloquially known as  _dead_, therefore it need not be compiled, optimised, or emitted in the final binary.

We saw how dead code elimination works together with inlining to reduce the amount of code generated by removing loops and branches that are proven unreachable.

You can take advantage of this to implement expensive debugging, and hide it behind

```
const debug = false
```

Combined with build tags this can be very useful.

#### 4.4.2. Further reading

-   [Using // +build to switch between debug and release builds](http://dave.cheney.net/2014/09/28/using-build-to-switch-between-debug-and-release)
    
-   [How to use conditional compilation with the go build tool](http://dave.cheney.net/2013/10/12/how-to-use-conditional-compilation-with-the-go-build-tool)
    

### 4.5. Compiler flags Exercises

Compiler flags are provided with:

```
go build -gcflags=$FLAGS
```

Investigate the operation of the following compiler functions:

-   `-S`  prints the (Go flavoured) assembly of the  _package_  being compiled.
    
-   `-l`  controls the behaviour of the inliner;  `-l`  disables inlining,  `-l -l`  increases it (more  `-l`  's increases the compilerâ€™s appetite for inlining code). Experiment with the difference in compile time, program size, and run time.
    
-   `-m`  controls printing of optimisation decision like inlining, escape analysis.  `-m`-m` prints more details about what the compiler was thinking.
    
-   `-l -N`  disables all optimisations.
    

If you find that subsequent runs of  `go build â€¦â€‹`  produce no output, delete the  `./max`  binary in your working directory.

#### 4.5.1. Further reading

-   [Codegen Inspection by Jaana Burcu Dogan](http://go-talks.appspot.com/github.com/rakyll/talks/gcinspect/talk.slide#1)
    

### 4.6. Bounds check elimination

Go is a bounds checked language. This means array and slice subscript operations are checked to ensure they are within the bounds of the respective types.

For arrays, this can be done at compile time. For slices, this must be done at run time.

```
var v = make([]int, 9)

var A, B, C, D, E, F, G, H, I int

func BenchmarkBoundsCheckInOrder(b *testing.B) {
	for n := 0; n < b.N; n++ {
		A = v[0]
		B = v[1]
		C = v[2]
		D = v[3]
		E = v[4]
		F = v[5]
		G = v[6]
		H = v[7]
		I = v[8]
	}
}
```

Use  `-gcflags=-S`  to disassemble  `BenchmarkBoundsCheckInOrder`. How many bounds check operations are performed per loop?

```
func BenchmarkBoundsCheckOutOfOrder(b *testing.B) {
	for n := 0; n < b.N; n++ {
		I = v[8]
		A = v[0]
		B = v[1]
		C = v[2]
		D = v[3]
		E = v[4]
		F = v[5]
		G = v[6]
		H = v[7]
	}
}
```

Does rearranging the order in which we assign the  `A`  through  `I`  affect the assembly. Disassemble  `BenchmarkBoundsCheckOutOfOrder`  and find out.

#### 4.6.1. Exercises

-   Does rearranging the order of subscript operations affect the size of the function? Does it affect the speed of the function?
    
-   What happens if  `v`  is moved inside the  `Benchmark`  function?
    
-   What happens if  `v`  was declared as an array,  `var v [9]int`?
    

## 5. Execution Tracer

The execution tracer was developed by  [Dmitry Vyukov](https://github.com/dvyukov)  for Go 1.5 and remained under documented, and under utilised, for several years.

Unlike sample based profiling, the execution tracer is integrated into the Go runtime, so it does just know what a Go program is doing at a particular point in time, but  _why_.

### 5.1. What is the execution tracer, why do we need it?

I think its easiest to explain what the execution tracer does, and why itâ€™s important by looking at a piece of code where the pprof,  `go tool pprof`  performs poorly.

The  `examples/mandelbrot`  directory contains a simple mandelbrot generator. This code is derived from  [Francesc Campoyâ€™s mandelbrot package](https://github.com/campoy/mandelbrot).

```
cd examples/mandelbrot
go build && ./mandelbrot
```

If we build it, then run it, it generates something like this

![mandelbrot](https://dave.cheney.net/high-performance-go-workshop/images/mandelbrot.png)

#### 5.1.1. How long does it take?

So, how long does this program take to generate a 1024 x 1024 pixel image?

The simplest way I know how to do this is to use something like  `time(1)`.

```
% time ./mandelbrot
real    0m1.654s
user    0m1.630s
sys     0m0.015s
```

Donâ€™t use  `time go run mandebrot.go`  or youâ€™ll time how long it takes to  _compile_  the program as well as run it.

#### 5.1.2. What is the program doing?

So, in this example the program took 1.6 seconds to generate the mandelbrot and write to to a png.

Is that good? Could we make it faster?

One way to answer that question would be to use Goâ€™s built in pprof support to profile the program.

Letâ€™s try that.

### 5.2. Generating the profile

To turn generate a profile we need to either

1.  Use the  `runtime/pprof`  package directly.
    
2.  Use a wrapper like  `github.com/pkg/profile`  to automate this.
    

### 5.3. Generating a profile with runtime/pprof

To show you that thereâ€™s no magic, letâ€™s modify the program to write a CPU profile to  `os.Stdout`.

```

import "runtime/pprof"

func main() {
	pprof.StartCPUProfile(os.Stdout)
	defer pprof.StopCPUProfile()
```

By adding this code to the top of the  `main`  function, this program will write a profile to  `os.Stdout`.

```
cd examples/mandelbrot-runtime-pprof
go run mandelbrot.go > cpu.pprof
```

We can use  `go run`  in this case because the cpu profile will only include the execution of  `mandelbrot.go`, not its compilation.

#### 5.3.1. Generating a profile with github.com/pkg/profile

The previous slide showed a super cheap way to generate a profile, but it has a few problems.

-   If you forget to redirect the output to a file then youâ€™ll blow up that terminal session. ğŸ˜ (hint:  `reset(1)`  is your friend)
    
-   If you write anything else to  `os.Stdout`, for example,  `fmt.Println`  youâ€™ll corrupt the trace.
    

The recommended way to use  `runtime/pprof`  is to  [write the trace to a file](https://godoc.org/runtime/pprof#hdr-Profiling_a_Go_program). But, then you have to make sure the trace is stopped, and file is closed before your program stops, including if someone `^Câ€™s it.

So, a few years ago I wrote a  [package](https://godoc.org/github.gom/pkg/profile)  to take care of it.

```

import "github.com/pkg/profile"

func main() {
	defer profile.Start(profile.CPUProfile, profile.ProfilePath(".")).Stop()
```

If we run this version, we get a profile written to the current working directory

```
% go run mandelbrot.go
2017/09/17 12:22:06 profile: cpu profiling enabled, cpu.pprof
2017/09/17 12:22:08 profile: cpu profiling disabled, cpu.pprof
```

Using  `pkg/profile`  is not mandatory, but it takes care of a lot of the boilerplate around collecting and recording traces, so weâ€™ll use it for the rest of this workshop.

#### 5.3.2. Analysing the profile

Now we have a profile, we can use  `go tool pprof`  to analyse it.

```
% go tool pprof -http=:8080 cpu.pprof
```

In this run we see that the program ran for 1.81s seconds (profiling adds a small overhead). We can also see that pprof only captured data for 1.53 seconds, as pprof is sample based, relying on the operating systemâ€™s  `SIGPROF`  timer.

Since Go 1.9 the  `pprof`  trace contains all the information you need to analyse the trace. You no longer need to also have the matching binary which produced the trace. ğŸ‰

We can use the  `top`  pprof function to sort functions recorded by the trace

```
% go tool pprof cpu.pprof
Type: cpu
Time: Mar 24, 2019 at 5:18pm (CET)
Duration: 2.16s, Total samples = 1.91s (88.51%)
Entering interactive mode (type "help" for commands, "o" for options)
(pprof) top
Showing nodes accounting for 1.90s, 99.48% of 1.91s total
Showing top 10 nodes out of 35
      flat  flat%   sum%        cum   cum%
     0.82s 42.93% 42.93%      1.63s 85.34%  main.fillPixel
     0.81s 42.41% 85.34%      0.81s 42.41%  main.paint
     0.11s  5.76% 91.10%      0.12s  6.28%  runtime.mallocgc
     0.04s  2.09% 93.19%      0.04s  2.09%  runtime.memmove
     0.04s  2.09% 95.29%      0.04s  2.09%  runtime.nanotime
     0.03s  1.57% 96.86%      0.03s  1.57%  runtime.pthread_cond_signal
     0.02s  1.05% 97.91%      0.04s  2.09%  compress/flate.(*compressor).deflate
     0.01s  0.52% 98.43%      0.01s  0.52%  compress/flate.(*compressor).findMatch
     0.01s  0.52% 98.95%      0.01s  0.52%  compress/flate.hash4
     0.01s  0.52% 99.48%      0.01s  0.52%  image/png.filter
```

We see that the  `main.fillPixel`  function was on the CPU the most when pprof captured the stack.

Finding  `main.paint`  on the stack isnâ€™t a surprise, this is what the program does; it paints pixels. But what is causing  `paint`  to spend so much time? We can check that with the  _cummulative_  flag to  `top`.

```
(pprof) top --cum
Showing nodes accounting for 1630ms, 85.34% of 1910ms total
Showing top 10 nodes out of 35
      flat  flat%   sum%        cum   cum%
         0     0%     0%     1840ms 96.34%  main.main
         0     0%     0%     1840ms 96.34%  runtime.main
     820ms 42.93% 42.93%     1630ms 85.34%  main.fillPixel
         0     0% 42.93%     1630ms 85.34%  main.seqFillImg
     810ms 42.41% 85.34%      810ms 42.41%  main.paint
         0     0% 85.34%      210ms 10.99%  image/png.(*Encoder).Encode
         0     0% 85.34%      210ms 10.99%  image/png.Encode
         0     0% 85.34%      160ms  8.38%  main.(*img).At
         0     0% 85.34%      160ms  8.38%  runtime.convT2Inoptr
         0     0% 85.34%      150ms  7.85%  image/png.(*encoder).writeIDATs
```

This is sort of suggesting that  `main.fillPixed`  is actually doing most of the work.

You can also visualise the profile with the  `web`  command, which looks like this:

```txt
Type: cpuTime: Sep 17, 2017 at 12:22pm (AEST)Duration: 1.81s, Total samples = 1.53s (84.33%)Showing nodes accounting for 1.53s, 100% of 1.53s totalmainpaintmandelbrot.go1s (65.36%)runtimemainproc.go0 of 1.53s (100%)mainmainmandelbrot.go0 of 1.53s (100%)1.53smainfillPixelmandelbrot.go0.27s (17.65%)of 1.27s (83.01%)1s(inline)image/pngEncodewriter.go0 of 0.26s (16.99%)0.26smainseqFillImgmandelbrot.go0 of 1.27s (83.01%)1.27sruntimemallocgcmalloc.go0.13s (8.50%)of 0.16s (10.46%)runtime(*mcache)nextFreemalloc.go0 of 0.03s (1.96%)0.03simage/png(*encoder)writeImagewriter.go0 of 0.19s (12.42%)main(*img)Atmandelbrot.go0 of 0.18s (11.76%)0.11simage/pngfilterwriter.go0.01s (0.65%)0.01scompress/zlib(*Writer)Writewriter.go0 of 0.07s (4.58%)0.07simage/png(*Encoder)Encodewriter.go0 of 0.26s (16.99%)image/png(*encoder)writeIDATswriter.go0 of 0.19s (12.42%)0.19simage/pngopaquewriter.go0 of 0.07s (4.58%)0.07sruntimeconvT2Inoptriface.go0 of 0.18s (11.76%)0.18ssyscallSyscallasm_darwin_amd64.s0.05s (3.27%)0.16sruntimememmovememmove_amd64.s0.02s (1.31%)0.02scompress/flate(*compressor)deflatedeflate.go0.01s (0.65%)of 0.07s (4.58%)compress/flate(*compressor)findMatchdeflate.go0 of 0.01s (0.65%)0.01scompress/flate(*compressor)writeBlockdeflate.go0 of 0.05s (3.27%)0.05sruntimemmapsys_darwin_amd64.s0.02s (1.31%)compress/flate(*huffmanBitWriter)writehuffman_bit_writer.go0 of 0.05s (3.27%)compress/flate(*dictWriter)Writedeflate.go0 of 0.05s (3.27%)0.05scompress/flate(*huffmanBitWriter)writeTokenshuffman_bit_writer.go0 of 0.05s (3.27%)compress/flate(*huffmanBitWriter)writeBitshuffman_bit_writer.go0 of 0.01s (0.65%)0.01scompress/flate(*huffmanBitWriter)writeCodehuffman_bit_writer.go0 of 0.04s (2.61%)0.04sruntimesystemstackasm_amd64.s0 of 0.03s (1.96%)runtime(*mcache)nextFreefunc1malloc.go0 of 0.02s (1.31%)0.02sruntime(*mheap)allocfunc1mheap.go0 of 0.01s (0.65%)0.01scompress/flatematchLendeflate.go0.01s (0.65%)runtime(*mcentral)growmcentral.go0 of 0.02s (1.31%)runtime(*mheap)allocmheap.go0 of 0.01s (0.65%)0.01sruntimeheapBitsinitSpanmbitmap.go0 of 0.01s (0.65%)0.01sruntimememclrNoHeapPointersmemclr_amd64.s0.01s (0.65%)bufio(*Writer)Flushbufio.go0 of 0.05s (3.27%)image/png(*encoder)Writewriter.go0 of 0.05s (3.27%)0.05sbufio(*Writer)Writebufio.go0 of 0.05s (3.27%)0.05scompress/flate(*Writer)Writedeflate.go0 of 0.07s (4.58%)compress/flate(*compressor)writedeflate.go0 of 0.07s (4.58%)0.07s0.01s0.07scompress/flate(*huffmanBitWriter)writeBlockhuffman_bit_writer.go0 of 0.05s (3.27%)0.05s0.05s0.01s0.05s0.04s0.07simage/png(*encoder)writeChunkwriter.go0 of 0.05s (3.27%)0.05sos(*File)Writefile.go0 of 0.05s (3.27%)0.05s0.19s0.26s0.07sinternal/poll(*FD)Writefd_unix.go0 of 0.05s (3.27%)syscallWritesyscall_unix.go0 of 0.05s (3.27%)0.05s1.27sos(*File)writefile_unix.go0 of 0.05s (3.27%)0.05s0.05s0.03sruntime(*mcache)refillmcache.go0 of 0.02s (1.31%)0.02sruntime(*mcentral)cacheSpanmcentral.go0 of 0.02s (1.31%)0.02s0.02s0.01sruntime(*mheap)alloc_mmheap.go0 of 0.01s (0.65%)0.01sruntime(*mheap)allocSpanLockedmheap.go0 of 0.01s (0.65%)runtime(*mheap)growmheap.go0 of 0.01s (0.65%)0.01s0.01sruntime(*mheap)sysAllocmalloc.go0 of 0.01s (0.65%)0.01sruntimesysMapmem_darwin.go0 of 0.01s (0.65%)0.01sruntimenewMarkBitsmheap.go0 of 0.01s (0.65%)0.01sruntimenewArenaMayUnlockmheap.go0 of 0.01s (0.65%)runtimesysAllocmem_darwin.go0 of 0.01s (0.65%)0.01s0.01s0.01s0.01ssyscallwritezsyscall_darwin_amd64.go0 of 0.05s (3.27%)0.05s0.05s
```

### 5.4. Tracing vs Profiling

Hopefully this example shows the limitations of profiling. Profiling told us what the profiler saw;  `fillPixel`  was doing all the work. There didnâ€™t look like there was much that could be done about that.

So now itâ€™s a good time to introduce the execution tracer which gives a different view of the same program.

#### 5.4.1. Using the execution tracer

Using the tracer is as simple as asking for a  `profile.TraceProfile`, nothing else changes.

```

import "github.com/pkg/profile"

func main() {
	defer profile.Start(profile.TraceProfile, profile.ProfilePath(".")).Stop()
```

When we run the program, we get a  `trace.out`  file in the current working directory.

```
% go build mandelbrot.go
% % time ./mandelbrot
2017/09/17 13:19:10 profile: trace enabled, trace.out
2017/09/17 13:19:12 profile: trace disabled, trace.out

real    0m1.740s
user    0m1.707s
sys     0m0.020s
```

Just like pprof, there is a tool in the  `go`  command to analyse the trace.

```
% go tool trace trace.out
2017/09/17 12:41:39 Parsing trace...
2017/09/17 12:41:40 Serializing trace...
2017/09/17 12:41:40 Splitting trace...
2017/09/17 12:41:40 Opening browser. Trace viewer s listening on http://127.0.0.1:57842
```

This tool is a little bit different to  `go tool pprof`. The execution tracer is reusing a lot of the profile visualisation infrastructure built into Chrome, so  `go tool trace`  acts as a server to translate the raw execution trace into data which Chome can display natively.

#### 5.4.2. Analysing the trace

We can see from the trace that the program is only using one cpu.

```
func seqFillImg(m *img) {
	for i, row := range m.m {
		for j := range row {
			fillPixel(m, i, j)
		}
	}
}
```

This isnâ€™t a surprise, by default  `mandelbrot.go`  calls  `fillPixel`  for each pixel in each row in sequence.

Once the image is painted, see the execution switches to writing the  `.png`  file. This generates garbage on the heap, and so the trace changes at that point, we can see the classic saw tooth pattern of a garbage collected heap.

The trace profile offers timing resolution down to the  _microsecond_  level. This is something you just canâ€™t get with external profiling.

go tool trace

Before we go on there are some things we should talk about the usage of the trace tool.

-   The tool uses the javascript debugging support built into Chrome. Trace profiles can only be viewed in Chrome, they wonâ€™t work in Firefox, Safari, IE/Edge. Sorry.
    
-   Because this is a Google product, it supports keyboard shortcuts; use  `WASD`  to navigate, use  `?`  to get a list.
    
-   Viewing traces can take a  **lot**  of memory. Seriously, 4Gb wonâ€™t cut it, 8Gb is probably the minimum, more is definitely better.
    
-   If youâ€™ve installed Go from an OS distribution like Fedora, the support files for the trace viewer may not be part of the main  `golang`  deb/rpm, they might be in some  `-extra`  package.
    

### 5.5. Using more than one CPU

We saw from the previous trace that the program is running sequentially and not taking advantage of the other CPUs on this machine.

Mandelbrot generation is known as  _embarassingly_parallel_. Each pixel is independant of any other, they could all be computed in parallel. So, letâ€™s try that.

```
% go build mandelbrot.go
% time ./mandelbrot -mode px
2017/09/17 13:19:48 profile: trace enabled, trace.out
2017/09/17 13:19:50 profile: trace disabled, trace.out

real    0m1.764s
user    0m4.031s
sys     0m0.865s
```

So the runtime was basically the same. There was more user time, which makes sense, we were using all the CPUs, but the real (wall clock) time was about the same.

Letâ€™s look a the trace.

As you can see this trace generated  _much_  more data.

-   It looks like lots of work is being done, but if you zoom right in, there are gaps. This is believed to be the scheduler.
    
-   While weâ€™re using all four cores, because each  `fillPixel`  is a relatively small amount of work, weâ€™re spending a lot of time in scheduling overhead.
    

### 5.6. Batching up work

Using one goroutine per pixel was too fine grained. There wasnâ€™t enough work to justify the cost of the goroutine.

Instead, letâ€™s try processing one row per goroutine.

```
% go build mandelbrot.go
% time ./mandelbrot -mode row
2017/09/17 13:41:55 profile: trace enabled, trace.out
2017/09/17 13:41:55 profile: trace disabled, trace.out

real    0m0.764s
user    0m1.907s
sys     0m0.025s
```

This looks like a good improvement, we almost halved the runtime of the program. Letâ€™s look at the trace.

As you can see the trace is now smaller and easier to work with. We get to see the whole trace in span, which is a nice bonus.

-   At the start of the program we see the number of goroutines ramp up to around 1,000. This is an improvement over the 1 << 20 that we saw in the previous trace.
    
-   Zooming in we see  `onePerRowFillImg`  runs for longer, and as the goroutine  _producing_  work is done early, the scheduler efficiently works through the remaining runnable goroutines.
    

### 5.7. Using workers

`mandelbrot.go`  supports one other mode, letâ€™s try it.

```
% go build mandelbrot.go
% time ./mandelbrot -mode workers
2017/09/17 13:49:46 profile: trace enabled, trace.out
2017/09/17 13:49:50 profile: trace disabled, trace.out

real    0m4.207s
user    0m4.459s
sys     0m1.284s
```

So, the runtime was much worse than any previous. Letâ€™s look at the trace and see if we can figure out what happened.

Looking at the trace you can see that with only one worker process the producer and consumer tend to alternate because there is only one worker and one consumer. Letâ€™s increase the number of workers

```
% go build mandelbrot.go
% time ./mandelbrot -mode workers -workers 4
2017/09/17 13:52:51 profile: trace enabled, trace.out
2017/09/17 13:52:57 profile: trace disabled, trace.out

real    0m5.528s
user    0m7.307s
sys     0m4.311s
```

So that made it worse! More real time, more CPU time. Letâ€™s look at the trace to see what happened.

That trace is a mess. There were more workers available, but the seemed to spend all their time fighting over the work to do.

This is because the channel is  _unbuffered_. An unbuffered channel cannot send until there is someone ready to receive.

-   The producer cannot send work until there is a worker ready to receive it.
    
-   Workers cannot receive work until there is someone ready to send, so they compete with each other when they are waiting.
    
-   The sender is not privileged, it cannot take priority over a worker that is already running.
    

What we see here is a lot of latency introduced by the unbuffered channel. There are lots of stops and starts inside the scheduler, and potentially locks and mutexes while waiting for work, this is why we see the  `sys`  time higher.

### 5.8. Using buffered channels

```

import "github.com/pkg/profile"

func main() {
	defer profile.Start(profile.TraceProfile, profile.ProfilePath(".")).Stop()
```

```
% go build mandelbrot.go
% time ./mandelbrot -mode workers -workers 4
2017/09/17 14:23:56 profile: trace enabled, trace.out
2017/09/17 14:23:57 profile: trace disabled, trace.out

real    0m0.905s
user    0m2.150s
sys     0m0.121s
```

Which is pretty close to the per row mode above.

Using a buffered channel the trace showed us that:

-   The producer doesnâ€™t have to wait for a worker to arrive, it can fill up the channel quickly.
    
-   The worker can quickly take the next item from the channel without having to sleep waiting on work to be produced.
    

Using this method we got nearly the same speed using a channel to hand off work per pixel than we did previously scheduling on goroutine per row.

Modify  `nWorkersFillImg`  to work per row. Time the result and analyse the trace.

### 5.9. Mandelbrot microservice

Itâ€™s 2019, generating Mandelbrots is pointless unless you can offer them on the internet as a serverless microservice. Thus, I present to you,  _Mandelweb_

```
% go run examples/mandelweb/mandelweb.go
2017/09/17 15:29:21 listening on http://127.0.0.1:8080/
```

[http://127.0.0.1:8080/mandelbrot](http://127.0.0.1:8080/mandelbrot)

#### 5.9.1. Tracing running applications

In the previous example we ran the trace over the whole program.

As you saw, traces can be very large, even for small amounts of time, so collecting trace data continually would generate far too much data. Also, tracing can have an impact on the speed of your program, especially if there is a lot of activity.

What we want is a way to collect a short trace from a running program.

Fortuntately, the  `net/http/pprof`  package has just such a facility.

#### 5.9.2. Collecting traces via http

Hopefully everyone knows about the  `net/http/pprof`  package.

```
import _ "net/http/pprof"
```

When imported, the  `net/http/pprof`  will register tracing and profiling routes with  `http.DefaultServeMux`. Since Go 1.5 this includes the trace profiler.

`net/http/pprof`  registers with  `http.DefaultServeMux`. If you are using that  `ServeMux`  implicitly, or explicitly, you may inadvertently expose the pprof endpoints to the internet. This can lead to source code disclosure. You probably donâ€™t want to do this.

We can grab a five second trace from mandelweb with  `curl`  (or  `wget`)

```
% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5
```

#### 5.9.3. Generating some load

The previous example was interesting, but an idle webserver has, by definition, no performance issues. We need to generate some load. For this Iâ€™m using  [`hey`  by JBD](https://github.com/rakyll/hey).

```
% go get -u github.com/rakyll/hey
```

Letâ€™s start with one request per second.

```
% hey -c 1 -n 1000 -q 1 http://127.0.0.1:8080/mandelbrot
```

And with that running, in another window collect the trace

```
% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 66169    0 66169    0     0  13233      0 --:--:--  0:00:05 --:--:-- 17390
% go tool trace trace.out
2017/09/17 16:09:30 Parsing trace...
2017/09/17 16:09:30 Serializing trace...
2017/09/17 16:09:30 Splitting trace...
2017/09/17 16:09:30 Opening browser.
Trace viewer is listening on http://127.0.0.1:60301
```

#### 5.9.4. Simulating overload

Letâ€™s increase the rate to 5 requests per second.

```
% hey -c 5 -n 1000 -q 5 http://127.0.0.1:8080/mandelbrot
```

And with that running, in another window collect the trace

% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                Dload  Upload   Total   Spent    Left  Speed
100 66169    0 66169    0     0  13233      0 --:--:--  0:00:05 --:--:-- 17390
% go tool trace trace.out
2017/09/17 16:09:30 Parsing trace...
2017/09/17 16:09:30 Serializing trace...
2017/09/17 16:09:30 Splitting trace...
2017/09/17 16:09:30 Opening browser. Trace viewer is listening on http://127.0.0.1:60301

#### 5.9.5. Extra credit, the Sieve of Eratosthenes

The  [concurrent prime sieve](https://github.com/golang/go/blob/master/doc/play/sieve.go)  is one of the first Go programs written.

Ivan Daniluk  [wrote a great post on visualising](http://divan.github.io/posts/go_concurrency_visualize/)  it.

Letâ€™s take a look at its operation using the execution tracer.

#### 5.9.6. More resources

-   Rhys Hiltner,  [Goâ€™s execution tracer](https://www.youtube.com/watch?v=mmqDlbWk_XA)  (dotGo 2016)
    
-   Rhys Hiltner,  [An Introduction to "go tool trace"](https://www.youtube.com/watch?v=V74JnrGTwKA)  (GopherCon 2017)
    
-   Dave Cheney,  [Seven ways to profile Go programs](https://www.youtube.com/watch?v=2h_NFBFrciI)  (GolangUK 2016)
    
-   Dave Cheney,  [High performance Go workshop](https://dave.cheney.net/training#high-performance-go)]
    
-   Ivan Daniluk,  [Visualizing Concurrency in Go](https://www.youtube.com/watch?v=KyuFeiG3Y60)  (GopherCon 2016)
    
-   Kavya Joshi,  [Understanding Channels](https://www.youtube.com/watch?v=KBZlN0izeiY)  (GopherCon 2017)
    
-   Francesc Campoy,  [Using the Go execution tracer](https://www.youtube.com/watch?v=ySy3sR1LFCQ)
    

## 6. Memory and Garbage Collector å†…å­˜å’Œåƒåœ¾å›æ”¶å™¨ GC

Go is a garbage collected language. This is a design principle, it will not change.

Go æ˜¯ä¸€é—¨è‡ªåŠ¨åƒåœ¾å›æ”¶çš„è¯­è¨€ã€‚
è¿™æ˜¯è®¾è®¡åŸåˆ™ï¼Œä¸ä¼šæ”¹å˜ã€‚

As a garbage collected language, the performance of Go programs is often determined by their interaction with the garbage collector.

ä½œä¸ºä¸€é—¨åƒåœ¾å›æ”¶çš„è¯­è¨€ï¼ŒGo ç¨‹åºçš„æ€§èƒ½å¸¸å¸¸æ˜¯ç”±åƒåœ¾å›æ”¶å™¨å†³å®šçš„ã€‚

Next to your choice of algorithms, memory consumption is the most important factor that determines the performance and scalability of your application.

ä½¿ç”¨çš„ç®—æ³•ä¸å†…å­˜æ¶ˆè´¹æ˜¯å†³å®šç¨‹åºçš„æ€§èƒ½ä¸æ‰©å±•æ€§çš„ä¸»è¦å› ç´ ã€‚


This section discusses the operation of the garbage collector, how to measure the memory usage of your program and strategies for lowering memory usage if garbage collector performance is a bottleneck.

æœ¬èŠ‚è®¨è®ºåƒåœ¾æ”¶é›†å™¨çš„å·¥ä½œæ–¹æ³•ï¼Œ
ä»¥åŠ å¦‚ä½•æµ‹è¯•ç¨‹åºçš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œ
è¿˜æœ‰ å½“åƒåœ¾æ”¶é›†å™¨æ€§èƒ½æˆä¸ºç“¶é¢ˆæ—¶ï¼Œå¦‚ä½•é™ä½å†…å­˜ä½¿ç”¨é‡çš„ç­–ç•¥ã€‚



### 6.1. Garbage collector world view åƒåœ¾æ”¶é›†å™¨é¢é¢è§‚

The purpose of any garbage collector is to present the illusion that there is an infinite amount of memory available to the program.

åƒåœ¾æ”¶é›†å™¨çš„ä½œç”¨æ˜¯ï¼Œç»™ç¨‹åºé€ æˆä¸€ç§é”™è§‰ï¼Œä»¥ä¸ºæœ‰æ— é™å†…å­˜å¯ç”¨ã€‚

You may disagree with this statement, but this is the base assumption of how garbage collector designers work.

ä½ æœ‰å¯èƒ½ä¸åŒæ„è¿™ç§è§‚ç‚¹ï¼Œä½†åƒåœ¾æ”¶é›†å™¨çš„ä½œè€…å°±æ˜¯ä»¥æ­¤ä¸ºç›®æ ‡æ¥è®¾è®¡çš„ã€‚


A stop the world, mark sweep GC is the most efficient in terms of total run time; good for batch processing, simulation, etc. However, over time the Go GC has moved from a pure stop the world collector to a concurrent, non compacting, collector. This is because the Go GC is designed for low latency servers and interactive applications.

> stop the world STW æš‚åœæ—¶é—´ï¼ŒGCæ—¶è¦æš‚åœæ•´ä¸ªç¨‹åºã€‚

æ ‡è®°æ¸…é™¤çš„GCæ–¹æ¡ˆï¼Œå…¶æ€»STWæ—¶é—´æœ€çŸ­ï¼›é€‚ç”¨äº batch processing, simulation ç­‰åœºæ™¯ã€‚
ä½†ç°åœ¨ Go çš„ GC æ–¹æ¡ˆå·²ç»ä»çº¯ç²¹çš„ STW åæ‰§è¡Œåƒåœ¾æ”¶é›†ä¼˜åŒ–ä¸º concurrent, non compacting çš„åƒåœ¾æ”¶é›†ã€‚
è¿™æ˜¯å› ä¸ºï¼Œ Go çš„ GC ä¸»è¦ä¸ºä½å»¶è¿ŸæœåŠ¡å™¨å’Œäº¤äº’å¼åº”ç”¨ç¨‹åºè€Œè®¾è®¡ã€‚

The design of the Go GC favors  _lower latency_  over  _maximum throughput_; it moves some of the allocation cost to the mutator to reduce the cost of cleanup later.

Go çš„ GC è®¾è®¡ç›®æ ‡æ˜¯ æœ€å¤§ååé‡ ä¸Š é™ä½å»¶è¿Ÿ ï¼›
å®ƒæŠŠåˆ†é…èµ„æºçš„å¼€é”€è½¬ç¨¼åˆ°ä¿®æ”¹è¿‡ç¨‹(mutator)ï¼Œä»¥æ­¤æ¥é™ä½æ¸…ç†èµ„æºæ—¶çš„å¼€é”€ã€‚


### 6.2. Garbage collector design åƒåœ¾å›æ”¶å™¨çš„è®¾è®¡

The design of the Go GC has changed over the years

-   Go 1.0, stop the world mark sweep collector based heavily on tcmalloc.
    
-   Go 1.3, fully precise collector, wouldnâ€™t mistake big numbers on the heap for pointers, thus leaking memory.
    
-   Go 1.5, new GC design, focusing on  _latency_  over  _throughput_.
    
-   Go 1.6, GC improvements, handling larger heaps with lower latency.
    
-   Go 1.7, small GC improvements, mainly refactoring.
    
-   Go 1.8, further work to reduce STW times, now down to the 100 microsecond range.
    
-   Go 1.10+,  [move away from pure cooperative goroutine scheduling](https://github.com/golang/proposal/blob/master/design/24543-non-cooperative-preemption.md)  to lower the latency when triggering a full GC cycle.
    

Go GC çš„è®¾è®¡ä¸€ç›´åœ¨æ”¹è¿›ã€‚

- Go 1.0 åŸºäº tcmalloc å®ç°çš„ STW æ ‡è®°æ¸…é™¤ã€‚
- Go 1.3 æ›´ç²¾ç¡®çš„åƒåœ¾æ”¶é›†å™¨ï¼Œèƒ½å¤„ç† heap ä¸Šçš„å¤§æ•°æŒ‡é’ˆï¼Œé¿å…å†…å­˜æ³„éœ²é—®é¢˜ã€‚
- Go 1.5 å…¨æ–° GC è®¾è®¡ï¼Œæ”¹å–„å¤§åé‡ä¸‹å»¶è¿Ÿè¡¨ç°ã€‚
- Go 1.6 GC æ”¹è¿›ï¼Œå¤„ç†å¤§ heap æ—¶ï¼Œå»¶è¿Ÿæ›´ä½ã€‚
- Go 1.7 å°å‰¯ GC æ”¹è¿›ï¼Œä¸»è¦æ˜¯é‡æ„ã€‚
- Go 1.8 è¿›ä¸€æ­¥é™ä½ STW æ—¶é—´ä¸º 100 å¾®ç§’ã€‚
- Go 1.10+ å»é™¤ å®Œå…¨åä½œå¼ Goroutine è°ƒåº¦ï¼Œé™ä½è§¦å‘ full GC cycle æ—¶çš„å»¶è¿Ÿã€‚

> åä½œå¼è°ƒåº¦ ä¸ æŠ¢å å¼è°ƒåº¦
>
> - åä½œï¼šè¢«è°ƒåº¦æ–¹ä¸»åŠ¨å¼ƒæƒï¼›
>   è¿˜ç»†åˆ†ä¸º ä¸»åŠ¨å‡ºè®© å’Œ æŠ¢æˆ˜æ ‡è®° å‡ ç§æ–¹æ³•ã€‚
>   ç¼ºç‚¹ï¼š
>       å¯¹ç”¨æˆ·ä¸å‹å¥½ã€‚
>       æ˜“å‡ºç°é•¿ä¹…æ— æ³•åœæ­¢çš„ä»£ç ï¼Œæ— æ³•åŠæ—¶åƒåœ¾å›æ”¶ï¼Œå…¶ä»– Goroutine æ— æ³•è°ƒåº¦ã€‚
>
> - æŠ¢å ï¼šè°ƒè¯•å™¨å¼ºåˆ¶å°†è¢«è°ƒè¯•æ–¹è¢«åŠ¨ä¸­æ–­ï¼›
>




### 6.3. Garbage collector monitoring  è§‚å¯Ÿåƒåœ¾æ”¶é›†å™¨çš„å·¥ä½œè¿‡ç¨‹

A simple way to obtain a general idea of how hard the garbage collector is working is to enable the output of GC logging.

æ‰“å¼€ GC æ—¥å¿—å¼€å…³ï¼Œå°±èƒ½çœ‹åˆ°åƒåœ¾æ”¶é›†å™¨çš„å·¥ä½œè¿‡ç¨‹ã€‚

These stats are always collected, but normally suppressed, you can enable their display by setting the  `GODEBUG`  environment variable.

è¿™äº›ç»Ÿè®¡ä¿¡æ¯æ˜¯æŒç»­åœ¨æ”¶é›†çš„ï¼Œä½†é€šå¸¸ä¸ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼Œ
æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ `GODEBUG` ç¯å¢ƒå˜é‡æ‰“å¼€æ˜¾ç¤ºå¼€å…³ã€‚

```log
% env GODEBUG=gctrace=1 godoc -http=:8080
gc 1 @0.012s 2%: 0.026+0.39+0.10 ms clock, 0.21+0.88/0.52/0+0.84 ms cpu, 4->4->0 MB, 5 MB goal, 8 P
gc 2 @0.016s 3%: 0.038+0.41+0.042 ms clock, 0.30+1.2/0.59/0+0.33 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 3 @0.020s 4%: 0.054+0.56+0.054 ms clock, 0.43+1.0/0.59/0+0.43 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 4 @0.025s 4%: 0.043+0.52+0.058 ms clock, 0.34+1.3/0.64/0+0.46 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 5 @0.029s 5%: 0.058+0.64+0.053 ms clock, 0.46+1.3/0.89/0+0.42 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 6 @0.034s 5%: 0.062+0.42+0.050 ms clock, 0.50+1.2/0.63/0+0.40 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 7 @0.038s 6%: 0.057+0.47+0.046 ms clock, 0.46+1.2/0.67/0+0.37 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 8 @0.041s 6%: 0.049+0.42+0.057 ms clock, 0.39+1.1/0.57/0+0.46 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
gc 9 @0.045s 6%: 0.047+0.38+0.042 ms clock, 0.37+0.94/0.61/0+0.33 ms cpu, 4->4->1 MB, 5 MB goal, 8 P
```

The trace output gives a general measure of GC activity. The output format of  `gctrace=1`  is described in  [the  `runtime`  package documentation](https://golang.org/pkg/runtime/#hdr-Environment_Variables).

æ ¹æ®è¾“å‡ºçš„è¿½è¸ªæ—¥å¿—å°±èƒ½åˆ¤æ–­ GC çš„å·¥ä½œçŠ¶æ€ã€‚
åœ¨ [`runtime`  package æ–‡æ¡£](https://golang.org/pkg/runtime/#hdr-Environment_Variables) ä¸­ä¼šæè¿° `gctrace=1` çš„è¾“å‡ºæ ¼å¼ã€‚


DEMO: Show  `godoc`  with  `GODEBUG=gctrace=1`  enabled

DEMO: å¯ç”¨ `GODEBUG=gctrace=1` æ—¶æŸ¥çœ‹ godoc

> Use this env var in production, it has no performance impact.

> å¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨è¿™ä¸ªç¯å¢ƒå˜é‡ï¼Œå®ƒå¯¹æ€§èƒ½æ²¡æœ‰å½±å“ã€‚

Using  `GODEBUG=gctrace=1`  is good when you  _know_  there is a problem, but for general telemetry on your Go application I recommend the  `net/http/pprof`  interface.

å½“ä½ ååˆ†äº†è§£é—®é¢˜æ—¶ï¼Œå¯é€šè¿‡ `GODEBUG=gctrace=1` åˆ†æã€‚
ä½†æˆ‘æ›´å»ºè®®ä½ ç”¨  `net/http/pprof` æ¥å£æ£€æµ‹è‡ªå·±çš„ Go ç¨‹åºã€‚

```go
import _ "net/http/pprof"
```

Importing the  `net/http/pprof`  package will register a handler at  `/debug/pprof`  with various runtime metrics, including:

-   A list of all the running goroutines,  `/debug/pprof/heap?debug=1`.
    
-   A report on the memory allocation statistics,  `/debug/pprof/heap?debug=1`.

`net/http/pprof`  will register itself with your default  `http.ServeMux`.

Be careful as this will be visible if you use  `http.ListenAndServe(address, nil)`.

DEMO:  `godoc -http=:8080`, show  `/debug/pprof`.

 
å¼•ç”¨ `net/http/pprof` åŒ…ä¼šåœ¨ HTTP å¤„ç†å™¨ä¸­æ³¨å†Œ `/debug/pprof` å¼€å¤´çš„åœ°å€ï¼Œå…¶ä¸­åŒ…å«å„ç§è¿è¡Œæ—¶æŒ‡æ ‡ä¿¡æ¯ã€‚

TODO pprof/heap åœ°å€å¯èƒ½å†™é”™äº†ã€‚

- è®¿é—®  `/debug/pprof/heap?debug=1` è¿”å›æ‰€æœ‰è¿è¡Œä¸­çš„ goroutine åˆ—è¡¨ã€‚

- è®¿é—®  `/debug/pprof/heap?debug=1` è¿”å›å†…å­˜åˆ†é…çš„ç»Ÿè®¡ä¿¡æ¯æŠ¥å‘Šã€‚


æ³¨æ„ï¼š
`net/http/pprof`  ä¼šæ³¨å†Œåˆ°é»˜è®¤çš„ `http.ServeMux` ä¸Šã€‚
åªè¦è°ƒç”¨äº† `http.ListenAndServe(address, nil)` ä»£ç ï¼Œè¿™äº›ç»Ÿè®¡ä¿¡æ¯å°±èƒ½è·å–åˆ°ã€‚

DEMO:  `godoc -http=:8080`, show  `/debug/pprof`.



#### 6.3.1. Garbage collector tuning

The Go runtime provides one environment variable to tune the GC,  `GOGC`.

The formula for GOGC is

goal=reachableâ‹…(1+GOGC100)goal=reachableâ‹…(1+GOGC100)

For example, if we currently have a 256MB heap, and  `GOGC=100`  (the default), when the heap fills up it will grow to

512MB=256MBâ‹…(1+100100)512MB=256MBâ‹…(1+100100)

-   Values of  `GOGC`  greater than 100 causes the heap to grow faster, reducing the pressure on the GC.
    
-   Values of  `GOGC`  less than 100 cause the heap to grow slowly, increasing the pressure on the GC.
    

The default value of 100 is  _just a guide_. you should choose your own value  _after profiling your application with production loads_.

### 6.4. Reducing allocations

Make sure your APIs allow the caller to reduce the amount of garbage generated.

Consider these two Read methods

```
func (r *Reader) Read() ([]byte, error)
func (r *Reader) Read(buf []byte) (int, error)
```

The first Read method takes no arguments and returns some data as a  `[]byte`. The second takes a  `[]byte`  buffer and returns the amount of bytes read.

The first Read method will  _always_  allocate a buffer, putting pressure on the GC. The second fills the buffer it was given.

Can you name examples in the std lib which follow this pattern?

### 6.5. strings and []bytes

In Go  `string`  values are immutable,  `[]byte`  are mutable.

Most programs prefer to work  `string`, but most IO is done with  `[]byte`.

Avoid  `[]byte`  to string conversions wherever possible, this normally means picking one representation, either a  `string`  or a  `[]byte`  for a value. Often this will be  `[]byte`  if you read the data from the network or disk.

The  [`bytes`](https://golang.org/pkg/bytes/)  package contains many of the same operationsâ€‰â€”â€‰`Split`,  `Compare`,  `HasPrefix`,  `Trim`, etcâ€‰â€”â€‰as the  [`strings`](https://golang.org/pkg/strings/)  package.

Under the hood  `strings`  uses same assembly primitives as the  `bytes`  package.

### 6.6. Using  `[]byte`  as a map key

It is very common to use a  `string`  as a map key, but often you have a  `[]byte`.

The compiler implements a specific optimisation for this case

```
var m map[string]string
v, ok := m[string(bytes)]
```

This will avoid the conversion of the byte slice to a string for the map lookup. This is very specific, it wonâ€™t work if you do something like

```
key := string(bytes)
val, ok := m[key]
```

Letâ€™s see if this is still true. Write a benchmark comparing these two methods of using a  `[]byte`  as a  `string`  map key.

### 6.7. Avoid string concatenation

Go strings are immutable. Concatenating two strings generates a third. Which of the following is fastest?

```
		s := request.ID
		s += " " + client.Addr().String()
		s += " " + time.Now().String()
		r = s
```

```
		var b bytes.Buffer
		fmt.Fprintf(&b, "%s %v %v", request.ID, client.Addr(), time.Now())
		r = b.String()
```

```
		r = fmt.Sprintf("%s %v %v", request.ID, client.Addr(), time.Now())
```

```
		b := make([]byte, 0, 40)
		b = append(b, request.ID...)
		b = append(b, ' ')
		b = append(b, client.Addr().String()...)
		b = append(b, ' ')
		b = time.Now().AppendFormat(b, "2006-01-02 15:04:05.999999999 -0700 MST")
		r = string(b)
```

```
		var b strings.Builder
		b.WriteString(request.ID)
		b.WriteString(" ")
		b.WriteString(client.Addr().String())
		b.WriteString(" ")
		b.WriteString(time.Now().String())
		r = b.String()
```

DEMO:  `go test -bench=. ./examples/concat`

### 6.8. Preallocate slices if the length is known

Append is convenient, but wasteful.

Slices grow by doubling up to 1024 elements, then by approximately 25% after that. What is the capacity of  `b`  after we append one more item to it?

```
func main() {
	b := make([]int, 1024)
	b = append(b, 99)
	fmt.Println("len:", len(b), "cap:", cap(b))
}
```

If you use the append pattern you could be copying a lot of data and creating a lot of garbage.

If know know the length of the slice beforehand, then pre-allocate the target to avoid copying and to make sure the target is exactly the right size.

Before

```
var s []string
for _, v := range fn() {
        s = append(s, v)
}
return s
```

After

```
vals := fn()
s := make([]string, len(vals))
for i, v := range vals {
        s[i] = v
}
return s
```

### 6.9. Using sync.Pool

The  `sync`  package comes with a  `sync.Pool`  type which is used to reuse common objects.

`sync.Pool`  has no fixed size or maximum capacity. You add to it and take from it until a GC happens, then it is emptied unconditionally. This is  [by design](https://groups.google.com/forum/#!searchin/golang-dev/gc-aware/golang-dev/kJ_R6vYVYHU/LjoGriFTYxMJ):

> If before garbage collection is too early and after garbage collection too late, then the right time to drain the pool must be during garbage collection. That is, the semantics of the Pool type must be that it drains at each garbage collection.â€‰â€”â€‰Russ Cox

sync.Pool in action

```
var pool = sync.Pool{New: func() interface{} { return make([]byte, 4096) }}

func fn() {
	buf := pool.Get().([]byte) // takes from pool or calls New
	// do work
	pool.Put(buf) // returns buf to the pool
}
```

`sync.Pool`  is not a cache. It can and will be emptied  _at_any_time_.

Do not place important items in a  `sync.Pool`, they will be discarded.

The design of sync.Pool emptying itself on each GC may change in Go 1.13 which will help improve its utility.

> This CL fixes this by introducing a victim cache mechanism. Instead of clearing Pools, the victim cache is dropped and the primary cache is moved to the victim cache. As a result, in steady-state, there are (roughly) no new allocations, but if Pool usage drops, objects will still be collected within two GCs (as opposed to one).â€‰â€”â€‰Austin Clements

[https://go-review.googlesource.com/c/go/+/166961/](https://go-review.googlesource.com/c/go/+/166961/)

### 6.10. Exercises

-   Using  `godoc`  (or another program) observe the results of changing  `GOGC`  using  `GODEBUG=gctrace=1`.
    
-   Benchmark byteâ€™s string(byte) map keys
    
-   Benchmark allocs from different concat strategies.
    

## 7. Tips and trips

A random grab back of tips and suggestions

This final section contains a number of tips to micro optimise Go code.

### 7.1. Goroutines

The key feature of Go that makes it a great fit for modern hardware are goroutines.

Goroutines are so easy to use, and so cheap to create, you could think of them as  _almost_  free.

The Go runtime has been written for programs with tens of thousands of goroutines as the norm, hundreds of thousands are not unexpected.

However, each goroutine does consume a minimum amount of memory for the goroutineâ€™s stack which is currently at least 2k.

2048 * 1,000,000 goroutines == 2GB of memory, and they havenâ€™t done anything yet.

Maybe this is a lot, maybe it isnâ€™t given the other usages of your application.

#### 7.1.1. Know when to stop a goroutine

Goroutines are cheap to start and cheap to run, but they do have a finite cost in terms of memory footprint; you cannot create an infinite number of them.

Every time you use the  `go`  keyword in your program to launch a goroutine, you must  **know**  how, and when, that goroutine will exit.

In your design, some goroutines may run until the program exits. These goroutines are rare enough to not become an exception to the rule.

If you donâ€™t know the answer, thatâ€™s a potential memory leak as the goroutine will pin its stackâ€™s memory on the heap, as well as any heap allocated variables reachable from the stack.

Never start a goroutine without knowing how it will stop.

#### 7.1.2. Further reading

-   [Concurrency Made Easy](https://www.youtube.com/watch?v=yKQOunhhf4A&index=16&list=PLq2Nv-Sh8EbZEjZdPLaQt1qh_ohZFMDj8)  (video)
    
-   [Concurrency Made Easy](https://dave.cheney.net/paste/concurrency-made-easy.pdf)  (slides)
    
-   [Never start a goroutine without knowning when it will stop](https://dave.cheney.net/practical-go/presentations/qcon-china.html#_never_start_a_goroutine_without_knowning_when_it_will_stop)  (Practical Go, QCon Shanghai 2018)
    

### 7.2. Go uses efficient network polling for some requests

The Go runtime handles network IO using an efficient operating system polling mechanism (kqueue, epoll, windows IOCP, etc). Many waiting goroutines will be serviced by a single operating system thread.

However, for local file IO, Go does not implement any IO polling. Each operation on a  `*os.File`  consumes one operating system thread while in progress.

Heavy use of local file IO can cause your program to spawn hundreds or thousands of threads; possibly more than your operating system allows.

Your disk subsystem does not expect to be able to handle hundreds or thousands of concurrent IO requests.

To limit the amount of concurrent blocking IO, use a pool of worker goroutines, or a buffered channel as a semaphore.

```
var semaphore = make(chan struct{}, 10)

func processRequest(work *Work) {
	semaphore <- struct{}{} // acquire semaphore
	// process request
	<-semaphore // release semaphore
}
```

### 7.3. Watch out for IO multipliers in your application

If youâ€™re writing a server process, its primary job is to multiplex clients connected over the network, and data stored in your application.

Most server programs take a request, do some processing, then return a result. This sounds simple, but depending on the result it can let the client consume a large (possibly unbounded) amount of resources on your server. Here are some things to pay attention to:

-   The amount of IO requests per incoming request; how many IO events does a single client request generate? It might be on average 1, or possibly less than one if many requests are served out of a cache.
    
-   The amount of reads required to service a query; is it fixed, N+1, or linear (reading the whole table to generate the last page of results).
    

If memory is slow, relatively speaking, then IO is so slow that you should avoid doing it at all costs. Most importantly avoid doing IO in the context of a requestâ€”donâ€™t make the user wait for your disk subsystem to write to disk, or even read.

### 7.4. Use streaming IO interfaces

Where-ever possible avoid reading data into a  `[]byte`  and passing it around.

Depending on the request you may end up reading megabytes (or more!) of data into memory. This places huge pressure on the GC, which will increase the average latency of your application.

Instead use  `io.Reader`  and  `io.Writer`  to construct processing pipelines to cap the amount of memory in use per request.

For efficiency, consider implementing  `io.ReaderFrom`  /  `io.WriterTo`  if you use a lot of  `io.Copy`. These interface are more efficient and avoid copying memory into a temporary buffer.

### 7.5. Timeouts, timeouts, timeouts

Never start an IO operating without knowing the maximum time it will take.

You need to set a timeout on every network request you make with  `SetDeadline`,  `SetReadDeadline`,  `SetWriteDeadline`.

### 7.6. Defer is expensive, or is it?

`defer`  is expensive because it has to record a closure for deferâ€™s arguments.

```
defer mu.Unlock()
```

is equivalent to

```
defer func() {
        mu.Unlock()
}()
```

`defer`  is expensive if the work being done is small, the classic example is  `defer`  ing a mutex unlock around a struct variable or map lookup. You may choose to avoid  `defer`  in those situations.

This is a case where readability and maintenance is sacrificed for a performance win.

Always revisit these decisions.

### 7.7. Avoid Finalisers

Finalisation is a technique to attach behaviour to an object which is just about to be garbage collected.

Thus, finalisation is non deterministic.

For a finaliser to run, the object must not be reachable by  _anything_. If you accidentally keep a reference to the object in the map, it wonâ€™t be finalised.

Finalisers run as part of the gc cycle, which means it is unpredictable when they will run and puts them at odds with the goal of reducing gc operation.

A finaliser may not run for a long time if you have a large heap and have tuned your appliation to create minimal garbage.

### 7.8. Minimise cgo

cgo allows Go programs to call into C libraries.

C code and Go code live in two different universes, cgo traverses the boundary between them.

This transition is not free and depending on where it exists in your code, the cost could be substantial.

cgo calls are similar to blocking IO, they consume a thread during operation.

Do not call out to C code in the middle of a tight loop.

#### 7.8.1. Actually, maybe avoid cgo

cgo has a high overhead.

For best performance I recommend avoiding cgo in your applications.

-   If the C code takes a long time, cgo overhead is not as important.
    
-   If youâ€™re using cgo to call a very short C function, where the overhead is the most noticeable, rewrite that code in Goâ€‰â€”â€‰by definition itâ€™s short.
    
-   If youâ€™re using a large piece of expensive C code is called in a tight loop, why are you using Go?
    

Is there anyone whoâ€™s using cgo to call expensive C code frequently?

##### [](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#further_reading_7)[Further reading](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#further_reading_7)

-   [cgo is not Go](http://dave.cheney.net/2016/01/18/cgo-is-not-go)
    

### 7.9. Always use the latest released version of Go

Old versions of Go will never get better. They will never get bug fixes or optimisations.

-   Go 1.4 should not be used.
    
-   Go 1.5 and 1.6 had a slower compiler, but it produces faster code, and has a faster GC.
    
-   Go 1.7 delivered roughly a 30% improvement in compilation speed over 1.6, a 2x improvement in linking speed (better than any previous version of Go).
    
-   Go 1.8 will deliver a smaller improvement in compilation speed (at this point), but a significant improvement in code quality for non Intel architectures.
    
-   Go 1.9-1.12 continue to improve the performance of generated code, fix bugs, and improve inlining and improve debuging.
    

Old version of Go receive no updates.  **Do not use them**. Use the latest and you will get the best performance.

#### 7.9.1. Further reading

-   [Go 1.7 toolchain improvements](http://dave.cheney.net/2016/04/02/go-1-7-toolchain-improvements)
    
-   [Go 1.8 performance improvements](http://dave.cheney.net/2016/09/18/go-1-8-performance-improvements-one-month-in)
    

#### 7.9.2. Move hot fields to the top of the struct

### 7.10. Discussion

Any questions?

## Final Questions and Conclusion

> Readable means reliableâ€‰â€”â€‰Rob Pike

Start with the simplest possible code.

_Measure_. Profile your code to identify the bottlenecks,  _do not guess_.

If performance is good,  _stop_. You donâ€™t need to optimise everything, only the hottest parts of your code.

As your application grows, or your traffic pattern evolves, the performance hot spots will change.

Donâ€™t leave complex code that is not performance critical, rewrite it with simpler operations if the bottleneck moves elsewhere.

Always write the simplest code you can, the compiler is optimised for  _normal_  code.

Shorter code is faster code; Go is not C++, do not expect the compiler to unravel complicated abstractions.

Shorter code is  _smaller_  code; which is important for the CPUâ€™s cache.

Pay very close attention to allocations, avoid unnecessary allocation where possible.

> I can make things very fast if they donâ€™t have to be correct.â€‰â€”â€‰Russ Cox

Performance and reliability are equally important.

I see little value in making a very fast server that panics, deadlocks or OOMs on a regular basis.

Donâ€™t trade performance for reliability.

----------

[1](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#_footnoteref_1). Hennessy et al: 1.4x annual performance improvment over 40 years.

Version dotgo-2019-3-g660848  
Last updated 2019-04-26 02:55:54 UTC


> Note: exec js in chrome console, to  remove anchor in https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html

```js
var ehs = document.querySelectorAll( "h2" );
ehs.forEach( e => {e.removeChild(e.childNodes[0])});

var ehs = document.querySelectorAll( "h2 a.link" );
ehs.forEach ( e => { 
    e.removeAttribute("href");
});

var ehs = document.querySelectorAll( "h3" );
ehs.forEach( e => {e.removeChild(e.childNodes[0])});

var ehs = document.querySelectorAll( "h3 a.link" );
ehs.forEach ( e => { 
    e.removeAttribute("href");
});

var ehs = document.querySelectorAll( "h4" );
ehs.forEach( e => {e.removeChild(e.childNodes[0])});

var ehs = document.querySelectorAll( "h4 a.link" );
ehs.forEach ( e => { 
    e.removeAttribute("href");
});
```


[^HighPerformanceWorkShopEN]:[High Performance Work Shop](https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#mechanical_sympathy)

[^HighPerformanceWorkShopGithub]:[High Performance Work Shop Github](https://github.com/davecheney/gophercon2018-performance-tuning-workshop)


[^HighPerformanceWorkShopCN1]:[è¯‘æ–‡1 High Performance Work Shop](https://www.yuque.com/ksco/uiondt/nimz8b)

[^HighPerformanceWorkShopCN2]:[è¯‘æ–‡2 High Performance Work Shop](https://blog.zeromake.com/pages/high-performance-go-workshop/)

[^CPUCache]:[CPUCache](https://coolshell.cn/articles/20793.html)

[^MemoryAndNativeCodePerformance]: [å†…å­˜ä¸æœ¬æœºä»£ç çš„æ€§èƒ½](https://www.infoq.cn/article/2013/07/Native-Performance)

[^PerformanceIntruction]:[æ€§èƒ½è°ƒä¼˜æ”»ç•¥](https://coolshell.cn/articles/7490.html)


[^GoMillionTCP]: [ç™¾ä¸‡ Go TCP è¿æ¥çš„æ€è€ƒ: epollæ–¹å¼å‡å°‘èµ„æºå ç”¨](https://colobu.com/2019/02/23/1m-go-tcp-connection/)


[^InterlCPUList]: [è‹±ç‰¹å°”å¾®å¤„ç†å™¨åˆ—è¡¨](https://zh.wikipedia.org/wiki/%E8%8B%B1%E7%89%B9%E5%B0%94%E5%BE%AE%E5%A4%84%E7%90%86%E5%99%A8%E5%88%97%E8%A1%A8)ã€‚

[^CPUMax4GHz]: [ä¸ºä»€ä¹ˆä¸»æµCPUçš„é¢‘ç‡æ­¢æ­¥äº4G?æˆ‘ä»¬è§¦åˆ°é¢‘ç‡å¤©èŠ±æ¿äº†å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/30409360)

[^qcraoPPROF]: [æ·±åº¦è§£å¯†Goè¯­è¨€ä¹‹pprof](https://qcrao.com/2019/11/10/dive-into-go-pprof/)

